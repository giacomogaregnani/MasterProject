\subsection{Monte Carlo approximation of probabilistic solvers}
Let us consider the numerical method introduced in \eqref{probabilityODE} and the Monte Carlo approximation 
\begin{equation}\label{eq:MCapproximation}
	\hat Z = \frac{1}{M} \sum_{i = 1}^M \phi\left(U_N^{(i)}\right).
\end{equation}
The mean square error (MSE) of $\hat Z$ is given by
\begin{equation}\label{eq:MCMSE}
\begin{aligned}
\MSE(\hat Z) &= \E\left[\left(\hat Z - \phi\left(u(T)\right)\right)^2\right] \\
&= \Var(\hat Z) + \E\left[\hat Z - \phi\left(u(T)\right)\right]^2 \\
&\leq \Var(\hat Z) + C h^{2\min\{2p, q\}},
\end{aligned}
\end{equation} 
where the second term is bounded thanks to proposition \ref{thm:weakorder} with $C > 0$. In the standard theory of SDE's, the first term is bounded by $CM^{-1}$, where $C$ is a positive constant. In the probabilistic solver we consider in this work, it is possible to bound the first term with a function of the time step $h$. Intuitively, this favorable property comes from the fact that the noise scale is of the same order of magnitude of the time step. 
\begin{lemma}\label{lem:varMC} Consider the numerical method \eqref{probabilityODE} applied to a one-dimensional ODE with $\psi$ any explicit Runge-Kutta method on $s$ stages and Assumption \ref{assumption_1}. Then the numerical solution $U_k$ at time $t_k = kh$ satisfies
\begin{equation}
	\Var(U_k) \leq C_1\Var(U_0) + C_2\sigma Q h^{2p},
\end{equation}
with $C_1$, $C_2$ positive constants.
\end{lemma}
\begin{proof} Let us consider as the numerical integrator $\psi$ the Explicit Euler method and $p = 1$, coherently with the common choice that $p$ is equal to the order or the deterministic integrator. Then, thanks to Assumption \ref{assumption_1} we can write the numerical solution $U_{k+1}$ as
\begin{equation}
	U_{k+1} = U_k + hf(U_k) + h^{3/2}\sigma Q Z_k,
\end{equation}
with $Z_k$ a random variable such that $\E\left[Z_k\right] = 1$ and independent of $U_k$ Then
\begin{equation}
\begin{aligned}
	\Var(U_{k+1}) &= \Var(U_k + hf(U_k)) + h^3\sigma Q \\
	&\leq 2\Var(U_k) + 2h^2 \Var(f(U_k)) + h^3\sigma Q,
\end{aligned}
\end{equation}
where we exploited that for any random variables $X$, $Y$,
\begin{equation}\label{eq:varProp}
\Var(X + Y) \leq 2\Var(X) + 2\Var(Y).
\end{equation}
Since $f$ is Lipschitz continuous with constant $C_L$, we can bound the second term in the sum above as
\begin{equation}\label{eq:lipschitzVar}
\begin{aligned}
\Var(f(U_k)) &= \Var(f(U_k) - f(\E\left[U_k\right])) \\
&\leq \E\left[(f(U_k) - f(\E\left[U_k\right]))^2\right] \\
&\leq C_L^2\E\left[(U_k - \E(U_k))^2\right] \\
&= C_L^2 \Var(U_k).
\end{aligned}
\end{equation}
Hence, we find
\begin{equation}
\begin{aligned}
 \Var(U_{k+1}) &\leq 2(1 + C_L^2h^2)\Var(U_k) + \sigma Q h^3 \\
 &\leq 2(1 + C_L^2h^2)^k\Var(U_0) + \sigma Q Th^2,\\
\end{aligned}
\end{equation}
thus the result is proved choosing $C_1 = 2(1 + C_L^2T^2)$ and $C_2 = T$. \\
Let us consider now any explicit Runge-Kutta method $\psi$, and let us rewrite $\eqref{probabilityODE}$ as
\begin{equation}
	U_{k+1} = U_k + h\tilde \psi (U_k) + h^{p + 1/2} \sigma Q Z_k,
\end{equation}
where $\tilde\psi(x) \defeq h^{-1}(\psi(x) - x)$ is given by
\begin{equation}
	\tilde\psi(U_k) = \sum_{i = 1}^{s} b_i K_i,
\end{equation} 
and $K_i$, $i = 1, \ldots, s$, are the stages of the Runge-Kutta method. Then, proceeding as above
\begin{equation}
	\Var(U_{k+1}) \leq 2\Var(U_k) + 2h^2 \Var(\tilde \psi (U_k)) + \sigma Q h^{2p + 1}.
\end{equation}
Let us consider the second term. A direct bound, following from a generalization on $s$ terms of \eqref{eq:varProp} is
\begin{equation}\label{eq:temp}
	\Var(\tilde \psi (U_k)) \leq s\sum_{i = 1}^{s} b_i^2 \Var(K_i).
\end{equation}
Hence, we can consider the variance of each stage singularly. Since we are only considering explicit Runge-Kutta method, it is possible to estimate the single variances recursively
\begin{equation}
\begin{aligned}
\Var(K_1) &= \Var(f(U_k)) \leq C_L^2 \Var(U_k), \\
\Var(K_2) &= \Var(f(U_k + ha_{21}K_1)) \leq C_L^2 \Var(U_k + ha_{21}K_1) \\
&\leq 2C_L^2 (\Var(U_k) + h^2a_{21}^2\Var(K_1)) \\
&\leq 2C_L^2 (1 + T^2a_{21}^2C_L^2)\Var(U_k) \leq C \Var(U_k) \\
\Var(K_i) &\leq \Var(f(U_k + h \textstyle\sum_{j=1}^{i-1}a_{ij}K_j)) \leq C \Var(U_k),
\end{aligned}
\end{equation}
where $C$ is a positive varying from one line to another depending on $C_L$, $T$ and the coefficients of the Runge-Kutta method. We then substitute in \eqref{eq:temp} and get
\begin{equation}
\begin{aligned}
\Var(U_{k+1}) &\leq 2 (1 + Cs\textstyle\sum_{i=1}^{s}b_i^2) \Var(U_k) + \sigma Q h^{2p + 1} \\
			  &\leq \tilde C \Var(U_0) + T \sigma Q h^{2p},
\end{aligned}
\end{equation}
thus giving the desired result with $C_1 = \tilde C$ and $C_2 = T$.
\end{proof}
\noindent Provided with this result we can now consider the variance of the Monte Carlo estimator $\hat Z$ introduced in \eqref{eq:MCapproximation}. Since the samples $U_N^{(i)}$ are independent and identically distributed as $U_N$, we have
\begin{equation}
\begin{aligned}
	\Var(\hat Z) &= \Var\left(\frac{1}{M} \sum_{i = 1}^M \phi\left(U_N^{(i)}\right)\right) \\
	&= \frac{1}{M^2} \sum_{i = 1}^M \Var\left(\phi\left(U_N\right)\right) \\
	&= \frac{1}{M} \Var\left(\phi\left(U_N\right)\right) \\
\end{aligned}
\end{equation}
If the function $\phi$ is Lipschitz continuous we can use \eqref{eq:lipschitzVar} and Lemma \ref{lem:varMC} and get
\begin{equation}
	\Var(\hat Z) \leq \frac{C}{M} \Var(U_N) \leq \frac{Ch^{2p}}{M},
\end{equation}
thus obtaining the following bound for the MSE of $\hat Z$ 
\begin{equation}
	\MSE(\hat Z) \leq C_1 h^{2\min\{2p, q\}} + C_2 \frac{h^{2p}}{M}.
\end{equation}
Let us remark \cite{CGS16} that a common choice for the noise scale $p$ is the order of the deterministic solver $q$, therefore the bias and the variance terms in the MSE are both of order $2q$ with respect to $h$.

\subsubsection{Numerical experiment}

\begin{figure}
	\centering
	\resizebox{0.6\linewidth}{!}{\input{plots/MonteCarloVariance2.tikz}}
	\caption{Variance and squared bias of the Monte Carlo estimator $\hat Z$ with Explicit Euler and RK4 applied to \eqref{eq:FitzNag}. The two components of the MSE have the same order of convergence with respect to the time step $h$.}
	\label{fig:MonteCarloVariance}
\end{figure}

We consider the FitzHug-Nagumo problem introduced in \eqref{eq:FitzNag} with the same initial conditions and parameter values and integrate it up to the final time $T = 10$ with the probabilistic integrator. We choose the function $\phi$ to be given by $\phi(X) = X^TX$ and generate a reference solution $Z$ with RK4 computed on a fine time step. We choose as deterministic integrator Explicit Euler and RK4 and the noise scale $p$ equal to $q$, i.e., one and four respectively. We choose $M = 10$ and the time step $h = 0.5 / 2^i$ with $i = 0, 1, \ldots, 11$. Then we compute $300$ times the estimator $\hat Z$ for all the values of the time step, thus estimating its variance and bias. Results (Figure \ref{fig:MonteCarloVariance}) confirm the result presented in Lemma \ref{lem:varMC}, as the order of convergence of the variance of $\hat{Z}$ to zero is of order $2$ and $8$ with respect to $h$ for Explicit Euler and RK4 respectively independently of $M$. 


\subsection{Multi-level Monte Carlo}

In this section, we will explain how to apply the Multi-level Monte Carlo method (MLMC) in this frame. Consider the approximation $U_k$ given by \eqref{probabilityODE} to the solution $u(t)$ of \eqref{ODE}. Given $\phi$ a function in $\mathcal C^\infty$, let us denote by $Z = \E(\phi(U_N)), Nh = T$ the expectation of the numerical solution at final time. If a standard Monte Carlo method over $M$ realizations of the numerical solution is applied, the only accessible quantity is
\begin{equation}\label{MonteCarlo}
	\hat Z = \frac{1}{M} \sum_{i = 1}^M \phi\left(U_N^{(i)}\right),
\end{equation}
where the index $i$ is referred to the $i$-th trajectory. Therefore, the quantity $\hat Z$ is an unbiased estimator of $Z$. Then, the Mean Square Error (MSE) of $\hat Z$ is given by
\begin{equation}\label{MSEMC}
\begin{aligned}
	\MSE(\hat Z) &= \E\left(\hat Z - \phi\left(u(T)\right)\right)^2 \\
	&= \Var\left(\hat Z\right) + \left(\E\left(\hat Z - \phi\left(u(T)\right)\right)\right)^2 \\
	&= \frac{C_1}{M} + C_2 h^{2\min\{2p, q\}},
\end{aligned}
\end{equation} 
where we have used Proposition \ref{thm:weakorder} with $C_1, C_2$ positive constants. If we introduce as a measure for the error 
\begin{equation}
	e = \sqrt{\MSE\left(\hat Z\right)},
\end{equation}
then in order to have $e = \OO(\epl)$, with $\epl$ fixed, one has to set
\begin{equation}
	h = \OO\left(\epl^{1 / \min\{2p, q\}} \right), \quad M = \OO(\epl^{-2}).
\end{equation}
If we measure the cost as the product between the number of timesteps and the number of trajectories, we find easily that in this case
\begin{equation}
	\mathrm{cost} = \OO\left(\epl^{-2 - 1/\min\{2p, q\}}\right).
\end{equation}
The idea of MLMC is introducing an \textit{hierarchical sampling}, introducing levels $l = 0, \ldots, L$, which have time step $h_l = T / N^l$ with $N_l = 2^l$. For each level, the number of trajectories is variable and is denoted by $M_l$. The estimator of $Z$ is then constructed as
\begin{equation}
	\bar Z = \sum_{l=0}^L \frac{1}{M_l} \sum_{i = 1}^{M_l}\left( \phi_l^{(i)} - \phi_{l-1}^{(i)} \right), \quad \phi_{l}^{(i)} = \phi \left(U_{N_l}^{(i)}\right).
\end{equation}
The values $\phi_l^{(i)}$ are constructed under two assumptions
\begin{enumerate}
	\item $\phi_l^{(i)}$ and $\phi_{l-1}^{(i)}$, with $\phi_{-1} \defeq 0$, are constructed using the same Brownian path,
	\item $\phi_l^{(i)}, \phi_{l-1}^{(i)}$ and $\phi_l^{(j)}, \phi_{l-1}^{(j)}$ are independent for $i \neq j$.
\end{enumerate}
The internal sum in $\bar Z$ is a telescopic sum, hence
\begin{equation}
	\E(\phi_L) = \E(\bar Z).
\end{equation}
Then we can compute the MSE of $\bar Z$ as
\begin{equation}
\begin{aligned}
	\MSE(\bar Z) &= \E\left(\bar Z - \phi\left(u(T)\right)\right)^2 \\
	&= \Var\left(\bar Z\right) + \left(\E\left(\bar Z - \phi\left(u(T)\right)\right)\right)^2 \\
	&= \Var\left(\bar Z\right) + \left(\E\left(\phi\left(U_{N_L}\right) - \phi\left(u(T)\right)\right)\right)^2 \\
	&= \Var\left(\bar Z\right) + \OO \left(h_L^{2\min\{2p, q\}}\right).
\end{aligned}
\end{equation}
The variance is then computable as
\begin{equation}
	\Var(\bar Z) = \sum_{l=0}^L \frac{1}{M_l^2} \sum_{i=1}^{M_l} \Var\left( \phi_l^{(i)} - \phi_{l-1}^{(i)} \right) = \sum_{l=0}^L \frac{V_l}{M_l}.
\end{equation}
Thanks to Proposition \ref{thm:strongConv} it is possible to estimate $V_l$.
\begin{lemma} If $\phi$ is Lipschitz continuous then 
\begin{equation}
	 V_l \leq C h_l^{2\min\{p, q\}},
\end{equation}
with $C > 0$ is a constant independent of $h_l$.
\end{lemma}
\begin{proof} Let us recall that for any random variable $Y_1, Y_2$, it is true that
\begin{equation}
	 \Var(Y_1 + Y_2) \leq 2 \left(\Var(Y_1) + \Var(Y_2)\right).
\end{equation}
Let us consider now the case $l = 0$. In this case
\begin{equation}
	V_0 = \phi_0 - \phi_{-1} = \OO(1),
\end{equation}
as $h_0 = T$. For $l \geq 1$, thanks to the property of the variance above 
\begin{equation}
\begin{aligned}
	\Var(\phi_l - \phi_{l-1}) &= \Var\left(\phi_l - \phi\left(u(T)\right) + \phi\left(u(T)\right) - \phi_{l-1}\right) \\
		&\leq 2\left(\Var\left(\phi_l - \phi\left(u(T)\right)\right) + \Var\left(\phi_{l-1} - \phi\left(u(T)\right)\right)\right)
\end{aligned}
\end{equation}
Then, considering singularly the two terms and denoting by $K$ the Lipschitz constant of $\phi$
\begin{equation}
\begin{aligned}
	\Var\left(\phi_l - \phi\left(u(T)\right)\right) &\leq \E \left(\phi_l - \phi\left(u(T)\right)\right)^2  = \E \left(\phi(U_{N_l}) - \phi\left(u(T)\right)\right)^2 \\
			&\leq K^2 \E \left(U_{N_l} - u(T)\right)^2  \\
			&\leq K^2 \E\left|U_{N_l} - u(T)\right|^2 \leq Ch_l^{2\min\{p, q\}},
\end{aligned}
\end{equation}
where the last bound is given by Proposition \ref{thm:strongConv}.
\end{proof}
\noindent Therefore, the MSE is given by
\begin{equation}
	\MSE(\bar Z) = C_1 h^{2\min\{2p, q\}}_L + C_2 \sum_{l=0}^L \frac{h_l^{2\min\{p, q\}}}{M_l}.
\end{equation}
We would like those two terms to balance, therefore we choose $M_l$ as
\begin{equation}
	M_l = \frac{h_l^{2\min\{p, q\}}L}{h_L^{2\min\{2p, q\}}},
\end{equation}
as in this way 
\begin{equation}
	\MSE(\bar Z) = C_1 h^{2\min\{2p, q\}}_L + C_2 \frac{L+1}{L} h_L^{2\min\{2p, q\}} = \OO\left(h^{2\min\{2p, q\}}_L\right).
\end{equation}
Hence, if we use as a measure of the error
\begin{equation}
	e = \sqrt{MSE\left(\bar Z \right)},
\end{equation}
and imposing $e = \OO(\epl)$ for a fixed $\epl$, we get for the finest time step
\begin{equation} \label{hLeps}
	h_L = \OO\left(\epl^{1/\min\{2p, q\}}\right).
\end{equation}
Let us compute the cost with this choice of the parameters. Defining the cost as the product of the number of time steps and the number of trajectories, we find
\begin{equation}
	\mathrm{cost} = \sum_{l=0}^L N_l M_l = \sum_{l=0}^L \frac{T}{h_l} \frac{h_l^{2\min\{p,q\}}L}{h_L^{2\min\{2p, q\}}}.
\end{equation}
For a matter of clarity in the computation, we consider three different cases. 

\subsubsection*{Case 1: $q \leq p$}
In this case, $\min\{p, q\} = q$ and $\min\{2p, q\} = q$. Therefore
\begin{equation}
\begin{aligned}
	\mathrm{cost} &=  \sum_{l=0}^L \frac{T}{h_l} \frac{h_l^{2q}L}{h_L^{2q}} = \frac{TL}{h_L} \sum_{l=0}^L \left(\frac{h_l}{h_L}\right)^{2q-1} \\
	&= \frac{TL}{h_L} \sum_{l=0}^L 2^{(L-l)(2q-1)} = \frac{TL}{h_L} 2^{L(2q-1)} \sum_{l=0}^L 2^{-l(2q-1)} \\
	&\leq L2^{2qL}\frac{1}{1 - 2^{1-2q}} \leq 2 L 2^{2qL} = \OO\left(L h_L^{-2q}\right),
\end{aligned}
\end{equation}
where we have assumed $q \geq 1$ so that the geometric series converges. Hence, in order to satisfy $e = \epl$ considering that $h_L = T / 2^L$ and \eqref{hLeps} we can impose
\begin{equation}\label{LCaseOne}
	L = \left|\log_2\epl^{1/q}\right|,
\end{equation} 
and therefore the cost can be expressed as 
\begin{equation}
	\mathrm{cost} = \OO\left(\left|\log_2 \epl^{1/q}\right| \epl^{-2}\right).
\end{equation}

\subsubsection*{Case 2: $q \geq 2p$}
In this case, $\min\{p, q\} = p$ and $\min\{2p, q\} = 2p$. Therefore
\begin{equation}
\begin{aligned}
\mathrm{cost} &=  \sum_{l=0}^L \frac{T}{h_l} \frac{h_l^{2p}L}{h_L^{4p}} = \frac{TL}{h_L^{2p+1}} \sum_{l=0}^L \left(\frac{h_l}{h_L}\right)^{2p-1} \\
&= \frac{TL}{h_L^{2p+1}} \sum_{l=0}^L 2^{(L-l)(2p-1)} = \frac{TL}{h_L^{2p+1}} 2^{L(2p-1)} \sum_{l=0}^L 2^{-l(2p-1)} \\
&\leq \frac{L2^{2pL}}{h_L^{2p}}\frac{1}{1 - 2^{1-2q}} = \OO\left(L h_L^{-4p}\right),
\end{aligned}
\end{equation}
Hence, in view of \eqref{hLeps} we impose as before 
\begin{equation}
	L = \left|\log_2 \epl^{1/2p}\right|,
\end{equation}
therefore the final expression of the cost is
\begin{equation}
	\mathrm{cost} = \OO\left(\left|\log_2 \epl^{1/2p}\right| \epl^{-2}\right).
\end{equation}

\subsubsection*{Case 3: $p < q \leq 2p$}
In this case, $\min\{p, q\} = p$ and $\min\{2p, q\} = q$. Therefore
\begin{equation}
\begin{aligned}
\mathrm{cost} &=  \sum_{l=0}^L \frac{T}{h_l} \frac{h_l^{2p}L}{h_L^{2q}} = \frac{TL}{h_L^{2q - 2p+1}} \sum_{l=0}^L \left(\frac{h_l}{h_L}\right)^{2p-1} \\
&= \frac{TL}{h_L^{2q-2p+1}} \sum_{l=0}^L 2^{(L-l)(2p-1)} = \frac{TL}{h_L^{2q-2p+1}} 2^{L(2p-1)} \sum_{l=0}^L 2^{-l(2p-1)} \\
&\leq \frac{L2^{2pL}}{h_L^{2q-2p}}\frac{1}{1 - 2^{1-2q}} = \OO\left(L h_L^{2p-2q-2p}\right) = \OO\left(L h_L^{-2q}\right).
\end{aligned}
\end{equation}
Hence the number of levels is given by
\begin{equation}
	L = \left|\log_2 \epl^{1/q} \right|,
\end{equation}
and the computational cost is given by
\begin{equation}
\mathrm{cost} = \OO\left(\left|\log_2 \epl^{1/q}\right| \epl^{-2}\right).
\end{equation}
Let us remark that in practice the method \eqref{probabilityODE} is tuned so that $p = q$, as in this case neither the strong or the weak order are spoiled by the noise added to the model. Therefore, the first of the three cases presented above is of the highest interest. The plot in Figure \ref{fig:MLMCtheory} shows that in this case MLMC is particularly favorable with respect to Monte Carlo when the integrator has an order $q$ which is small (e.g., $q = 1$).

%\begin{figure}
%	\centering
%	\resizebox{0.6\linewidth}{!}{\input{plots/MLMCtheorycost.tikz}}
%	\caption{Theoretical cost as a function of the desired accuracy and of the order $q$ of the numerical integrator if Monte Carlo or MLMC are applied.}
%	\label{fig:MLMCtheory}
%\end{figure}

\subsubsection{Numerical example}
We consider the Fitzhug-Nagumo problem \eqref{eq:FitzNag} and we aim to verify the cost of MLMC with respect to standard Monte Carlo for the estimation of the expectation of the solution at final time when applying the numerical method \eqref{probabilityODE}. We consider the case $q = p = 1$, using as a deterministic integrator the explicit Euler method. Hence, once a value of accuracy $\epl$ is requested, the number of stages $L$ as well as the time steps $h_l, l = 0, \ldots, L$, are imposed using \eqref{LCaseOne} and \eqref{hLeps}. In order to set up the standard Monte Carlo method, we consider the cost obtained in the MLMC simulation, denote by $\hat C$ and impose it to be equal for the standard Monte Carlo. In order to obtain a good balance between the error terms in \eqref{MSEMC} we impose
\begin{equation}
\begin{aligned}
	\frac{T}{h} M &= \hat C, \\
	M &= \ceil{h^{-2q}},
\end{aligned}
\end{equation}
thus obtaining for the time step
\begin{equation}
	h = \left(\frac{T}{\hat C}\right)^{1 / (2q + 1)}.
\end{equation}
In this way, the computational cost for MLMC and standard Monte Carlo are imposed to be artificially equal and the two methods can be compared for their weak error with respect to an accurate solution. We impose for MLMC four values of accuracy $\epl = 0.1, 0.01, 0.001, 0.0001$, and apply the aforementioned technique to compare MLMC and Monte Carlo. Results (Figure \ref{fig:MLMCpractice}) show that imposing $L$ and $h_l, l = 0, \ldots, L$ as above the obtained accuracy in the same order of magnitude as $\epl$. Furthermore, the obtained accuracy is smaller for MLMC than MC if the cost 

\begin{figure}
	\centering
	\resizebox{0.6\linewidth}{!}{\input{plots/MLMCpracticecost.tikz}}
	\caption{Accuracy of MLMC and standard Monte Carlo for the FitzHug-Nagumo problem with fixed cost.}
	\label{fig:MLMCpractice}
\end{figure}