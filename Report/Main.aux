\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{CGS16}
\citation{CGS16}
\citation{CGS16}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Bayesian statistics and Markov chain Monte Carlo}{2}{section.2}}
\newlabel{sec:ONE}{{2}{2}{Bayesian statistics and Markov chain Monte Carlo}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Bayes' formula}{2}{subsection.2.1}}
\newlabel{eq:BayesRule}{{1}{2}{Bayes' formula}{equation.2.1}{}}
\MT@newlabel{eq:BayesRule}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Parametrized models}{3}{subsection.2.2}}
\newlabel{eq:GaussianNoise}{{2}{3}{Parametrized models}{equation.2.2}{}}
\MT@newlabel{eq:GaussianNoise}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}An example: parametrized differential equations}{4}{subsubsection.2.2.1}}
\newlabel{sect:exBayes}{{2.2.1}{4}{An example: parametrized differential equations}{subsubsection.2.2.1}{}}
\newlabel{eq:exSDE}{{3}{4}{An example: parametrized differential equations}{equation.2.3}{}}
\MT@newlabel{eq:exSDE}
\MT@newlabel{eq:exSDE}
\citation{Gil05}
\citation{KaS05}
\citation{KaS05}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Markov chain Monte Carlo methods}{5}{subsection.2.3}}
\newlabel{eq:MonteCarloMCMC}{{4}{5}{Markov chain Monte Carlo methods}{equation.2.4}{}}
\MT@newlabel{eq:MonteCarloMCMC}
\citation{MLR16}
\citation{KaS05}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Metropolis-Hastings.\relax }}{6}{algocf.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:MH}{{1}{6}{Metropolis-Hastings algorithm}{algocf.1}{}}
\MT@newlabel{eq:MHalpha}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Metropolis-Hastings algorithm}{6}{subsubsection.2.3.1}}
\newlabel{eq:MHalpha}{{5}{6}{Metropolis-Hastings algorithm}{equation.2.5}{}}
\citation{Vih12}
\citation{Vih12}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Robust adaptive Metropolis.\relax }}{7}{algocf.2}}
\newlabel{alg:RAM}{{2}{7}{An adaptive approach}{algocf.2}{}}
\MT@newlabel{eq:MHalpha}
\MT@newlabel{eq:RAMupdate}
\newlabel{eq:MHalphasym}{{6}{7}{Metropolis-Hastings algorithm}{equation.2.6}{}}
\newlabel{eq:gaussianProp}{{7}{7}{Metropolis-Hastings algorithm}{equation.2.7}{}}
\MT@newlabel{eq:MHalphasym}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}An adaptive approach}{7}{subsubsection.2.3.2}}
\citation{Vih12}
\citation{Vih12}
\citation{KaS05}
\MT@newlabel{eq:gaussianProp}
\newlabel{eq:RAMupdate}{{8}{8}{An adaptive approach}{equation.2.8}{}}
\newlabel{eq:RAMtestPi}{{9}{8}{An adaptive approach}{equation.2.9}{}}
\MT@newlabel{eq:RAMtestPi}
\citation{DPD15}
\citation{ADH10}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Samples produced by MH and RAM for the distribution \MT_extended_eqref:n  {eq:RAMtestPi}. The contour lines of the density function are plotted for all the sets of results. In the first row we show the results obtained with MH for a normal update with covariance $\Sigma = \sigma ^2 I $ with $\sigma = \{0.01, 0.5, 2.0\}$ from left to right. In the second row we show the results obtained with RAM with the same values of $\Sigma $ as an initial guess of the covariance structure. \relax }}{9}{figure.caption.6}}
\MT@newlabel{eq:RAMtestPi}
\newlabel{fig:RAMexample}{{1}{9}{Samples produced by MH and RAM for the distribution \eqref {eq:RAMtestPi}. The contour lines of the density function are plotted for all the sets of results. In the first row we show the results obtained with MH for a normal update with covariance $\Sigma = \sigma ^2 I $ with $\sigma = \{0.01, 0.5, 2.0\}$ from left to right. In the second row we show the results obtained with RAM with the same values of $\Sigma $ as an initial guess of the covariance structure. \relax }{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Acceptance ratios for MH and RAM with posterior distribution \MT_extended_eqref:n  {eq:RAMtestPi}\relax }}{9}{table.caption.5}}
\MT@newlabel{eq:RAMtestPi}
\newlabel{tab:RAMalphaStar}{{1}{9}{Acceptance ratios for MH and RAM with posterior distribution \eqref {eq:RAMtestPi}\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Pseudo-marginal Metropolis-Hastings}{9}{subsubsection.2.3.3}}
\newlabel{sec:MCWM}{{2.3.3}{9}{Pseudo-marginal Metropolis-Hastings}{subsubsection.2.3.3}{}}
\citation{AnR09}
\citation{MLR16}
\citation{AnR09}
\citation{MLR16}
\citation{AnR09}
\citation{MLR16}
\citation{MLR16}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Monte Carlo within Metropolis.\relax }}{10}{algocf.3}}
\newlabel{alg:MCWM}{{3}{10}{Pseudo-marginal Metropolis-Hastings}{algocf.3}{}}
\MT@newlabel{eq:MCWMestimators}
\MT@newlabel{eq:MCWMalpha}
\newlabel{eq:MCWMestimators}{{10}{10}{Pseudo-marginal Metropolis-Hastings}{equation.2.10}{}}
\newlabel{eq:MCWMalpha}{{11}{10}{Pseudo-marginal Metropolis-Hastings}{equation.2.11}{}}
\MT@newlabel{eq:MCWMalpha}
\citation{KPS94}
\MT@newlabel{eq:MCWMestimators}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}How to deal with inadmissible parameter values}{11}{subsubsection.2.3.4}}
\newlabel{eq:truncGauss}{{12}{11}{How to deal with inadmissible parameter values}{equation.2.12}{}}
\MT@newlabel{eq:truncGauss}
\citation{GeS11}
\citation{GeS11}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Monitoring convergence.\relax }}{12}{algocf.4}}
\newlabel{alg:Convergence}{{4}{12}{Monitoring convergence}{algocf.4}{}}
\MT@newlabel{eq:RhoMCMC}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.5}Monitoring convergence}{12}{subsubsection.2.3.5}}
\newlabel{eq:RhoMCMC}{{13}{12}{Monitoring convergence}{equation.2.13}{}}
\citation{CGS16}
\citation{CGS16}
\citation{HLW02}
\citation{HaW96}
\@writefile{toc}{\contentsline {section}{\numberline {3}Probabilistic Methods for Ordinary Differential Equations}{14}{section.3}}
\newlabel{sec:TWO}{{3}{14}{Probabilistic Methods for Ordinary Differential Equations}{section.3}{}}
\newlabel{eq:ODE}{{14}{14}{Probabilistic Methods for Ordinary Differential Equations}{equation.3.14}{}}
\newlabel{numericalODE}{{15}{14}{Probabilistic Methods for Ordinary Differential Equations}{equation.3.15}{}}
\newlabel{eq:probMethod}{{16}{14}{Probabilistic Methods for Ordinary Differential Equations}{equation.3.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Deterministic methods}{15}{subsection.3.1}}
\newlabel{def:RK}{{3.1}{15}{}{definition.3.1}{}}
\MT@newlabel{eq:ODE}
\MT@newlabel{eq:ODE}
\citation{HLW02}
\MT@newlabel{eq:ODE}
\MT@newlabel{numericalODE}
\MT@newlabel{eq:ODE}
\MT@newlabel{eq:probMethod}
\citation{CGS16}
\citation{CGS16}
\citation{KeH16}
\citation{KeH16}
\citation{CGS16}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Motivation of probabilistic methods}{17}{subsection.3.2}}
\newlabel{eq:Lorenz}{{3.2}{17}{Motivation of probabilistic methods}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Method properties}{17}{subsection.3.3}}
\MT@newlabel{eq:probMethod}
\citation{CGS16}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Trajectory of the solution of the Lorenz system in the state space.\relax }}{18}{figure.caption.9}}
\newlabel{fig:LorenzTraj}{{2}{18}{Trajectory of the solution of the Lorenz system in the state space.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Strong convergence}{18}{subsubsection.3.3.1}}
\MT@newlabel{eq:probMethod}
\newlabel{assumption_1}{{3.1}{18}{}{assumption.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Solution of the Lorenz system obtained with the deterministic solver (thick black) and realizations of the solution obtained with the probabilistic solver (light gray).\relax }}{19}{figure.caption.10}}
\newlabel{fig:Lorenz}{{3}{19}{Solution of the Lorenz system obtained with the deterministic solver (thick black) and realizations of the solution obtained with the probabilistic solver (light gray).\relax }{figure.caption.10}{}}
\MT@newlabel{eq:probMethod}
\newlabel{assumption_2}{{3.2}{19}{}{assumption.3.2}{}}
\newlabel{thm:Gronwall}{{3.1}{20}{Discrete Gronwall Lemma}{lemma.3.1}{}}
\newlabel{thm:strongConv}{{3.1}{20}{Strong Convergence}{theorem.3.1}{}}
\newlabel{strongConvDisc}{{17}{20}{Strong Convergence}{equation.3.17}{}}
\newlabel{strongConvCont}{{3.1}{20}{Strong Convergence}{equation.3.17}{}}
\MT@newlabel{eq:probMethod}
\MT@newlabel{eq:ODE}
\citation{CGS16}
\MT@newlabel{strongConvDisc}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Weak convergence}{21}{subsubsection.3.3.2}}
\MT@newlabel{eq:probMethod}
\MT@newlabel{eq:ODE}
\MT@newlabel{eq:probMethod}
\newlabel{LieNotation}{{18}{21}{Weak convergence}{equation.3.18}{}}
\newlabel{modifiedODE}{{3.3.2}{21}{Weak convergence}{equation.3.18}{}}
\citation{CGS16}
\citation{CGS16}
\newlabel{modifiedSDE}{{19}{22}{Weak convergence}{equation.3.19}{}}
\MT@newlabel{LieNotation}
\newlabel{LieNotationModif}{{3.3.2}{22}{Weak convergence}{equation.3.19}{}}
\MT@newlabel{modifiedSDE}
\newlabel{assumption_3}{{3.3}{22}{}{assumption.3.3}{}}
\MT@newlabel{eq:ODE}
\newlabel{thm:weakorder}{{3.2}{22}{}{theorem.3.2}{}}
\MT@newlabel{eq:probMethod}
\MT@newlabel{eq:ODE}
\MT@newlabel{modifiedSDE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Numerical experiment - Weak convergence}{22}{subsubsection.3.3.3}}
\newlabel{eq:FitzNag}{{20}{22}{Numerical experiment - Weak convergence}{equation.3.20}{}}
\MT@newlabel{eq:probMethod}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Weak order of convergence of \MT_extended_eqref:n  {eq:probMethod} applied to \MT_extended_eqref:n  {eq:FitzNag}.\relax }}{23}{figure.caption.11}}
\MT@newlabel{eq:probMethod}
\MT@newlabel{eq:FitzNag}
\newlabel{fig:weakorder}{{4}{23}{Weak order of convergence of \eqref {eq:probMethod} applied to \eqref {eq:FitzNag}.\relax }{figure.caption.11}{}}
\MT@newlabel{eq:probMethod}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Monte Carlo approximation}{23}{subsubsection.3.3.4}}
\newlabel{sec:MonteCarlo}{{3.3.4}{23}{Monte Carlo approximation}{subsubsection.3.3.4}{}}
\MT@newlabel{eq:probMethod}
\newlabel{eq:MCapproximation}{{21}{23}{Monte Carlo approximation}{equation.3.21}{}}
\newlabel{eq:MCMSE}{{3.3.4}{23}{Monte Carlo approximation}{equation.3.21}{}}
\newlabel{lem:varMC}{{3.2}{24}{}{lemma.3.2}{}}
\MT@newlabel{eq:probMethod}
\newlabel{eq:indepProof}{{22}{24}{Monte Carlo approximation}{equation.3.22}{}}
\newlabel{eq:varProp}{{23}{24}{Monte Carlo approximation}{equation.3.23}{}}
\newlabel{eq:lipschitzVar}{{24}{24}{Monte Carlo approximation}{equation.3.24}{}}
\newlabel{eq:boundOfSum}{{25}{25}{Monte Carlo approximation}{equation.3.25}{}}
\MT@newlabel{eq:probMethod}
\MT@newlabel{eq:varProp}
\newlabel{eq:temp}{{26}{25}{Monte Carlo approximation}{equation.3.26}{}}
\MT@newlabel{eq:temp}
\newlabel{lem:varimRK}{{3.3}{26}{}{lemma.3.3}{}}
\MT@newlabel{eq:probMethod}
\MT@newlabel{eq:indepProof}
\MT@newlabel{eq:lipschitzVar}
\MT@newlabel{eq:varProp}
\MT@newlabel{eq:lipschitzVar}
\newlabel{eq:partialLemRKI}{{27}{27}{Monte Carlo approximation}{equation.3.27}{}}
\MT@newlabel{eq:varProp}
\MT@newlabel{eq:lipschitzVar}
\MT@newlabel{eq:varProp}
\MT@newlabel{eq:partialLemRKI}
\MT@newlabel{eq:boundOfSum}
\MT@newlabel{eq:MCapproximation}
\newlabel{prop:MSE}{{3.3}{28}{}{theorem.3.3}{}}
\citation{CGS16}
\MT@newlabel{eq:lipschitzVar}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.5}Numerical experiment - Monte Carlo}{29}{subsubsection.3.3.5}}
\MT@newlabel{eq:FitzNag}
\newlabel{fig:MonteCarloVarianceH}{{5a}{30}{Variation of the time step.\relax }{figure.caption.12}{}}
\newlabel{sub@fig:MonteCarloVarianceH}{{a}{30}{Variation of the time step.\relax }{figure.caption.12}{}}
\newlabel{fig:MonteCarloVarianceM}{{5b}{30}{Variation of the number of trajectories.\relax }{figure.caption.12}{}}
\newlabel{sub@fig:MonteCarloVarianceM}{{b}{30}{Variation of the number of trajectories.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Variance and squared bias of the Monte Carlo estimator $\mathaccentV {hat}05EZ$ with Explicit Euler and RK4 applied to \MT_extended_eqref:n  {eq:FitzNag}. The two components of the MSE have the same order of convergence with respect to the time step $h$. Conversely, the order of convergence with respect to the number of trajectories $M$ with fixed $h$ of the variance of $\mathaccentV {hat}05EZ$ is equal to one for both methods\relax }}{30}{figure.caption.12}}
\MT@newlabel{eq:FitzNag}
\newlabel{fig:MonteCarloVariance}{{5}{30}{Variance and squared bias of the Monte Carlo estimator $\hat Z$ with Explicit Euler and RK4 applied to \eqref {eq:FitzNag}. The two components of the MSE have the same order of convergence with respect to the time step $h$. Conversely, the order of convergence with respect to the number of trajectories $M$ with fixed $h$ of the variance of $\hat Z$ is equal to one for both methods\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.6}Multi-level Monte Carlo}{30}{subsubsection.3.3.6}}
\MT@newlabel{eq:MCapproximation}
\citation{Gil08}
\MT@newlabel{eq:varProp}
\newlabel{hLeps}{{28}{32}{Multi-level Monte Carlo}{equation.3.28}{}}
\MT@newlabel{hLeps}
\newlabel{LCaseOne}{{3.3.6}{33}{Case 1: $q \leq p$}{section*.13}{}}
\MT@newlabel{hLeps}
\citation{HeS92}
\citation{HaW96}
\MT@newlabel{eq:probMethod}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.7}Stability analysis}{34}{subsubsection.3.3.7}}
\citation{ADH10}
\citation{DPD15}
\citation{PSG12}
\@writefile{toc}{\contentsline {section}{\numberline {4}Bayesian inference of the parameters of an ODE}{35}{section.4}}
\newlabel{sec:THREE}{{4}{35}{Bayesian inference of the parameters of an ODE}{section.4}{}}
\newlabel{eq:ODEParam}{{29}{35}{Bayesian inference of the parameters of an ODE}{equation.4.29}{}}
\MT@newlabel{eq:ODEParam}
\MT@newlabel{eq:ODEParam}
\newlabel{eq:BayesODE}{{30}{35}{Bayesian inference of the parameters of an ODE}{equation.4.30}{}}
\newlabel{eq:likelihood}{{4}{35}{Bayesian inference of the parameters of an ODE}{equation.4.30}{}}
\MT@newlabel{eq:ODEParam}
\MT@newlabel{eq:BayesODE}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Approximation of the likelihood}{35}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Approximation of the likelihood at a fixed value $\mathaccentV {bar}016\theta $.\relax }}{36}{figure.caption.16}}
\newlabel{fig:LikelihoodApprox}{{6}{36}{Approximation of the likelihood at a fixed value $\bar \theta $.\relax }{figure.caption.16}{}}
\newlabel{eq:likMSE}{{31}{36}{Approximation of the likelihood}{equation.4.31}{}}
\MT@newlabel{eq:probMethod}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Numerical experiment - Likelihood}{36}{subsubsection.4.1.1}}
\MT@newlabel{eq:FitzNag}
\MT@newlabel{eq:likMSE}
\citation{GiS02}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Numerical experiment - Posterior distributions}{37}{subsection.4.2}}
\MT@newlabel{eq:FitzNag}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Convergence of the posterior distribution}{37}{subsection.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Posterior distribution for the parameter $\theta $ defining the FitzHug-Nagumo model. The posterior distributions given by the probabilistic and the deterministic solvers are displayed in blue and red respectively. The true value of the parameters is displayed in thick green dots.\relax }}{38}{figure.caption.17}}
\newlabel{fig:MCMC_FHN}{{7}{38}{Posterior distribution for the parameter $\theta $ defining the FitzHug-Nagumo model. The posterior distributions given by the probabilistic and the deterministic solvers are displayed in blue and red respectively. The true value of the parameters is displayed in thick green dots.\relax }{figure.caption.17}{}}
\citation{GiS02}
\newlabel{eq:TVvsHell}{{32}{39}{Convergence of the posterior distribution}{equation.4.32}{}}
\MT@newlabel{eq:TVvsHell}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Convergence of the Monte Carlo approximation}{40}{subsection.4.4}}
\newlabel{sec:ParH}{{4.4}{40}{Convergence of the Monte Carlo approximation}{subsection.4.4}{}}
\citation{Jon04}
\citation{FlJ10}
\citation{FlJ10}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Considerations on the MCMC approximation}{41}{subsection.4.5}}
\newlabel{eq:CLTMarkov}{{33}{41}{Considerations on the MCMC approximation}{equation.4.33}{}}
\citation{FlJ10}
\MT@newlabel{eq:CLTMarkov}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Numerical experiment - Batch means estimator}{42}{subsubsection.4.5.1}}
\MT@newlabel{eq:FitzNag}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Numerical experiment - Convergence of the posterior distribution}{42}{subsection.4.6}}
\MT@newlabel{eq:FitzNag}
\citation{HaW96}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Variance of the Monte Carlo approximation given by MCMC with respect to the number of samples $N$. The convergence to zero is linear with respect to the number of samples. The estimator given by the batch means method converges to the true value of the variance with a quadratic order.\relax }}{43}{figure.caption.18}}
\newlabel{fig:BatchMeans}{{8}{43}{Variance of the Monte Carlo approximation given by MCMC with respect to the number of samples $N$. The convergence to zero is linear with respect to the number of samples. The estimator given by the batch means method converges to the true value of the variance with a quadratic order.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Numerical example - Brussellator}{43}{subsection.4.7}}
\newlabel{eq:BRUSS}{{4.7}{43}{Numerical example - Brussellator}{subsection.4.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Convergence of the parameter to its stationary value and of the Hellinger distance of the probability distributions.\relax }}{44}{figure.caption.19}}
\newlabel{fig:ConvergenceMCMC}{{9}{44}{Convergence of the parameter to its stationary value and of the Hellinger distance of the probability distributions.\relax }{figure.caption.19}{}}
\bibstyle{siam}
\bibdata{anmc.bib}
\bibcite{ADH10}{1}
\bibcite{AnR09}{2}
\bibcite{CGS16}{3}
\bibcite{DPD15}{4}
\bibcite{FlJ10}{5}
\bibcite{GeS11}{6}
\bibcite{GiS02}{7}
\bibcite{Gil08}{8}
\bibcite{Gil05}{9}
\bibcite{HLW02}{10}
\bibcite{HaW96}{11}
\bibcite{HeS92}{12}
\bibcite{Jon04}{13}
\bibcite{KaS05}{14}
\bibcite{KeH16}{15}
\bibcite{KPS94}{16}
\bibcite{MLR16}{17}
\bibcite{PSG12}{18}
\bibcite{Vih12}{19}
