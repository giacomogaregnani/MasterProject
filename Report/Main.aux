\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Bayesian statistics and Markov chain Monte Carlo}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Bayes' formula}{1}{subsection.1.1}}
\newlabel{eq:BayesRule}{{1}{1}{Bayes' formula}{equation.1.1}{}}
\MT@newlabel{eq:BayesRule}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Parametrized models}{1}{subsection.1.2}}
\newlabel{eq:GaussianNoise}{{2}{2}{Parametrized models}{equation.1.2}{}}
\MT@newlabel{eq:GaussianNoise}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}An example: parametrized differential equations}{2}{subsubsection.1.2.1}}
\newlabel{sect:exBayes}{{1.2.1}{2}{An example: parametrized differential equations}{subsubsection.1.2.1}{}}
\newlabel{eq:exSDE}{{3}{2}{An example: parametrized differential equations}{equation.1.3}{}}
\citation{Gil05}
\citation{KaS05}
\MT@newlabel{eq:exSDE}
\MT@newlabel{eq:exSDE}
\citation{KaS05}
\citation{MLR16}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Markov chain Monte Carlo methods}{4}{subsection.1.3}}
\newlabel{eq:MonteCarloMCMC}{{4}{4}{Markov chain Monte Carlo methods}{equation.1.4}{}}
\MT@newlabel{eq:MonteCarloMCMC}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Metropolis-Hastings algorithm}{4}{subsubsection.1.3.1}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Metropolis-Hastings.\relax }}{5}{algocf.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:MH}{{1}{5}{Metropolis-Hastings algorithm}{algocf.1}{}}
\MT@newlabel{eq:MHalpha}
\newlabel{eq:MHalpha}{{5}{5}{Metropolis-Hastings algorithm}{equation.1.5}{}}
\newlabel{eq:MHalphasym}{{6}{5}{Metropolis-Hastings algorithm}{equation.1.6}{}}
\newlabel{eq:gaussianProp}{{7}{5}{Metropolis-Hastings algorithm}{equation.1.7}{}}
\MT@newlabel{eq:MHalphasym}
\citation{Vih12}
\citation{Vih12}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Robust adaptive Metropolis.\relax }}{6}{algocf.2}}
\newlabel{alg:RAM}{{2}{6}{An adaptive approach}{algocf.2}{}}
\MT@newlabel{eq:MHalpha}
\MT@newlabel{eq:RAMupdate}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}An adaptive approach}{6}{subsubsection.1.3.2}}
\MT@newlabel{eq:gaussianProp}
\newlabel{eq:RAMupdate}{{8}{6}{An adaptive approach}{equation.1.8}{}}
\citation{DPD15}
\citation{ADH10}
\newlabel{eq:RAMtestPi}{{9}{7}{An adaptive approach}{equation.1.9}{}}
\MT@newlabel{eq:RAMtestPi}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Acceptance ratios for MH and RAM with posterior distribution \MT_extended_eqref:n  {eq:RAMtestPi}\relax }}{7}{table.caption.4}}
\MT@newlabel{eq:RAMtestPi}
\newlabel{tab:RAMalphaStar}{{1}{7}{Acceptance ratios for MH and RAM with posterior distribution \eqref {eq:RAMtestPi}\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Pseudo-marginal Metropolis-Hastings}{7}{subsubsection.1.3.3}}
\newlabel{sec:MCWM}{{1.3.3}{7}{Pseudo-marginal Metropolis-Hastings}{subsubsection.1.3.3}{}}
\citation{AnR09}
\citation{MLR16}
\citation{AnR09}
\citation{MLR16}
\citation{AnR09}
\citation{MLR16}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Samples produced by MH and RAM for the distribution \MT_extended_eqref:n  {eq:RAMtestPi}. The contour lines of the density function are plotted for all the sets of results. In the first row we show the results obtained with MH for a normal update with covariance $\Sigma = \sigma ^2 I $ with $\sigma = \{0.01, 0.5, 2.0\}$ from left to right. In the second row we show the results obtained with RAM with the same values of $\Sigma $ as an initial guess of the covariance structure. \relax }}{8}{figure.caption.5}}
\MT@newlabel{eq:RAMtestPi}
\newlabel{fig:RAMexample}{{1}{8}{Samples produced by MH and RAM for the distribution \eqref {eq:RAMtestPi}. The contour lines of the density function are plotted for all the sets of results. In the first row we show the results obtained with MH for a normal update with covariance $\Sigma = \sigma ^2 I $ with $\sigma = \{0.01, 0.5, 2.0\}$ from left to right. In the second row we show the results obtained with RAM with the same values of $\Sigma $ as an initial guess of the covariance structure. \relax }{figure.caption.5}{}}
\newlabel{eq:MCWMestimators}{{10}{8}{Pseudo-marginal Metropolis-Hastings}{equation.1.10}{}}
\newlabel{eq:MCWMalpha}{{11}{8}{Pseudo-marginal Metropolis-Hastings}{equation.1.11}{}}
\MT@newlabel{eq:MCWMalpha}
\citation{MLR16}
\citation{KPS94}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Monte Carlo within Metropolis.\relax }}{9}{algocf.3}}
\newlabel{alg:MCWM}{{3}{9}{Pseudo-marginal Metropolis-Hastings}{algocf.3}{}}
\MT@newlabel{eq:MCWMestimators}
\MT@newlabel{eq:MCWMalpha}
\MT@newlabel{eq:MCWMestimators}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.4}How to deal with inadmissible parameter values}{9}{subsubsection.1.3.4}}
\newlabel{eq:truncGauss}{{12}{9}{How to deal with inadmissible parameter values}{equation.1.12}{}}
\MT@newlabel{eq:truncGauss}
\citation{GeS11}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Monitoring convergence.\relax }}{10}{algocf.4}}
\newlabel{alg:Convergence}{{4}{10}{Monitoring convergence}{algocf.4}{}}
\MT@newlabel{eq:RhoMCMC}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.5}Monitoring convergence}{10}{subsubsection.1.3.5}}
\citation{GeS11}
\newlabel{eq:RhoMCMC}{{13}{11}{Monitoring convergence}{equation.1.13}{}}
\citation{CGS16}
\@writefile{toc}{\contentsline {section}{\numberline {2}Probabilistic Methods for Ordinary Differential Equations}{12}{section.2}}
\newlabel{ODE}{{14}{12}{Probabilistic Methods for Ordinary Differential Equations}{equation.2.14}{}}
\MT@newlabel{ODE}
\newlabel{numericalODE}{{2}{12}{Probabilistic Methods for Ordinary Differential Equations}{equation.2.14}{}}
\MT@newlabel{ODE}
\newlabel{probabilityODE}{{15}{12}{Probabilistic Methods for Ordinary Differential Equations}{equation.2.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Deterministic methods}{12}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Motivation}{12}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Numerical example}{12}{subsubsection.2.2.1}}
\newlabel{eq:Lorenz}{{16}{12}{Numerical example}{equation.2.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Trajectory of the solution of the Lorenz system in the state space.\relax }}{13}{figure.caption.8}}
\newlabel{fig:LorenzTraj}{{2}{13}{Trajectory of the solution of the Lorenz system in the state space.\relax }{figure.caption.8}{}}
\MT@newlabel{eq:Lorenz}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Method properties}{13}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Strong convergence}{13}{subsubsection.2.3.1}}
\MT@newlabel{probabilityODE}
\newlabel{thm:Gronwall}{{2.1}{13}{Discrete Gronwall Lemma}{theorem.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Solution of the Lorenz system obtained with the deterministic solver (thick black) and realizations of the solution obtained with the probabilistic solver (light gray).\relax }}{14}{figure.caption.9}}
\newlabel{fig:Lorenz}{{3}{14}{Solution of the Lorenz system obtained with the deterministic solver (thick black) and realizations of the solution obtained with the probabilistic solver (light gray).\relax }{figure.caption.9}{}}
\newlabel{assumption_1}{{2.1}{14}{}{assumption.2.1}{}}
\MT@newlabel{probabilityODE}
\newlabel{assumption_2}{{2.2}{15}{}{assumption.2.2}{}}
\newlabel{thm:strongConv}{{2.2}{15}{Strong Convergence}{theorem.2.2}{}}
\newlabel{strongConvDisc}{{2.2}{15}{Strong Convergence}{theorem.2.2}{}}
\newlabel{strongConvCont}{{2.2}{15}{Strong Convergence}{theorem.2.2}{}}
\MT@newlabel{probabilityODE}
\MT@newlabel{ODE}
\citation{CGS16}
\citation{CGS16}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Weak convergence}{16}{subsubsection.2.3.2}}
\MT@newlabel{ODE}
\MT@newlabel{probabilityODE}
\newlabel{LieNotation}{{17}{16}{Weak convergence}{equation.2.17}{}}
\newlabel{modifiedODE}{{2.3.2}{16}{Weak convergence}{equation.2.17}{}}
\newlabel{modifiedSDE}{{18}{16}{Weak convergence}{equation.2.18}{}}
\MT@newlabel{LieNotation}
\newlabel{LieNotationModif}{{2.3.2}{16}{Weak convergence}{equation.2.18}{}}
\MT@newlabel{modifiedSDE}
\newlabel{assumption_3}{{2.3}{16}{}{assumption.2.3}{}}
\MT@newlabel{ODE}
\newlabel{thm:weakorder}{{2.3}{16}{}{theorem.2.3}{}}
\MT@newlabel{probabilityODE}
\MT@newlabel{ODE}
\MT@newlabel{modifiedSDE}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Weak order of convergence of \MT_extended_eqref:n  {probabilityODE} applied to \MT_extended_eqref:n  {eq:FitzNag}.\relax }}{17}{figure.caption.10}}
\MT@newlabel{probabilityODE}
\MT@newlabel{eq:FitzNag}
\newlabel{fig:weakorder}{{4}{17}{Weak order of convergence of \eqref {probabilityODE} applied to \eqref {eq:FitzNag}.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Numerical verification of weak order}{17}{subsubsection.2.3.3}}
\MT@newlabel{ODE}
\newlabel{eq:FitzNag}{{19}{17}{Numerical verification of weak order}{equation.2.19}{}}
\MT@newlabel{probabilityODE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}Monte Carlo approximation}{17}{subsubsection.2.3.4}}
\MT@newlabel{probabilityODE}
\newlabel{eq:MCapproximation}{{20}{17}{Monte Carlo approximation}{equation.2.20}{}}
\newlabel{eq:MCMSE}{{2.3.4}{17}{Monte Carlo approximation}{equation.2.20}{}}
\newlabel{lem:varMC}{{2.1}{18}{}{lemma.2.1}{}}
\MT@newlabel{probabilityODE}
\newlabel{eq:indepProof}{{21}{18}{Monte Carlo approximation}{equation.2.21}{}}
\newlabel{eq:varProp}{{22}{18}{Monte Carlo approximation}{equation.2.22}{}}
\newlabel{eq:lipschitzVar}{{23}{18}{Monte Carlo approximation}{equation.2.23}{}}
\newlabel{eq:boundOfSum}{{24}{19}{Monte Carlo approximation}{equation.2.24}{}}
\MT@newlabel{probabilityODE}
\MT@newlabel{eq:varProp}
\newlabel{eq:temp}{{25}{19}{Monte Carlo approximation}{equation.2.25}{}}
\MT@newlabel{eq:temp}
\newlabel{lem:varimRK}{{2.2}{20}{}{lemma.2.2}{}}
\MT@newlabel{probabilityODE}
\MT@newlabel{eq:indepProof}
\MT@newlabel{eq:lipschitzVar}
\MT@newlabel{eq:varProp}
\MT@newlabel{eq:lipschitzVar}
\newlabel{eq:partialLemRKI}{{26}{20}{Monte Carlo approximation}{equation.2.26}{}}
\MT@newlabel{eq:varProp}
\MT@newlabel{eq:lipschitzVar}
\MT@newlabel{eq:varProp}
\MT@newlabel{eq:partialLemRKI}
\MT@newlabel{eq:boundOfSum}
\citation{CGS16}
\MT@newlabel{eq:MCapproximation}
\newlabel{prop:MSE}{{2.4}{22}{}{theorem.2.4}{}}
\MT@newlabel{eq:lipschitzVar}
\newlabel{fig:MonteCarloVarianceH}{{5a}{23}{Variation of the time step.\relax }{figure.caption.11}{}}
\newlabel{sub@fig:MonteCarloVarianceH}{{a}{23}{Variation of the time step.\relax }{figure.caption.11}{}}
\newlabel{fig:MonteCarloVarianceM}{{5b}{23}{Variation of the number of trajectories.\relax }{figure.caption.11}{}}
\newlabel{sub@fig:MonteCarloVarianceM}{{b}{23}{Variation of the number of trajectories.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Variance and squared bias of the Monte Carlo estimator $\mathaccentV {hat}05EZ$ with Explicit Euler and RK4 applied to \MT_extended_eqref:n  {eq:FitzNag}. The two components of the MSE have the same order of convergence with respect to the time step $h$. Conversely, the order of convergence with respect to the number of trajectories $M$ with fixed $h$ of the variance of $\mathaccentV {hat}05EZ$ is equal to one for both methods\relax }}{23}{figure.caption.11}}
\MT@newlabel{eq:FitzNag}
\newlabel{fig:MonteCarloVariance}{{5}{23}{Variance and squared bias of the Monte Carlo estimator $\hat Z$ with Explicit Euler and RK4 applied to \eqref {eq:FitzNag}. The two components of the MSE have the same order of convergence with respect to the time step $h$. Conversely, the order of convergence with respect to the number of trajectories $M$ with fixed $h$ of the variance of $\hat Z$ is equal to one for both methods\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.5}Numerical experiment}{23}{subsubsection.2.3.5}}
\MT@newlabel{eq:FitzNag}
\citation{HeS92}
\citation{HaW96}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.6}Stability analysis}{24}{subsubsection.2.3.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}To do at Christmas}{24}{subsection.2.4}}
\citation{ADH10}
\citation{DPD15}
\citation{PSG12}
\@writefile{toc}{\contentsline {section}{\numberline {3}Bayesian inference of the parameters of an ODE}{25}{section.3}}
\newlabel{eq:ODEParam}{{27}{25}{Bayesian inference of the parameters of an ODE}{equation.3.27}{}}
\MT@newlabel{eq:ODEParam}
\MT@newlabel{eq:ODEParam}
\newlabel{eq:BayesODE}{{28}{25}{Bayesian inference of the parameters of an ODE}{equation.3.28}{}}
\newlabel{eq:likelihood}{{3}{25}{Bayesian inference of the parameters of an ODE}{equation.3.28}{}}
\MT@newlabel{eq:ODEParam}
\MT@newlabel{eq:BayesODE}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Approximation of the likelihood}{25}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Approximation of the likelihood at a fixed value $\mathaccentV {bar}016\theta $.\relax }}{26}{figure.caption.12}}
\newlabel{fig:MonteCarloVarianceH}{{6}{26}{Approximation of the likelihood at a fixed value $\bar \theta $.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Numerical experiment}{26}{subsubsection.3.1.1}}
\MT@newlabel{eq:FitzNag}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Numerical example}{26}{subsection.3.2}}
\MT@newlabel{eq:FitzNag}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Posterior distribution for the parameter $\theta $ defining the FitzHug-Nagumo model. The posterior distributions given by the probabilistic and the deterministic solvers are displayed in blue and red respectively. The true value of the parameters is displayed in thick green dots.\relax }}{27}{figure.caption.13}}
\newlabel{fig:MCMC_FHN}{{7}{27}{Posterior distribution for the parameter $\theta $ defining the FitzHug-Nagumo model. The posterior distributions given by the probabilistic and the deterministic solvers are displayed in blue and red respectively. The true value of the parameters is displayed in thick green dots.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Convergence of the posterior distribution}{27}{subsection.3.3}}
\citation{GiS02}
\citation{GiS02}
\newlabel{eq:TVvsHell}{{29}{28}{Convergence of the posterior distribution}{equation.3.29}{}}
\MT@newlabel{eq:TVvsHell}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Convergence of the parameter mean}{29}{subsection.3.4}}
\newlabel{sec:ParH}{{3.4}{29}{Convergence of the parameter mean}{subsection.3.4}{}}
\citation{Jon04}
\citation{FlJ10}
\citation{FlJ10}
\citation{FlJ10}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Considerations on the MCMC approximation}{30}{subsection.3.5}}
\newlabel{eq:CLTMarkov}{{30}{30}{Considerations on the MCMC approximation}{equation.3.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Variance of the Monte Carlo approximation given by MCMC with respect to the number of samples $N$. The convergence to zero is linear with respect to the number of samples. The estimator given by the batch means method gives a good approximation of the variance.\relax }}{31}{figure.caption.14}}
\newlabel{fig:BatchMeans}{{8}{31}{Variance of the Monte Carlo approximation given by MCMC with respect to the number of samples $N$. The convergence to zero is linear with respect to the number of samples. The estimator given by the batch means method gives a good approximation of the variance.\relax }{figure.caption.14}{}}
\MT@newlabel{eq:CLTMarkov}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Numerical experiment}{31}{subsubsection.3.5.1}}
\MT@newlabel{eq:FitzNag}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Convergence of the parameter to its stationary value and of the Hellinger distance of the probability distributions.\relax }}{32}{figure.caption.15}}
\newlabel{fig:ConvergenceMCMC}{{9}{32}{Convergence of the parameter to its stationary value and of the Hellinger distance of the probability distributions.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Numerical experiment}{32}{subsection.3.6}}
\MT@newlabel{eq:FitzNag}
\bibstyle{siam}
\bibdata{anmc.bib}
\bibcite{ADH10}{1}
\bibcite{AnR09}{2}
\bibcite{CGS16}{3}
\bibcite{DPD15}{4}
\bibcite{FlJ10}{5}
\bibcite{GeS11}{6}
\bibcite{GiS02}{7}
\bibcite{Gil05}{8}
\bibcite{HaW96}{9}
\bibcite{HeS92}{10}
\bibcite{Jon04}{11}
\bibcite{KaS05}{12}
\bibcite{KPS94}{13}
\bibcite{MLR16}{14}
\bibcite{PSG12}{15}
\bibcite{Vih12}{16}
