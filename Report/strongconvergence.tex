\subsection{Strong convergence}

In this section we prove a result about strong convergence of the method defined in \eqref{probabilityODE}. The following discrete Gronwall lemma is needed in the proof. 
\begin{theorem}[Discrete Gronwall Lemma]\label{thm:Gronwall} Let $y_n$ be a nonnegative sequence and $C_1, C_2$ positive constants. If
\begin{equation}
	y_n \leq C_1 + C_2 \sum_{k=0}^{n-1} y_k,
\end{equation}
then 
\begin{equation}
	y_n \leq C_1 \exp(nC_2).
\end{equation}
\end{theorem}
\noindent Two assumptions are necessary to prove the strong convergence result. The first assumption is on the noise model.
\begin{assumption}\label{assumption_1}
\begin{equation}
	\E^h|\xi_k(t)\xi_k(t)^T|^2_F \leq Kt^{2p+1}.
\end{equation}
Furthermore, there exists a matrix $Q$ independent of $h$ such that 
\begin{equation}
	\E^h[\xi_k(h)\xi_h(h)^T] = Qh^{2p+1},
\end{equation}
where $p\geq 1$.
\end{assumption}
\noindent Let us remark that if $Q = \sigma I$, with $I$ the identity matrix in $\R^{d\times d}$ and $\sigma > 0$, the method \eqref{probabilityODE} can be simulated by
\begin{equation}
	U_{k+1} = \Psi_h(U_k) + \sqrt{\sigma} h^{p + \frac{1}{2}} Z_k,
\end{equation}
where $Z_k$ is a Gaussian random vector with independent entries $Z_{k,i}  \sim\mathcal{N}(0,1), i = 1, \ldots, d$. \\
An assumption on the numerical method is needed.
\begin{assumption}\label{assumption_2}  The function $f$ and a sufficient number of its derivatives are bounded uniformly in $\R^n$ in order to ensure that $f$ is globally Lipschitz and that the numerical flow map $\Psi_h$ has uniform local truncation error of order $q + 1$
\begin{equation}
	\sup_{u\in\R^n} |\Psi_t(u) - \Phi_t(u)| \leq Kt^{q+1}.
\end{equation}
\end{assumption}
\noindent The following result hold.
\begin{theorem}[Strong Convergence]\label{thm:strongConv} Under assumptions \ref{assumption_1} and \ref{assumption_2} it follows that there is $K>0$ such that
\begin{equation}\label{strongConvDisc}
	\sup_{0<kh<T} \E^h|u_k - U_K|^2 \leq Kh^{2\min\{p,q\}}.
\end{equation}
Furthermore
\begin{equation}\label{strongConvCont}
	\sup_{0\leq t \leq T} \E^h|u(t) - U(t)| \leq Kh^{\min\{p, q\}}.
\end{equation}
\end{theorem}
\noindent This result implies that a reasonable choice in $p$ of Assumption \ref{assumption_1} is $p = q$.
\begin{proof}
Given the method in \eqref{probabilityODE} and writing the exact solution of \eqref{ODE} as
\begin{equation}
	u_{k+1} = \Phi_h(u_k),
\end{equation}
one can compute the truncation error $\epsilon_k = \Psi_h(U_k) - \Phi_h(U_k)$, so that
\begin{equation}
	U_{k+1} = \Phi_h(U_k) + \epsilon_k + \xi_k(h).
\end{equation}
Therefore
\begin{align}
	e_{k+1} &= u_k - U_k \\
			&= \Phi_h(u_k) - \Phi_h(u_k - e_k) - \epsilon_k - \xi_k(h).
\end{align}
Taking the expectation and under Assumption \ref{assumption_1}
\begin{equation}
	\E^h|e_{k+1}|^2 = \E^h|\Phi_h(u_k) - \Phi_h(u_k - e_k) - \epsilon_k|^2 + \OO(h^{2p + 1}).
\end{equation}
Developing the square and since $\Phi_h$ is Lipschitz continuous with constant $(1 + Lh)$ and $\epsilon_k = \OO(h^{q+1})$ thanks to Assumption \ref{assumption_2}
\begin{align}
	\E^h|e_{k+1}|^2 \leq &(1 + Lh)^2 \E^h|e_k|^2 + \E^h\left|\left(h^{\frac{1}{2}}(\Phi_h(u_k) - \Phi_h(u_k - e_k), h^{-\frac{1}{2}}\epsilon_k\right)\right| \\
						 &+ \OO\left(h^{2q+2}\right) + \OO\left(h^{2p + 1}\right). 
\end{align}
Then, using Cauchy-Schwarz on the inner product
\begin{align}
	\E^h|e_{k+1}|^2 &\leq \left(1 + \OO\left(h\right)\right)\E^h|e_k|^2 + \OO\left(h^{2q + 1}\right) + \OO\left(h^{2p + 1}\right) \\
	&\leq C_1 h \E^h|e_k|^2 + \E|e_k|^2 + \OO\left(h^{2q + 1}\right) + \OO\left(h^{2p + 1}\right) \\
	&\leq C_1 h \sum_{i = 0}^{k}\E^h|e_i|^2 + \OO\left(h^{-1}\right)\left(\OO\left(h^{2q + 1}\right) + \OO\left(h^{2p + 1}\right)\right)\\
	&\leq C_1 h \sum_{i = 0}^{k}\E^h|e_i|^2 + \OO\left(h^{2q}\right) + \OO\left(h^{2p}\right).
\end{align}
Therefore by Proposition \ref{thm:Gronwall}
\begin{align}
	\E^h|e_{k}|^2 &\leq C_2 h^{2\min\{p,q\}} \exp(C_1kh)\\ 
		      &\leq C_2 h^{2\min\{p,q\}} \exp(C_1T) \\	
		      &\leq Ch^{2\min\{p,q\}}.	
\end{align}  	
\end{proof}