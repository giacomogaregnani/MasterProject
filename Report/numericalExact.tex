\subsubsection{Numerical example}

We consider the test equation
\begin{equation}
\begin{aligned}
	\dv{u(t)}{t} &= \lambda u(t), \\
	u(0) &= 1
\end{aligned}
\end{equation}
with $\lambda$ a real negative parameter. If $\lambda$ is big in absolute value, the equation is stiff. In this experiment, we are not interested in stiff equations, therefore we consider $\lambda = -0.5$. The analytical solution of this equation is known and is given by
\begin{equation}
	u(t) = \exp(\lambda t), \quad t > 0.
\end{equation}
We are interested in verifying the order of convergence of the noisy MH presented in section \ref{sect:MCWM}. We consider the prior distribution to be a Gaussian centered in the true value of the parameter $\bar \lambda = -0.5$ with unitary variance. Then, we generate data from the analytical solution at $t = 1$ with a normal disturbance, i.e.,
\begin{equation}
	d = \exp(\lambda) + \epl, \quad \epl \sim \mathcal{N}(0, \Gamma),
\end{equation}
with $\Gamma = 0.001$. In this way, it is possible to generate the true posterior distribution $\pi(\lambda|d)$ through Bayes' rule
\begin{equation}
	\pi(\lambda|d) \propto \mathcal{Q}(\lambda) \diffL(d|\exp(\lambda)),
\end{equation}
where the prior distribution is given by
\begin{equation}
	\mathcal{Q}(\lambda) = (2\pi)^{-1/2}\exp(-\frac{1}{2}(\lambda - \bar\lambda)),
\end{equation}
and the likelihood is given by
\begin{equation}
	\diffL(d|\exp(\lambda)) = (2\pi\Gamma)^{-1/2}\exp(-\frac{1}{2\Gamma}(\exp(\lambda) - d)^2).
\end{equation}
Normalizing the product of prior and likelihood, we obtain the true posterior distribution for the parameter $\lambda$. We consider now the RAM algorithm for the probabilistic method \eqref{probabilityODE} with explicit Euler as a deterministic solver. We consider $h = \left\{0.1, 0.05, 0.01\right\}$ and $N = \left\{1, 10, 100, 1000, 10000\right\}$. In this way, we can observe for each value of $h$ the convergence of the posterior distribution $\pi^{h,\sigma}_N$ to the exact distribution. In order to estimate the total variation distance between $\pi^{h,\sigma}_N$ and $\pi$ we consider the bound given by the Hellinger distance between the normal distributions with the estimate values of mean and variance \eqref{eq:TVvsHell}.

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.49\linewidth}
		\centering
		\resizebox{1\linewidth}{!}{\input{plots/ExactMCH1.tikz}}  
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\centering
		\resizebox{1\linewidth}{!}{\input{plots/ExactMCH05.tikz}}  
	\end{subfigure}    
	\caption{Distribution $\pi^{h,\sigma}_N$ for the different values of $h$ and $N$.}
	\label{fig:ExactConvergence}
\end{figure}
