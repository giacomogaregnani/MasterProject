\documentclass{beamer}

\usetheme{Warsaw}
\usecolortheme{orchid}

\usepackage{algorithm,algorithmic}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{pgfplots} 
\usepackage{graphicx}
\usepackage[utf8x]{inputenc}
\usepackage{tikz}
\usepackage{bbm}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{lipsum}
\usepackage{color}
\usepackage{physics}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\newcommand{\eqtext}[1]{\ensuremath{\stackrel{#1}{=}}}
\newcommand{\leqtext}[1]{\ensuremath{\stackrel{#1}{\leq}}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\epl}{\varepsilon}
\newcommand{\defeq}{\coloneqq}
\newcommand{\uDarcy}{\color{red}{u}}
\newcommand{\sksum}{\textstyle\sum}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{problemTh}{Problem}
\newtheorem{assumption}{Assumption}
\renewcommand{\phi}{\varphi}
\definecolor{mygray}{gray}{0.8}

% Add numbers and take out navigation symbols
\setbeamertemplate{footline}[frame number]
\beamertemplatenavigationsymbolsempty

\title{Probabilistic solvers for ODE's \\ and Bayesian inference of parametrized models}
\subtitle{Master Project - Master in CSE}
\author{Giacomo Garegnani \\ Supervisor: Prof. Assyr Abdulle \\ Experts: Prof. Fabio Nobile, Dr. Kostas Zygalakis}
\institute{EPFL}
\date{02/02/2017}

\begin{document}

\frame{\titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OUTLINE %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Outline of the presentation}
	\begin{enumerate}
		\item Introduction on Bayesian inference and MCMC
		\item Probabilistic solvers for ODE's
		\item Bayesian inference inverse problems with differential equations
	\end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PART ONE %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
	\frametitle{Bayesian inference and MCMC}
	\framesubtitle{Bayes' formula}
	
	Consider $\Omega$ event space, $\mathcal{A}$  $\sigma$-algebra, $P$ probability measure and $(\Omega, \mathcal{A}, P)$. Given $A$, $B$ in $\Omega$, Bayes' formula reads 
	\begin{equation*}
		P(A\mid B) = \frac{P(B\mid A)P(A)}{P(B)} \propto P(B\mid A)P(A).
	\end{equation*}
	Normalization constant $P(B)$ can be replaced as
	\begin{equation*}
		P(A\mid B) = \frac{P(B\mid A)P(A)}{\int_{\Omega}P(B\mid A)P(A)},
	\end{equation*}
	as $P(A\mid B)$ is a probability distribution.
\end{frame}

\begin{frame}
	\frametitle{Bayesian inference and MCMC}
	\framesubtitle{Bayesian inference}
	
	\underline{Problem.} Consider two events $A$, $B$ in $\Omega$ and the probability space $(\Omega, \mathcal{A}, P)$. We want to infer the probability distribution of $A$ given $B$ as
	\begin{equation*}
		\underbrace{\pi(A\mid B)}_{\text{posterior}} \propto \overbrace{\mathcal{Q}(A)}^{\text{prior}} \underbrace{\mathcal{L}(B \mid A)}_{\text{likelihood}}
	\end{equation*}
	In models parametrized by a parameter $\theta$, we deduce the distribution of $\theta$ through observations $\mathcal{Y}_n = \{y_1, y_2, \ldots, y_n\}$ as
	\begin{equation*}
		\pi(\theta\mid\mathcal{Y}_n) \propto \mathcal{Q}(\theta) \mathcal{L}(\mathcal{Y}_n \mid \theta).
	\end{equation*}
\end{frame}

\begin{frame}
	\frametitle{Bayesian inference and MCMC}
	\framesubtitle{MCMC - motivation}
	
	\underline{Goal.} Approximate the expectation under the distribution $\pi(\theta \mid \mathcal{Y})$ of a functional of the parameter $\theta \in \R^{N_p}$ with a Monte Carlo sum, i.e.,
	\begin{equation*}\label{eq:MonteCarlo}
			 \E^\pi\left[g(\theta)\right] = \int_{\R^{N_p}} g(\theta)\pi(\dd\theta\mid\mathcal{Y})  \approx \frac{1}{N}\sum_{k = 1}^{N} g(\theta^{(k)}),
	\end{equation*}
	where $\theta^{(k)}$ are realizations of $\theta$. \\[0.5cm]
	\underline{Problem.} How do we generate samples $\theta^{(k)}$, with $k = 1, \ldots, N$ so that the approximation \eqref{eq:MonteCarlo} holds? \\
	$\rightsquigarrow$ MCMC \cite[e.g.]{Gil05} \\[0.5cm]
	\underline{Idea.} Generate samples $\theta^{(k)}$, with from a Markov chain with kernel $P$ until the chain reaches its stationary distribution. Different choices of the Markov kernel lead to different MCMC algorithms.	
\end{frame}

\begin{frame}
	\frametitle{Bayesian inference and MCMC}
	\framesubtitle{Metropolis-Hastings}		
	
	\underline{Metropolis-Hastings (MH).} Choose a \textit{proposal distribution} $q(x,y)$ such that
	\begin{equation*}
		\int_{\R^{N_p}}q(x, y)\dd y = 1,
	\end{equation*}
	Then, given current sample $\theta^{(i)}$, the transition kernel $P_{\mathrm{MH}}$ giving $\theta^{(i+1)}$ is defined as
	\begin{equation*}
		P_{\mathrm{MH}}(\theta^{(i)}, \theta^{(i+1)}) \defeq \alpha(\theta^{(i)}, \theta^{(i+1)})q(\theta^{(i)},\theta^{(i+1)}) + \delta_{\theta^{(i)}}(\theta^{(i+1)})\rho(\theta^{(i)}),
	\end{equation*}
	where $\alpha(\theta^{(i)}, \vartheta)$ is the \textit{acceptance probability} and is given by
	\begin{equation*}\label{eq:MHalpha}
		\alpha(\theta^{(i)}, \vartheta) = \min\left\{\frac{\pi(\vartheta)q(\vartheta, \theta^{(i)})}{\pi(\theta^{(i)})q(\theta^{(i)}, \vartheta)}, 1\right\},
	\end{equation*}
	and $\rho$ is defined as
	\begin{equation*}
	 	\rho(\theta^{(i)}) \defeq 1 - \int_{\R^{N_p}}\alpha(\theta^{(i)}, x)q(\theta^{(i)}, x)\dd x.
	\end{equation*}
\end{frame}

\begin{frame}
	\frametitle{Bayesian inference and MCMC}
	\framesubtitle{Metropolis-Hastings - Pseudocode}
	\begin{algorithm}[H]	
		\begin{algorithmic}
			\STATE Given $\theta^{(0)} \in \R^{N_p}, N \in \N_0$.
			\FOR{$i = 0, \ldots, N$}
			\STATE Draw $\vartheta$ from $q(\theta^{(i)}, \cdot)$;
			\STATE Compute the acceptance probability $\alpha(\theta^{(i)}, \vartheta)$ as $$\alpha(\theta^{(i)}, \vartheta) = \min\left\{\frac{\pi(\vartheta)q(\vartheta, \theta^{(i)})}{\pi(\theta^{(i)})q(\theta^{(i)}, \vartheta)}, 1\right\};$$
			\STATE Draw $u$ from $\mathcal{U}(0, 1)$;
			\IF{$\alpha > u$}
			\STATE Accept $\vartheta$, set $\theta_{i+1} = \vartheta$; 
			\ELSE
			\STATE set $\theta_{i+1} = \theta^{(i)}$
			\ENDIF
			\ENDFOR
		\end{algorithmic}
		\caption{Metropolis-Hastings.}
	\end{algorithm}
\end{frame}

\begin{frame}
	\frametitle{Bayesian inference and MCMC}
	\framesubtitle{Metropolis-Hastings - Observations}
	
	\underline{Remark.} If the proposal distribution is symmetric, i.e., $q(x,y) = q(y,x)$, then 
	\begin{equation*}
		\alpha(\theta^{(i)}, \vartheta) = \min\left\{\frac{\pi(\vartheta)}{\pi(\theta^{(i)})}, 1\right\}.
	\end{equation*}
	For example, Gaussian proposal \cite{KaS05}
	\begin{equation*}
		q(x, y) \propto \exp(-\frac{1}{2}(x - y)^T\Sigma^{-1}(x - y)).
	\end{equation*}
	\underline{Problems.} Two main problems
	\begin{enumerate}
		\item How to choose an efficient proposal distribution?
		\item How to modify MH if it is not possible to evaluate the posterior distribution?
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Bayesian inference and MCMC}
	\framesubtitle{Robust adaptive Metropolis (RAM) \cite{Vih12}}
	
	\underline{Problem.} Bad proposal distribution $q(x,y) \implies$ inefficient algorithms. Measure efficiency with \textit{acceptance ratio}. \\[0.5cm]
	\underline{Idea.} Adapt $q(x,y)$ to obtain a chosen acceptance ratio $\alpha^*$. Choose $q(x,y)$ Gaussian, the new guess $\vartheta$ is 
	\begin{equation*}
		\vartheta = \theta_k + S_n z_n, \quad Z_n \sim \mathcal{N}(0, I),
	\end{equation*}
	where $S_n \in \R^{N_p\times N_p}$ is lower triangular definite positive. Then, 
	\begin{equation*}
		S_{n+1}S_{n+1}^T = S_n\left(I + \eta_n\left(\alpha(\theta^{(n)}, \vartheta) - \alpha^*\right)\frac{z_nz_n^T}{z_n^Tz_n}\right)S_n^T,
	\end{equation*}
	where $\eta_n \xrightarrow{n\to\infty} 0$.
	
\end{frame}

\begin{frame}
	\frametitle{Bayesian inference and MCMC}
	\framesubtitle{Robust adaptive Metropolis \cite{Vih12}, numerical experiment}
	
	Two-dimensional distribution $\pi$ with density \cite{KaS05}
	\begin{equation*}
		\pi(X) \propto \exp(-10(X_1^2 - X_2)^2 - (X_1 - 0.25)^4),
	\end{equation*}
	Setting of the experiment. Given $\sigma = \{0.01, 0.5, 2.0\}$, compare
	\begin{itemize}
		\item standard MH with Gaussian proposal with covariance $\sigma I$,
		\item RAM with $S_0 = \sigma I$ and $\alpha^* = 0.4$.
	\end{itemize}
	Draw $N = 5000$ samples and compute final acceptance ratio $\bar \alpha$.
\end{frame}


\begin{frame}
	\frametitle{Bayesian inference and MCMC}
	\framesubtitle{Robust adaptive Metropolis \cite{Vih12}, numerical experiment}
	\begin{figure}[t]
		\centering
		\begin{subfigure}{0.32\linewidth}
			\centering
			\tiny{$\bar \alpha = 0.96$}
			\includegraphics[width=1\linewidth]{../plots/MHvsRAM/MH_small}
		\end{subfigure}
		\begin{subfigure}{0.32\linewidth}
			\centering
			\tiny{$\bar \alpha = 0.35$}
			\includegraphics[width=1\linewidth]{../plots/MHvsRAM/MH_medium}
		\end{subfigure}
		\begin{subfigure}{0.32\linewidth}
			\centering
			\tiny{$\bar \alpha = 0.06$}
			\includegraphics[width=1\linewidth]{../plots/MHvsRAM/MH_big}
		\end{subfigure}
		
		\begin{subfigure}{0.32\linewidth}
			\centering
			\tiny{$\bar \alpha = 0.43$}
			\includegraphics[width=1\linewidth]{../plots/MHvsRAM/RAM_small}
		\end{subfigure}
		\begin{subfigure}{0.32\linewidth}
			\centering
			\tiny{$\bar \alpha = 0.40$}
			\includegraphics[width=1\linewidth]{../plots/MHvsRAM/RAM_medium}
		\end{subfigure}
		\begin{subfigure}{0.32\linewidth}
			\centering
			\tiny{$\bar \alpha = 0.38$}
			\includegraphics[width=1\linewidth]{../plots/MHvsRAM/RAM_big}
		\end{subfigure}
	\end{figure}
	Samples produced by MH and RAM for the distribution with standard MH (first row) and RAM (second row).
\end{frame}

\begin{frame}
	\frametitle{Bayesian inference and MCMC}
	\framesubtitle{Pseudo-marginal MCMC \cite{ADH10, DPD15, MLR16}}
	
	\underline{Problem.} Impossible to evaluate $\pi(\theta)$ (no closed form available). \\[0.5cm]
	\underline{Idea.} Find evaluable $\pi(\theta, \xi)$ that admits $\pi(\theta)$ as marginal distribution, then compute
	\begin{equation*}
		\hat \pi_M(\theta) = \frac{1}{M} \sum_{i = 1}^{M} \pi(\theta, \xi^{(i)}),
	\end{equation*}
	with $\xi^{(i)}$, $i = 1, \ldots, M$ realizations of $\xi$. Use $\hat \pi_M$ for $\alpha(\theta^{(i)}, \vartheta)$. \\[0.5cm]
	\underline{Remark.} The rest of MH is unchanged.	
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PART TWO %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
	\frametitle{Outline of the presentation}
	\begin{enumerate}
		\item \color{mygray} Introduction on Bayesian inference and MCMC
		\item \color{black} Probabilistic solvers for ODE's
		\item Bayesian inference inverse problems with differential equations
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Probabilistic solvers for ODE's}
	\framesubtitle{Method presentation \cite{CGS16}}
	
	\underline{Problem.} Given $f\colon\R^d\to\R^d$ and the autonomous ODE 
	\begin{equation*}
		u'(t) = f(u), \quad	u(0) = u_0, 
	\end{equation*}
	build a probabilistic numerical solution. There exists flow map $\Phi_t(y)$ such that
	\begin{equation*}
		u(t) = \Phi_t(u_0).
	\end{equation*}
	\underline{Idea.} Given $h > 0$, the flow map of a Runge-Kutta method $\Psi_h(y)$ is
	\begin{equation*}
		u_{k+1} = \Psi_h(u_k), \quad k = 0, 1, \ldots,
	\end{equation*}
	consider $\xi_k(h)$ i.i.d. random variables in $\R^d$ and compute
	\begin{equation*}
		U_{k+1} = \underbrace{\Psi_h(U_k)}_{\text{deterministic}} + \overbrace{\xi_k(h)}^{\text{random}}, \quad k = 0, 1, \ldots,
	\end{equation*}
	
\end{frame}

\begin{frame}
	\frametitle{Probabilistic solvers for ODE's}
	\framesubtitle{Deterministic methods \cite{HLW02}}
	
	Deterministic component $\rightsquigarrow$ Runge-Kutta methods.
	\begin{definition}\label{def:RK} Given $s \in \N^*$, $(b_i)_{i=1}^s$, $(a_{ij})_{i,j=1}^s$, $h > 0$, one step of an $s$-stage Runge-Kutta method is 
		\begin{equation*}
		\begin{aligned}
		K_i &= f(U_0 + h\sksum_{j=1}^s a_{ij}K_j), && i = 1, \ldots, s, \\
		U_1 &= U_0 + h \sksum_{i=1}^s b_i K_i.
		\end{aligned}
		\end{equation*}
		The method is of order $q$ if $\exists C > 0$ independent of $h$ such that
		\begin{equation*}
			\norm{u(h)-U_1} \leq Ch^{q+1}.
		\end{equation*}
	\end{definition}
	Monte Carlo computations $\rightsquigarrow$ Explicit methods privileged.
	
\end{frame}

\begin{frame}
	\frametitle{Probabilistic solvers for ODE's}
	\framesubtitle{Method motivation}
	
	Consider chaotic differential equation, e.g., Lorenz system
	\begin{equation*}\label{eq:Lorenz}
	\begin{aligned}
		x' &= \sigma(y - x), \quad &&x(0) = -10,\\
		y' &= x(\rho - z) - y, \quad &&y(0) = -1,\\
		z' &= xy - \beta z, \quad &&z(0) = 40.
	\end{aligned}
	\end{equation*}
	Small perturbation $\implies$ uncontrollable deviation of the solution. \\
	Deterministic solvers not reliable for any time step $h > 0$. \\
	$\rightsquigarrow$ Family of $M$ probabilistic numerical solutions.
\end{frame}

\iffalse
\begin{frame}
	\frametitle{Probabilistic solvers for ODE's}
	\framesubtitle{Method motivation}
	
	\begin{figure}[t]
		\centering
		\begin{subfigure}{1\linewidth}
			\resizebox{1.0\linewidth}{!}{\input{../plots/LorenzX.tikz}}
		\end{subfigure}
		\begin{subfigure}{1\linewidth}
			\resizebox{1.0\linewidth}{!}{\input{../plots/LorenzY.tikz}}
		\end{subfigure}
	\end{figure}

	Components $x$ and $y$ of Lorenz system with deterministic solver (thick black) and probabilistic solver (light gray). Gauss method on 2 stages as $\Psi_h(y)$.
\end{frame}
\fi

\begin{frame}
	\frametitle{Probabilistic solvers for ODE's}
	\framesubtitle{Method properties}
	
	Relevant properties to be analyzed
	\begin{itemize}
		\item strong order of convergence,
		\item weak order of convergence,
		\item behavior Monte Carlo approximations,
		\item stability.
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Probabilistic solvers for ODE's}
	\framesubtitle{Method properties - strong convergence \cite{CGS16}}
	
	\underline{Recall.} The probabilistic method is defined as
	\begin{equation*}
		U_{k+1} = \Psi_h(U_k) + \xi_k(h), \quad k = 0, 1, \ldots,
	\end{equation*}
	for suitable random variables $\xi_k(h)$. \\[0.5cm]
	
	It is possible to prove a result of strong convergence.
	
	\begin{definition}[Strong convergence] The probabilistic method has strong order $r$ if $\exists C > 0$ independent of $h$ such that for $h$ small enough
		\begin{equation*}
			\sup_{t_k = kh} \E\abs{U_k - u(t_k)} \leq Ch^r.
		\end{equation*}
	\end{definition}
	
\end{frame}
	
\begin{frame}
	\frametitle{Probabilistic solvers for ODE's}
	\framesubtitle{Method properties - strong convergence \cite{CGS16}}
	
	\begin{assumption}[Variance of random variables] The variables $\xi_k(t)$ satisfy for $p \geq 1$
		\begin{equation*}
			\E|\xi_k(t)\xi_k(t)^T|^2_F \leq Kt^{2p+1}.
		\end{equation*}
		Furthermore, there exists a matrix $Q$ independent of $h$ such that 
		\begin{equation*}
		\E[\xi_k(h)\xi_h(h)^T] = Qh^{2p+1},
		\end{equation*}
	\end{assumption}
		
	\begin{assumption}[Order of the deterministic component] The function $f$ and a sufficient number of its derivatives are bounded uniformly in $\R^n$ in order to ensure that $f$ is globally Lipschitz and that the numerical flow map $\Psi_h$ has uniform local truncation error of order $q + 1$, i.e., 
		\begin{equation*}
			\sup_{u\in\R^n} |\Psi_t(u) - \Phi_t(u)| \leq Kt^{q+1}.
		\end{equation*}
	\end{assumption}	
\end{frame}

\begin{frame}
	\frametitle{Probabilistic solvers for ODE's}
	\framesubtitle{Method properties - strong convergence \cite{CGS16}}
	
	\begin{theorem}[Strong Convergence] Under the assumptions above, there exists $K>0$ such that
		\begin{equation*}
		\sup_{0<kh<T} \E|u_k - U_K|^2 \leq Kh^{2\min\{p,q\}}.
		\end{equation*}
	\end{theorem}
	
	\underline{Idea of the proof.} Compute truncation error between exact and numerical solutions, divide deterministic and probabilistic contribution and derive a recurrence on the error. Apply then discrete Gronwall's lemma to bound the error.
\end{frame}

\begin{frame}
	\frametitle{Probabilistic solvers for ODE's}
	\framesubtitle{Method properties - weak convergence \cite{CGS16}}

	\begin{definition}[Weak convergence] The probabilistic method has weak order $r$ if $\exists C > 0$ independent of $h$ such that for any function $\phi$ sufficiently smooth
		\begin{equation*}
		\sup_{t_k = kh} \abs{\E[\phi(U_k)] - \phi(u(t_k))} \leq Ch^r,
		\end{equation*}
		for $h$ small enough.
	\end{definition}
	
	\underline{Idea.} Introduce a modified SDE 
	\begin{equation}
		\dd{\tilde u} = f^h{\tilde u}\dd{t} + \sqrt{h^{2p} Q} \dd{W},
	\end{equation}
	and study the convergence of $U_k$ and $\tilde u$ to $u$ for $h \to 0$.
\end{frame}

\begin{frame}
	\frametitle{Probabilistic solvers for ODE's}
	\framesubtitle{Method properties - weak convergence \cite{CGS16}}

	\begin{theorem}[Weak convergence] For any function $\phi$ sufficiently smooth
		\begin{equation*}
		\abs{\phi(u(T)) - \E[\phi(U_k)]} \leq Kh^{\min\{2p, q\}}, \quad kh = T,
		\end{equation*}
		and 
		\begin{equation*}
		\abs{\E[\phi(\tilde u(T))] - \E[\phi(U_k)]} \leq Kh^{2p + 1}, \quad kh = T.
		\end{equation*}
	\end{theorem}
	
	\underline{Idea of the proof.} Use techniques of backwards error analysis, finding a modified ODE and SDE such that the numerical error is of higher order.
	
\end{frame}

\begin{frame}
	\frametitle{Probabilistic solvers for ODE's}
	\framesubtitle{Method properties - Monte Carlo}
	
	\underline{Problem.} Study convergence of Monte Carlo approximations. \\[0.5cm]
	Numerical solution $\rightsquigarrow$ $\mathcal Q_h$ (inaccessible), \\
	$M$ samples of numerical solution  $\rightsquigarrow$ $Q_h^M$ (accessible), \\
	Exact solution $\rightsquigarrow$ $\delta_u$. \\[0.5cm]
	
	Convergence scheme, we expect
	\begin{equation*}
	\mathcal Q_h^M \xrightarrow{M\to \infty} \mathcal Q_h \xrightarrow{h\to 0} \delta_u.
	\end{equation*}
	Second convergence already treated, first unclear \cite{KeH16}.
	
\end{frame}

\begin{frame}[allowframebreaks]
	\frametitle{References}
	
	\setbeamertemplate{bibliography item}[text]
	\bibliographystyle{apalike}
	\bibliography{../anmc}
\end{frame}

\end{document}
