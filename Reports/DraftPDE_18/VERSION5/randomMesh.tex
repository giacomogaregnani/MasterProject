\documentclass[10pt]{article}

\input{ex_shared}

\begin{document}
	
\maketitle	

\section{Formulation} 

\section{A priori error analysis}

\subsubsection*{Galerkin}
Define	$\Pi_h \colon \btilde V_h \to V_h$ such that if $\tilde v_h(x) = \sum_i \tilde v_i \tilde \phi_i(x)$ then $\Pi_h \tilde v_h(x) = \sum_i \tilde v_i \phi_i(x)$. Analogously, we denote by $\Pi_h^{-1}\colon V_h \to \btilde V_h$ the inverse operator such that if $v_h = \sum_i v_i \phi_i(x)$, then $\Pi_h^{-1} v_h(x) = \sum_i v_i \tilde \phi_i(x)$. It is clear that $\Pi_h \circ \Pi_h^{-1} = \mathrm{Id}$. Consider
\begin{equation}
\begin{aligned}
	a(u_h, v_h) &= f(v_h), \quad \forall v_h \in V_h,\\
	a(\tilde u_h, \tilde v_h) &= f(\tilde v_h), \quad \forall \tilde v_h \in \tilde V_h.
\end{aligned}
\end{equation}
Galerkin ``orthogonality'', $\forall v_h \in V_h$
\begin{equation}
\begin{aligned}
	a(u_h - \tilde u_h, v_h) &= f(v_h - \tilde v_h) + a(\tilde u_h, \tilde v_h - v_h)\\
	&\leq \big(C_f + C_a \norm{\tilde u_h}_V \big) \norm{v_h - \tilde v_h}_V.
\end{aligned}
\end{equation}
for all $\tilde v_h \in \btilde V_h$. Choose $\tilde v_h = \Pi_h^{-1} v_h$, 
\begin{equation}
	a(u_h - \tilde u_h, v_h) \leq \big(C_f + C_a \norm{\tilde u_h}_V \big) \norm{v_h - \Pi_h^{-1} v_h}_V.
\end{equation}

\subsubsection*{Interpolation estimates}

\textbf{Goal:} Estimate $\norm{\nabla (v_h - \Pi_h v_h)}_{\LL^2}$.
\begin{equation}
\begin{aligned}
	\norm{\nabla (v_h - \Pi_h v_h)}_{\LL^2} &= \norm{\sum_i v_i \nabla(\phi_i - \tilde \phi_i)}_{\LL^2}\\
	&\leq \sum_i \abs{v_i} \norm{\nabla(\phi_i - \tilde \phi_i)}_{\LL^2}.
\end{aligned}
\end{equation}
If $S_i$ support of $\phi_i$ and $\btilde S_i$ support of $\tilde \phi_i$
\begin{equation}
\begin{aligned}
	\norm{\nabla(\phi_i - \tilde \phi_i)}^2_{\LL^2} &= \int_D \abs{\nabla(\phi_i - \tilde \phi_i)}^2\\
	&= \int_{S_i} \abs{\nabla(\phi_i - \tilde \phi_i)}^2 + \int_{\tilde S_i \setminus S_i} \abs{\nabla \tilde \phi_i}^2.
\end{aligned}
\end{equation}
\textbf{One-dimensional case.} Define
\begin{equation}
\begin{aligned}
	\bar h_i &= \frac{h_{i+1} - h_i}{2}, \quad i = 1, \ldots, N-1 \\
	\bar h_0 &= 0, \quad \bar h_N = 0.
\end{aligned}
\end{equation}
And the points $\tilde x_i = x_i + \alpha_i\bar h_i^{p+1}$ for some $p > 1$, $\alpha_i$ such that $\abs{\alpha_i} \leq M$ and $i = 0, \ldots, N$. Estimate the two terms separately. Linear basis functions
\begin{equation}
\nabla \tilde \phi_i = \begin{dcases}
\frac{1}{h_i + (\alpha_i\bar h_i^{p+1} - C_{i-1}\bar h_{i-1}^{p+1})}, & \mbox{in } (\tilde x_{i-1}, \tilde x_i), \\
\frac{-1}{h_{i+1} + (\alpha_{i+1}\bar h_{i+1}^{p+1} - \alpha_i\bar h_i^{p+1})}, & \mbox{in } (\tilde x_i, \tilde x_{i+1})
\end{dcases}
\end{equation}
Then
\begin{equation}
\begin{aligned}
\int_{\tilde S_i \setminus S_i} \abs{\nabla \tilde \phi_i}^2 &= \int_{\tilde x_{i-1}}^{x_{i-1}}\abs{\nabla \tilde \phi_i}^2 + \int_{\tilde x_{i+1}}^{x_{i+1}}\abs{\nabla \tilde \phi_i}^2 \\
&= \frac{1}{(h_i + (\alpha_i\bar h_i^{p+1} - \alpha_{i-1}\bar h_{i-1}^{p+1}))^2} \alpha_{i-1} \bar h_{i-1}^{p+1} \\
&\quad + \frac{1}{(h_{i+1} + (\alpha_{i+1}\bar h_{i+1}^{p+1} - \alpha_i\bar h_i^{p+1}))^2} \alpha_{i+1} \bar h_{i+1}^{p+1} \\
&\leq \Big(\frac{1}{(h_i - M(\bar h_i^{p+1} + \bar h_{i-1}^{p+1}))^2} + \frac{1}{(h_{i+1} -M (\bar h_{i+1}^{p+1} + \bar h_i^{p+1}))^2} \Big) M h^{p+1} \\
&\leq C\Big(\frac{1}{h_i^2} + \frac{1}{h_{i+1}^2}\Big)h^{p+1} \leq Ch^{p-1},
\end{aligned}
\end{equation}
where in the last step we used the fact that $h \leq C h_i$ (quasi-uniform).

\subsubsection*{Almost sure convergence}

Finally
\begin{equation}
\begin{aligned}
\alpha \norm{u_h - U_h}_V^2 &\leq a(u_h - \tilde u_h, u_h - U_h) + a(\tilde u_h - U_h, u_h - U_h)\\
&\leq \big(C_f + C_a \norm{\tilde u_h}_V \big) \norm{u_h - U_h - \Pi_h^{-1}(u_h - U_h)}_V + C_a \norm{\tilde u_h - U_h}_V \norm{u_h - U_h}_V.
\end{aligned}
\end{equation}
Hence
\begin{equation}
\begin{aligned}
\norm{u_h - U_h}_V &\leq \frac{1}{\alpha} \big(C_f + C_a \norm{\tilde u_h}_V \big) \mathrm{int. estimate} + C_a \norm{\tilde u_h - U_h}_V.
\end{aligned}
\end{equation}

\subsubsection*{$\LL^2(\Omega)$ convergence}

\section{A posteriori error analysis}

\section{Inverse problems}

%\subsection{Proof of concept} Let us consider the following simple PDE
%\begin{equation}\label{eq:TestPDEInverse}
%\begin{aligned}
%-u_\theta'' &= \theta, && \text{in } (0, 1),\\
%u_\theta &= 0, && \text{in } \{0, 1\},
%\end{aligned}
%\end{equation}
%whose exact solution is given by
%\begin{equation}
%	u_\theta(x) = \frac{\theta}{2}(x - x^2).
%\end{equation}
%Let us consider as prior distribution $\pi_{\mathrm{Pr}}(\theta)$ on the parameter $\theta$ a Gaussian $\mathcal{N}(\theta_0, \sigma_0^2)$. Moreover, let us consider a scalar $0 < h < 1$ and a single observation $\bar u$ given by
%\begin{equation}
%	\bar u = u_{\bar \theta}(\bar x) + \epl, \quad \epl \sim \mathcal{N}(0, \gamma^2),
%\end{equation}
%where $\bar \theta$ is the true value of the parameter and $0 < \bar x < 1$ is a point in the domain. For any value of $\theta$, the likelihood of the observation is therefore given up to a constant by
%\begin{equation}
%	\pi(\bar u \mid \theta) \propto \exp\Big(-\frac{1}{2\gamma^2}(u_\theta(\bar x) - \bar u )^2 \Big)
%\end{equation}
%Since the exact solution of the PDE is known, it is possible to compute analytically the posterior distribution of the parameter $\theta$. In particular, we have thanks to Bayes' rule
%\begin{equation}
%	\pi(\theta \mid \bar u) \propto \pi_{\mathrm{Pr}}(\theta) \pi(\bar u \mid \theta).
%\end{equation}
%Replacing the expressions of prior and likelihood, we get
%\begin{equation}
%	\pi(\theta \mid \bar u) \propto \exp\Big(-\frac{(\theta - \theta_0)^2}{2\sigma_0^2} - \frac{1}{2\gamma^2}(u_\theta(\bar x) - \bar u )^2 \Big).
%\end{equation}
%Developing the squares in the exponential and disregarding all the terms independent of $\theta$, which enter the proportionality constant, we get
%\begin{equation}
%	\pi(\theta \mid \bar u) \propto \exp\Big(-\frac{(\theta^2 - 2\theta_0\theta)\gamma^2 + \big(u_\theta(\bar x)^2 - 2u_\theta(\bar x)\bar u\big)\sigma_0^2 }{2\sigma_0^2\gamma^2}\Big).
%\end{equation}
%We now replace the expression of $u_\theta(\bar x)$ with its analytic value and obtain
%\begin{equation}
%	\pi(\theta \mid \bar u) \propto \exp\Bigg(-\frac{(\theta^2 - 2\theta_0\theta)\gamma^2 + \big(\frac{\theta^2}{4}(\bar x - \bar x^2)^2 - \theta(\bar x - \bar x^2)\bar u\big)\sigma_0^2 }{2\sigma_0^2\gamma^2}\Bigg).
%\end{equation}
%Grouping the terms with respect to the powers of $\theta$, we get
%\begin{equation}
%	\pi(\theta \mid \bar u) \propto \exp\Bigg(-\frac{\big(\gamma^2 + \frac{\sigma_0^2}{4}(\bar x - \bar x^2)^2\big)\theta^2 - 2\big(\theta_0 \gamma^2 + \frac{\sigma_0^2}{2}(\bar x - \bar x^2)\bar u\big)\theta}{2\sigma_0^2\gamma^2}\Bigg).
%\end{equation}
%Rearranging the terms and completing the square at the numerator we get
%\begin{equation}
%	\pi(\theta \mid \bar u) \propto \exp\left(-\frac{\left(\theta - \frac{\theta_0 \gamma^2 + \frac{\sigma_0^2}{2}(\bar x - \bar x^2)\bar u}{\gamma^2 + \frac{\sigma_0^2}{4}(\bar x - \bar x^2)^2}\right)^2}{2\frac{\sigma_0^2\gamma^2}{\gamma^2 + \frac{\sigma_0^2}{4}(\bar x - \bar x^2)^2}}\right).
%\end{equation}
%This shows that the posterior distribution is a Gaussian $\pi(\theta \mid \bar u) \eqtext{D} \mathcal{N}(\mu_{\theta|\bar u}, \sigma^2_{\theta|\bar u})$, where the parameters are given by
%\begin{equation}
%\begin{aligned}
%	\mu_{\theta|\bar u} &= \frac{\theta_0 \gamma^2 + \frac{\sigma_0^2}{2}(\bar x - \bar x^2)\bar u}{\gamma^2 + \frac{\sigma_0^2}{4}(\bar x - \bar x^2)^2}, \\
%	\sigma^2_{\theta|\bar u} &= \frac{\sigma_0^2\gamma^2}{\gamma^2 + \frac{\sigma_0^2}{4}(\bar x - \bar x^2)^2}.
%\end{aligned}
%\end{equation}
%Let us remark that in the limit for $\gamma \to 0$, i.e., when the observation is noiseless, we have $\sigma^2_\theta \to 0$ and 
%\begin{equation}
%	\mu_{\theta|\bar u} \to \frac{\frac{\sigma_0^2}{2}(\bar x - \bar x^2)\frac{\bar \theta}{2}(\bar x - \bar x^2)}{\frac{\sigma_0^2}{4}(\bar x - \bar x^2)^2} = \bar \theta, \\
%\end{equation}
%which means that, as expected, the exact posterior distribution tends for noiseless observations to the Dirac delta $\delta_{\bar \theta}(\theta)$.
%
%Let us now consider an approximated posterior distribution, where the forward model, and thus the likelihood, are evaluated with a linear FEM solution, that we denote as usual by $u_h$. In particular, let us consider the space discretization parameter $h$ and $\bar x = h / 2$. For \eqref{eq:TestPDEInverse}, linear FEM are exact on the nodes of the mesh, hence $u_h(h / 2) = u(h) / 2$ due to homogeneous Dirichlet boundary conditions. Therefore, for any value of $\theta$, the forward model predicts
%\begin{equation}
%	u_h(h / 2) = \frac{\theta}{4}(h - h^2).
%\end{equation}
%With the same choice of prior and error model as before, it is easy to verify that in this case the posterior distribution $\pi_h(\theta\mid\bar u)$ is Gaussian with parameters
%\begin{equation}
%\begin{aligned}
%	\mu_{\theta|\bar u, h} &= \frac{\theta_0 \gamma^2 + \frac{\sigma_0^2}{4}(h - h^2)\bar u}{\gamma^2 + \frac{\sigma_0^2}{16}(h - h^2)^2}, \\
%	\sigma^2_{\theta|\bar u, h} &= \frac{\sigma_0^2\gamma^2}{\gamma^2 + \frac{\sigma_0^2}{16}(h - h^2)^2}.
%\end{aligned}
%\end{equation}
%In the noiseless limit, i.e., for $\gamma \to 0$, we find trivially that $\sigma_{\theta, h}^2 \to 0$ and the mean 
%\begin{equation}
%	\mu_{\theta|\bar u, h} \to \frac{\frac{\sigma_0^2}{4}(h - h^2)\frac{\bar \theta}{8}(2h - h^2)}{\frac{\sigma_0^2}{16}(h - h^2)^2} = \frac{2 - h}{2(1 - h)}\bar \theta.
%\end{equation}
%In the limit for $h \to 0$, the mean of the posterior distribution is unbiased, meaning that for $\gamma, h \to 0$ we have $\pi_h(\theta\mid \bar u) \to \delta_{\bar \theta}(\theta)$. Nonetheless, for any positive value $h > 0$, the posterior distribution converges to a biased value. For example, if $\bar \theta = 1$ and $h = 1 / 10$, we have that $\mu_{\theta, h} \to 19 / 18 \approx 1.06$ for $\gamma \to 0$.
%
%We now consider the probabilistic FEM solution. Let us suppose that the observation is still in $\bar x = h / 2$. In this case, the probabilistic method predicts
%\begin{equation}
%	U_h(h/2) = u(H_1) / 2 = \frac{\theta}{4}(H_1 - H_1^2),
%\end{equation}
%where $H_1 = \tilde x_1 = h + P_1$. For simplicity of notation, since the only random space discretization involved is $H_1$, we denote $H \defeq H_1$. Moreover, we denote by $\pi(H)$ the density of $H$. In this case, it is not possible to evaluate $\pi(\theta \mid \bar u)$ as the likelihood term $\pi(\bar u \mid \theta)$ depends on the auxiliary variable $H$. Nonetheless, we have
%\begin{equation}
%	\pi(\bar u \mid \theta) = \int_\R \pi(\bar u \mid \theta, H) \pi(H \mid \theta) \dd H.
%\end{equation}
%Hence, we have the following Bayes' rule
%\begin{equation}
%	\pi(\theta \mid \bar u) \propto \pi(\theta) \int_\R \pi(\bar u \mid \theta, H) \pi(H \mid \theta) \dd H.
%\end{equation}
%Replacing the definition of all the objects involved, and considering that $H \sim \mathcal{U}(-h^{p+1}/2, h^{p+1}/2)$ and that $H$ is independent of $\theta$, so that $\pi(H \mid \theta) = \pi(H)$, we get
%\begin{equation}
%	\pi_{h,\mathrm{prob}}(\theta \mid \bar u) \propto \exp\Big(-\frac{(\theta-\theta_0)^2}{2\sigma_0^2}\Big)\int_{-h^p/2}^{h^p/2} h^{-p}\exp\Big(-\frac{\big(\bar u - U_h(h/2)\big)^2}{2\gamma^2}\Big) \dd H.
%\end{equation}
%Unfortunately, \corr{the posterior distribution is not computable analytically in this case, as the integrand does not admit a closed form primitive.} It is nonetheless possible to draw values from the distribution with density $\pi_{h, \mathrm{prob}}$ using an unbiased estimator of the integral, for example obtained through a Monte Carlo simulation. It is indeed possible to prove (ref..) that an MCMC algorithm which employs an unbiased likelihood estimator targets the exact posterior distribution. 
%
%\section{Numerical experiments} 
%
%\begin{figure}[t!]
%	\begin{center}
%		\begin{tabular}{cc}%{c@{\hspace{0.3cm}}c}
%			\includegraphics[]{solution1Def} & \includegraphics[]{solution2Def} \\
%			\includegraphics[]{solution3Def} & \includegraphics[]{solution4Def} \\
%		\end{tabular}
%	\end{center}
%	\caption{Realizations of the probabilistic numerical solution (grey) compared to the deterministic finite element solution (black). The probabilistic method collapses to the classic solver.}
%	\label{fig:ResultsCollapse}
%\end{figure}
%
%\subsection{Convergence} We consider the following test elliptic PDE
%
%\begin{equation}\label{eq:TestPDE}
%\begin{aligned}
%	-u'' &= \sin(2\pi x), && \text{in } (0, 1),\\
%	u &= 0, && \text{in } \{0, 1\},
%\end{aligned}
%\end{equation}	
%and solve it using the probabilistic numerical integrator. In order to evaluate the error, we consider the closed-form exact solution of \eqref{eq:TestPDE}, which is given by
%\begin{equation}
%	u_{\mathrm{ex}} = \frac{1}{4\pi^2} \sin(2 \pi x).
%\end{equation}
%In Figure \ref{fig:ResultsCollapse} we show $M = 100$ realizations of the probabilistic solution obtained setting $h = 3 \cdot 2^{-1}$ for $i = 1, 2, 3, 4$ and with $p = 1$. It is possible to remark how the uncertainty due to the space discretization is accounted by the probabilistic method, as the realizations collapse towards the deterministic finite element solution when $h$ becomes smaller.
%
%We then consider $h = 3 \cdot 2^{-i}$ for $i = 1, 2, \ldots, 8$ and compute one realization of the numerical solution for the three values $p = \{1, 1.5, 2\}$, thus computing the $L^2$ error with respect to the exact solution $u_{\mathrm{ex}}$. In Figure \ref{fig:ResultsConvergence} we show the results we obtain, which confirm the theoretical result presented in Theorem \ref{thm:Convergence} and the validity of our analysis.
%
%\begin{figure}[t]
%	\centering
%	\includegraphics[]{L2Convergence}
%	\caption{Convergence in $L^2$ of the probabilistic numerical solution towards the exact solution of the PDE for different values of $p$.}
%	\label{fig:ResultsConvergence}
%\end{figure}
%
%\subsection{Bayesian inverse problems} Let us consider the following PDE (same test problem as in \cite{CGS16})
%\begin{equation}\label{eq:PDEStrongBayes}
%\begin{aligned}
%-\big(\kappa(x)u'\big)' &= 4x, && \text{in } (0, 1),\\
%	u(0) &= 0,\\
%	u(1) &= 2,
%\end{aligned}
%\end{equation}	
%where $\kappa(x)$ is piecewise constant on ten intervals in $(0, 1)$. In particular we choose $\kappa(x) = \kappa_i$ for $x \in (i/10, (i+1)/10)$ and $i = 0, \ldots, 9$, where the values $\kappa_i$ are given a prior $\kappa_i \sim \log \mathcal{N}(0, 1)$, i.e., $\kappa_i = \exp(\theta_i)$ for $\theta_i \sim \mathcal{N}(0, 1)$. We generate punctual observations $\bar u_i$ in $\bar x_i = 0.1, 0.2, \ldots, 0.9$, which are given by a realization $\bar \kappa$ of $\kappa$ taken from the prior and then an accurate solution of \eqref{eq:PDEStrongBayes} corrupted by zero-mean Gaussian additive noise with standard deviation $10^{-5}$. We wish then to retrieve the value of $\bar \kappa$, i.e., the true value of $\kappa$, using a Metropolis-Hastings algorithm. In order to approximate the likelihood and hence compute the posterior distribution for each guess of $\kappa$, we use either deterministic FEM or the probabilistic method presented above. We choose the robust adaptive MCMC (RAM) \cite{Vih12} for both the choices of the forward solver and apply a random pseudo-marginal algorithm for the probabilistic method \cite{MLR16}. In particular, we tune the RAM algorithm to attain the optimal acceptance ratio of 23.4\%.

\bibliographystyle{siamplain}
\bibliography{anmc}
\end{document}