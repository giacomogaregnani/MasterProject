\documentclass{siamart1116}

% basics
\usepackage[left=3cm,right=3cm,top=3cm,bottom=4cm]{geometry}
\usepackage[utf8x]{inputenc}
\usepackage[title,titletoc]{appendix}
\usepackage{afterpage}

% maths
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\newsiamremark{assumption}{Assumption}
\newsiamremark{remark}{Remark}
\numberwithin{theorem}{section}

% plots
\usepackage{pgfplots} 
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{here}
\usepackage[labelfont=bf]{caption}

% title and authors
\newcommand{\TheTitle}{Convergence properties of Monte Carlo estimators drawn from probabilistic integrators of ordinary differential equations} 
\newcommand{\TheAuthors}{A. Abdulle, G. Garegnani}
\headers{Monte Carlo estimators drawn from probabilistic integrators of ODEs}{\TheAuthors}
\title{{\TheTitle}}
\author{Assyr Abdulle\thanks{Mathematics Section, \'Ecole Polytechnique F\'ed\'erale de Lausanne (\email{assyr.abdulle@epfl.ch})}
	\and
	Giacomo Garegnani\thanks{Mathematics Section, \'Ecole Polytechnique F\'ed\'erale de Lausanne (\email{giacomo.garegnani@epfl.ch})}}

% my commands 
\DeclarePairedDelimiter{\ceil}{\left\lceil}{\right\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\|}{\|}
\renewcommand{\phi}{\varphi}
\newcommand{\eqtext}[1]{\ensuremath{\stackrel{#1}{=}}}
\newcommand{\leqtext}[1]{\ensuremath{\stackrel{#1}{\leq}}}
\newcommand{\iid}{\ensuremath{\stackrel{\text{i.i.d.}}{\sim}}}
\newcommand{\totext}[1]{\ensuremath{\stackrel{#1}{\to}}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\epl}{\varepsilon}
\newcommand{\diffL}{\mathcal{L}}
\newcommand{\prior}{\mathcal{Q}}
\newcommand{\defeq}{\coloneqq}
\newcommand{\eqdef}{\eqqcolon}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\E}{\operatorname{\mathbb{E}}}
\newcommand{\MSE}{\operatorname{MSE}}
\newcommand{\MH}{\mathrm{MH}}
\newcommand{\ttt}{\texttt}
\newcommand{\Hell}{d_{\mathrm{Hell}}}
\newcommand{\sksum}{\textstyle\sum}
\newcommand{\dd}{\mathrm{d}}

\ifpdf
\hypersetup{
	pdftitle={\TheTitle},
	pdfauthor={\TheAuthors}
}
\fi

\begin{document}
	
\maketitle

\begin{abstract} We consider a probabilistic integrator for ordinary differential equations (ODEs) which has recently been introduced \cite{CGS16}. The existing analysis of strong and weak convergence is integrated in this work with a novel bound on the mean square error (MSE) of estimators obtained with Monte Carlo simulations. Exploiting this result, we deduce a result of convergence of the posterior distribution in the frame of Bayesian inverse problems involving parameterized ODEs.	
\end{abstract}

\section{Introduction} Extremely efficient deterministic methods for ordinary differential equations (ODEs) have been developed and extensively studied in the last decades, although only recently a serious effort has been made to quantify in a probabilistic manner the natural error introduced by numerical schemes. In this work we consider an integrator \cite{CGS16} for ODEs that accounts for the statistical uncertainty given by the error. The main idea laying behind this novel numerical scheme is building a set of trajectories from the realizations of a random process, thus providing a statistical measure of the numerical solution which is not punctual as in the deterministic case.

It has been empirically observed that the probabilistic method has better performances than its classic deterministic counterpart in the context of Bayesian inference inverse problems \cite{CGS16}. In particular, the posterior distributions obtained with Markov chain Monte Carlo methods (MCMC) seem to account well for the uncertainty given by the numerical approximation.

The properties of strong and weak convergence of this numerical scheme have been extensively analyzed. In this work, we consider Monte Carlo estimators drawn from this probabilistic integrator and deduce a novel bound on their mean square error. In particular, we show that Monte Carlo estimators converge independently of the number of trajectories chosen in simulations when the ODE is fully deterministic. This result implies a remarkable gain in computational cost, as only an $\OO(1)$ number of trajectories is needed if the time step is small enough. Finally, we apply this bound to Bayesian inference inverse problems involving parameterized ODEs, thus predicting the distance between approximated and exact posterior distributions of the inferred parameter.

The structure of this work is the following. In Section \ref{sect:ProbMethod} we present the probabilistic solver of ODEs together with its properties of convergence. Then, in Section \ref{sect:MonteCarlo} we show the main result of this work concerning Monte Carlo estimators drawn from the probabilistic solution. We show in Section \ref{sect:Bayes} how this result applies in the frame of Bayesian inference inverse problems. Finally, we present a numerical experiment in Section \ref{sect:NumExp}, thus drawing our final considerations in the conclusion.

\section{Probabilistic methods for ODEs}\label{sect:ProbMethod}
Let us consider a function $f\colon \R^d \to \R^d$ and the following autonomous ODE
\begin{equation}\label{eq:ODE}
\begin{aligned}
	u'(t) &= f(u), &&  t \in (0, T], \\
	u(0)  &= u_0, && u_0 \in \R^d,
\end{aligned}
\end{equation}
where $d$ is the dimension of the problem and $u_0$ is the initial condition. It is possible to write the solution of \eqref{eq:ODE} in terms of the flow of the differential equation $\Phi\colon\R\times\R^d\to\R^d$ as
\begin{equation}
	u(t) = \Phi(t, u_0).
\end{equation}
Given a time step $h >0$, we now consider a discretization of the time interval $[0, T]$ defined as $t_k = kh$, with $k = 0, \ldots, N$, where $T = Nh$, and the numerical solution $u_k$ approximating $u(t_k)$ given by a Runge-Kutta method \cite{HLW02}. The numerical solution can be written in terms of the flow $\Psi\colon\R\times\R^d\to\R^d$ of the numerical method as
\begin{equation}
	u_{k+1} = \Psi(h, u_k).
\end{equation}
In the probabilistic method we analyze in this work, first introduced in \cite{CGS16}, we consider independent identically distributed (i.i.d.) random variables $\xi_k$, opportunely scaled with respect to the time step, adding them to the deterministic numerical solution as 
\begin{equation}\label{eq:probabilisticMethod}
\begin{aligned}
U_{k+1} &= \Psi(h, U_k) + \xi_k(h), \\
U_0 &= u_0.
\end{aligned}
\end{equation}
The convergence of the numerical scheme has been extensively analyzed \cite{CGS16}. We report here the main results that are needed in this work. 

The following assumptions are needed to prove a result of strong convergence. The first assumption concerns the random variables $\xi_k(h)$ in \eqref{eq:probabilisticMethod}.
\begin{assumption}\label{ass:AssumptionOne} There exist constants $p > 1$, $C > 0$ such that the random variables $\xi_k(t)$ satisfy
\begin{equation}
	\E|\xi_k(t)\xi_k(t)^T|^2_F \leq Ct^{2p+1}.
\end{equation}
Furthermore, there exists a matrix $Q$ independent of $h$ such that 
\begin{equation}
	\E\xi_k(h)\xi_h(h)^T = Qh^{2p+1}.
\end{equation}
\end{assumption}
Moreover, it is necessary to assume that the deterministic component identified by its flow $\Psi$ converges to the true solution of \eqref{eq:ODE}. 
\begin{assumption}\label{ass:AssumptionTwo}  The function $f$ and a sufficient number of its derivatives are bounded uniformly in $\R^n$ in order to ensure that $f$ is globally Lipschitz and that the numerical flow map $\Psi$ has uniform local truncation error of order $q + 1$, i.e., 
	\begin{equation}
		\sup_{u\in\R^n} |\Psi_t(u) - \Phi_t(u)| \leq Kt^{q+1}.
	\end{equation}
\end{assumption}
Under these two assumptions, it is possible to prove the following result \cite{CGS16}.
\begin{theorem}[Strong Convergence]\label{thm:strongConv} Under assumptions \ref{ass:AssumptionOne} and \ref{ass:AssumptionTwo}, there is a constant $C>0$ such that
	\begin{equation}\label{strongConvDisc}
		\sup_{0<kh<T} \E|u_k - U_K|^2 \leq Ch^{2\min\{p,q\}}.
	\end{equation}
	Furthermore, the continuous version $U(t)$ of the numerical solution given by \ref{eq:probabilisticMethod} satisfies
	\begin{equation}\label{strongConvCont}
		\sup_{0\leq t \leq T} \E|u(t) - U(t)| \leq Ch^{\min\{p, q\}}.
	\end{equation}
\end{theorem}

Under a further stability assumption, a result of weak convergence can be proven. 
\begin{assumption}\label{ass:AssumptionThree} The function $f$ in \eqref{eq:ODE} is in $\mathcal C^\infty$ and all its derivatives are uniformly bounded in $\R^n$. Furthermore, $f$ is such that for all functions $\phi$ in $\mathcal C ^\infty(\R^n, \R)$ 
	\begin{equation}
	\begin{aligned}
		\sup_{u\in\R^n} \abs{\phi(\Phi(h, u))} &\leq (1 + Lh) \sup_{u\in\R^n} \left|\phi(u)\right|, \\
		\sup_{u\in\R^n} \abs{\E \phi(U_1\mid U_0 = u)} &\leq (1 + Lh) \sup_{u\in\R^n} \left|\phi(u)\right|, \\
	\end{aligned}
	\end{equation}
	for a constant $L > 0$.
\end{assumption}

It is then possible to prove the following result \cite{CGS16}.
\begin{theorem}[Weak convergence]\label{thm:WeakConvergence} Under assumptions \ref{ass:AssumptionOne}, \ref{ass:AssumptionTwo} and \ref{ass:AssumptionThree} and for any function $\phi$ in $\mathcal{C}^\infty(\R^d, \R)$, there is a constant $C > 0$ such that
\begin{equation}
	\abs{\phi(u(T)) - \E\phi(U_N)} \leq Ch^{\min\{2p, q\}}, \quad Nh = T,
\end{equation}
	where $u$ is the solution of \eqref{eq:ODE}.
\end{theorem}

We will exploit the results presented in this section to deduce the novel properties we analyze in this work.

\section{Convergence of Monte Carlo estimators}\label{sect:MonteCarlo}
As it has been highlighted in \cite{KeH16}, the behavior of Monte Carlo estimators computed using \eqref{eq:probabilisticMethod} has not been investigated yet. Given a function $\phi\colon\R^d\to\R$ in $\mathcal{C}^\infty(\R^d, \R)$, we consider the unbiased estimator $\hat Z$ defined as
\begin{equation}
	\hat Z \defeq M^{-1} \sksum_{i = 1}^M \phi(U_N^{(i)}),
\end{equation}
where $M\in\N^*$ is the number of values $U^{(i)}_N$, with $i = 1, \ldots, M$, which are drawn from the numerical solution at final time $T = Nh$. In particular, we are interested in controlling the convergence of the mean square error (MSE) of $\hat Z$ with respect to the time step $h$ and the number of trajectories $M$. Thanks to the result of weak convergence of Theorem \ref{thm:WeakConvergence}, we have a first bound given by
\begin{equation}\label{eq:MSEdefinition}
\begin{aligned}
	 \MSE(\hat Z) &= \Var\hat Z + \big(\E(\hat Z - \phi(u(T)))\big)^2\\
	  &\leq \Var\hat Z + C h^{2\min\{2p, q\}},
\end{aligned}
\end{equation}
where $C$ is a positive constant. The variance of the estimator can be bounded by considering the structure of Runge-Kutta methods together with the additive nature of the introduced noise. Let us remark that trivially from the independence of samples we have
\begin{equation}
\begin{aligned}
	\Var\hat Z &= \Var M^{-1} \sksum_{i = 1}^M \phi(U_N^{(i)}) \\
	&= M^{-1} \Var\phi\left(U_N\right). \\
\end{aligned}
\end{equation}
Moreover, if we denote by $L_\phi$ the Lipschitz constant of the function $\phi$ we have
\begin{equation}\label{eq:BoundOfVarHatZ}
	\Var\hat Z \leq M^{-1} L_\phi^2 \Var U_N.
\end{equation}
Therefore, it is sufficient to bound the variance of the numerical solution itself in order to obtain a bound on the MSE. A further assumption on the Runge-Kutta method is needed.
\begin{assumption}\label{ass:AssumptionRK} The flow of the Runge-Kutta method satisfies
	\begin{equation}
		\abs{\Psi_h(y) - \Psi_h(z)} \leq (1 + Ch)\abs{y - z}, \text{ for all } y, z \in \R^d,
	\end{equation}
	where $C > 0$ is a constant independent of $h$.
\end{assumption}
We can now prove the following result.

\begin{lemma}\label{lem:VarianceSolution} Consider the numerical method \eqref{eq:probabilisticMethod} applied to a one-dimensional ODE with $\Psi$ any Runge-Kutta method satisfying Assumption \ref{ass:AssumptionRK}. Then, under Assumption \ref{ass:AssumptionOne}, the solution $U_k$ at time $t_k = kh$ satisfies
	\begin{equation}\label{eq:BoundVariance}
			\Var U_k \leq C_1 \Var U_0 + C_2 Q h^{2p}, \quad k = 1, \ldots, N, 
	\end{equation}
	where $C_1$, $C_2 > 0$ are constants independent of $h$, and where $p$ is the exponent of Assumption \ref{ass:AssumptionOne}.
\end{lemma}
\begin{proof} Let us recall that for any Lipschitz function $g$ of constant $L$ and random variable $Y$, it is true that
	\begin{equation}\label{eq:VarLipschitz}
		\Var g(Y) \leq L^2 \Var Y.
	\end{equation}
	Let us consider the numerical solution, since $\xi_k(h)$ is independent of $U_k$, for any $k = 1, 2, \ldots$, we have
	\begin{equation}
		\Var U_k =  \Var\Psi_h(U_{k-1}) + Qh^{2p+1},
	\end{equation}
	where we exploited Assumption \ref{ass:AssumptionOne}. Then, thanks to \eqref{eq:VarLipschitz} and under Assumption \ref{ass:AssumptionRK}, we have
	\begin{equation}
		\Var U_k \leq (1 + Ch)^2 \Var U_{k-1} + Qh^{2p+1}.
	\end{equation}
	Recurring on $k$, we get
	\begin{equation}
		\Var U_k \leq (1 + Ch)^{2k} \Var U_0 + \sksum_{i=0}^{k-1} (1 + Ch)^{2i} Qh^{2p + 1}.
	\end{equation}
	The first term is bounded as
	\begin{equation}
		(1 + Ch)^{2k} \leq e^{2CT}.
	\end{equation}
	For the second term, we have that 
	\begin{equation}
		\sksum_{i=0}^{k-1} (1 + Ch)^{2i} \leq T e^{2CT}h^{-1}.
	\end{equation}
	Hence, we fix
	\begin{equation}
		C_1 = e^{2CT}, \quad C_2 =  T e^{2CT},
	\end{equation}
	thus proving the desired result.
\end{proof}

We can now state the result of convergence for the MSE of the Monte Carlo estimator.

\begin{theorem}\label{thm:MSE} Under the assumptions of Lemma \ref{lem:VarianceSolution} and if the function $\phi$ in $\mathcal C^\infty(\R^d, \R)$ is Lipschitz continuous, the following bound for the MSE of $\hat Z$ is valid
	\begin{equation}
		\MSE(\hat Z) \leq C_1 h^{2\min\{2p, q\}} + C_2 M^{-1} (\Var(U_0) + h^{2p}),
	\end{equation}	
	where $C_1$, $C_2$ are positive constants independent of $h$.	
\end{theorem}

\begin{proof}
	The result is trivially proved replacing \eqref{eq:BoundVariance} in \eqref{eq:BoundOfVarHatZ} and then in \eqref{eq:MSEdefinition}.
\end{proof}

\begin{remark} If the initial condition $u_0$ is deterministic and $U_0 = u_0$, the MSE of the Monte Carlo estimator is bounded by a function of the time step as
	\begin{equation}
		\MSE(\hat Z) \leq C_1 h^{2\min\{2p, q\}} + C_2 M^{-1} h^{2p}.
	\end{equation}
	Hence, the estimator $\hat Z$ convergence in the mean square sense towards the solution of the ODE regardless of the number of trajectories that are used in a simulation context.
\end{remark}

\section{Bayesian inference of parametrized ODEs}\label{sect:Bayes}

It has been shown in \cite{CGS16} that the numerical solution provided by the probabilistic method \eqref{eq:probabilisticMethod} has a favorable behavior in the context of Bayesian inference problems involving ODEs. Let us consider a real parameter $\theta$ in a domain $\Omega \subset \R^n$, with $n \in \N^*$, a function $f \colon \R^d\times \Omega \to\R^d$ and the ODE
\begin{equation}\label{eq:ParamODE}
\begin{aligned}
	u'(t, \theta) &= f(u, \theta), &&  t \in (0, T], \\
	u(0, \theta)  &= u_0(\theta), && u_0(\theta) \in \R^d,
\end{aligned}
\end{equation}
where we highlight the dependence of the solution $u$ on the value of the parameter. We then consider a set of data that we denote by $\mathcal{Y} = \{y_1, \ldots, y_D\}$, where $D \in \N^*$ is the number of observations. In particular, the values $y_i \in \R^d$, with $i = 1, 2, \ldots, D$, are given by an additive Gaussian perturbation of the state computed at the true value $\bar \theta$ of the parameter, i.e.
\begin{equation}
	y_i = u(t_i, \bar \theta) + \epl_i, \quad \epl_i \iid \mathcal{N}(0, \Gamma), \quad i = 1, \ldots, D,
\end{equation}
where $\Gamma \in \R^{d\times d}$ is the covariance matrix of the observational noise.

Given the observations, it is possible to perform inference on the value of the parameter $\theta$ following Bayes' rule, i.e.
\begin{equation}\label{eq:ExactPosterior}
	\pi(\theta\mid\mathcal Y) \propto \prior(\theta) \diffL(\mathcal Y\mid \theta),
\end{equation}
where $\pi$ is the posterior distribution, $\prior$ is the prior distribution on the parameter and $\diffL$ is the likelihood function. The proportionality constant is not of interest in this discussion, and can be retrieved by normalizing $\pi$ so that it is a probability density function. 

Since \eqref{eq:ParamODE} does not admit a closed-form solution, we approximate numerically the value of the likelihood function. We choose to exploit the probabilistic method \eqref{eq:probabilisticMethod}, averaging over the variable $\xi$ to obtain the posterior distribution \cite{CGS16} as 
\begin{equation}\label{eq:PosteriorSemiEstimator}
	\pi_h(\theta\mid\mathcal Y) \propto \prior(\theta) \int  \diffL^\xi_h(\mathcal Y\mid \theta) \dd \xi,	
\end{equation}
where $\pi_h$ and $\diffL^\xi_h$ are the approximated posterior and likelihood obtained with \eqref{eq:probabilisticMethod} with time step $h$. The value of $\pi_h$ is still not computable in practice, as it requires evaluating the exact expectation of $\diffL^\xi_h$, which is not accessible. Hence, a Monte Carlo simulation has to be performed in order to retrieve an estimator of $\pi_h$. In particular, we compute $M$ trajectories of \eqref{eq:probabilisticMethod}, thus obtaining the estimator $\hat \pi_{h, M}$ defined as
\begin{equation}
	\hat \pi_{h,M}(\theta\mid\mathcal Y) \propto \prior(\theta) \hat \diffL_{h,M}(\mathcal Y\mid \theta),
\end{equation}
where the estimator of the likelihood function is given by the Monte Carlo average
\begin{equation}
	\hat \diffL_{h,M}(\mathcal Y\mid \theta) = M^{-1}\sksum_{i=1}^M \diffL^{\xi^{(i)}}_h(\mathcal Y\mid \theta).
\end{equation}
Here, we denote by $\diffL^{\xi^{(i)}}$ the likelihood computed using the $M$ realizations of \eqref{eq:probabilisticMethod}.

The estimator of the posterior distribution can be exploited in the pseudo-marginal versions of the Metropolis-Hastings MCMC algorithms \cite{ADH10, DPD15}, which are used to generate samples from $\pi$. Exploiting the result on Monte Carlo averages in Theorem \ref{thm:MSE}, we can quantify the average distance between the posterior distribution and its estimator with respect to the time step $h$ and the number of trajectories $M$.

\begin{theorem}\label{thm:HellingerTheorem} If the initial condition of \eqref{eq:ParamODE} is deterministic and under the assumptions of Theorem \ref{thm:MSE},
	\begin{equation}\label{eq:HellingerTheorem}
		\E \Hell(\pi(\theta|\mathcal{Y}), \hat \pi_{h,M}(\theta|\mathcal{Y})) \leq C \big(h^{2\min\{2p, q\}} + M^{-1}h^{2p}\big)^{1/2}, 
	\end{equation}
	where we maintained the notation above, $\Hell(\cdot, \cdot)$ denotes the Hellinger distance and $C$ is a positive constant independent of $h$ and $M$.
\end{theorem}
\begin{proof} By definition of the Hellinger distance we have
	\begin{equation}
		2\E\Hell^2(\pi(\theta\mid\mathcal{Y}),\hat\pi^M_h(\theta\mid\mathcal{Y})) = \E\int_\Omega (\pi(\theta\mid\mathcal{Y})^{1/2} - \hat\pi_{h, M}(\theta\mid\mathcal{Y})^{1/2}) \dd \theta.
	\end{equation}
	Replacing the definition of $\pi$ and $\pi_{h, M}$ we get
	\begin{equation}
		2\E\Hell^2(\pi(\theta\mid\mathcal{Y}),\hat\pi^M_h(\theta\mid\mathcal{Y})) = \int_\Omega \E \big(\diffL(\mathcal{Y}\mid\theta)^{1/2} - \diffL_{h,M}(\mathcal Y\mid\theta)^{1/2}\big)^2 \prior(\theta)\dd \theta.
	\end{equation}
	Furthermore, by definition of the MSE we get
	\begin{equation}
		2\E\Hell^2(\pi(\theta\mid\mathcal{Y}),\hat\pi^M_h(\theta\mid\mathcal{Y})) = \int_\Omega \MSE\big(\diffL_{h,M}(\mathcal Y\mid\theta)^{1/2}\big) \prior(\theta)\dd \theta.
	\end{equation}
	We now exploit Theorem \ref{thm:MSE} thus obtaining
	\begin{equation}
		\E\Hell^2(\pi(\theta\mid\mathcal{Y}),\hat\pi^M_h(\theta\mid\mathcal{Y})) \leq \tilde C(\theta) \big(h^{2\min\{2p, q\}} + M^{-1}h^{2p}\big).
	\end{equation}
	Let us remark that the constant $\tilde C(\theta)$ can depend on the value of $\theta$, as in Theorem \ref{thm:MSE} the constants appearing in the final bound depend on the Lipschitz constant $L_f$ of the function $f$. We now apply Jensen's inequality to get 
	\begin{equation}
		\E\Hell(\pi(\theta\mid\mathcal{Y}),\hat\pi^M_h(\theta\mid\mathcal{Y})) \leq \tilde C(\theta)^{1/2} \big(h^{2\min\{2p, q\}} + M^{-1}h^{2p}\big)^{1/2}.
	\end{equation}
	We then consider the constant $C$ appearing in \eqref{eq:HellingerTheorem} to be defined as
	\begin{equation}
		C = \sup_{\theta \in \Omega} \tilde C(\theta)^{1/2},
	\end{equation}
	thus obtaining the desired result.
\end{proof}

\begin{remark} The bound above can be used to set the number of trajectories $M$ needed to approximate the acceptance probability in a Metropolis-Hastings algorithm. Since the quality of the approximation of the posterior function is practically dependent only on the time step, we believe that if $h$ is small enough it is sufficient to choose $M = \OO(1)$, therefore saving computational time.
\end{remark}

\begin{remark} In practice, the value of $p$ appearing in Assumption \ref{ass:AssumptionOne} is fixed to the order $q$ of the chosen Runge-Kutta scheme \cite{CGS16}. Hence, the bound above simplifies to
	\begin{equation}
		\E\Hell(\pi(\theta\mid\mathcal{Y}),\pi^M_h(\theta\mid\mathcal{Y})) \leq C h^{q},
	\end{equation}
	where we assumed $M^{-1}$ to be an $\OO(1)$.
\end{remark}

\section{Numerical experiment}\label{sect:NumExp}

\begin{figure}
	\centering
	\begin{subfigure}{0.49\linewidth}
		\centering
		\resizebox{1.0\linewidth}{!}{\input{plots/MonteCarloVariance2.tikz}}
		\caption{Variation of the time step.}
		\label{fig:MonteCarloVarianceH}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\centering
		\resizebox{1.0\linewidth}{!}{\input{plots/MonteCarloVariance3.tikz}}
		\caption{Variation of the number of trajectories.}
		\label{fig:MonteCarloVarianceM}
	\end{subfigure}
	\caption{Variance and squared bias of the Monte Carlo estimator $\hat Z$ with Explicit Euler and RK4 applied to \eqref{eq:FitzNag}. The two components of the MSE have the same order of convergence with respect to the time step $h$. Conversely, the order of convergence with respect to the number of trajectories $M$ with fixed $h$ of the variance of $\hat Z$ is equal to one for both methods.}
	\label{fig:MonteCarloVariance}
\end{figure}

We consider the FitzHug-Nagumo problem, defined by the following ODE
\begin{equation}\label{eq:FitzNag}
\begin{aligned}
	x' &= c\big(x - \frac{x^3}{3} + y\big), && x(0) = -1, \\
	y' &= -\frac{1}{c}(x - a + by), && y(0) = 1,
\end{aligned}
\end{equation}
here $a, b, c$ are real parameters with values $a = 0.2$, $b = 0.2$, $c = 3$, and integrate it up to the final time $T = 10$ with the probabilistic integrator. We choose the function $\phi$ to be given by $\phi(X) = X^TX$ and generate a reference solution $Z$ with an high-order numerical scheme on a fine time discretization. We then choose as deterministic integrator the explicit Euler method and the classic fourth-order Runge-Kutta scheme \cite{HLW02}. Moreover, we set the noise scale $p$ equal to $q$, i.e., one and four respectively. We choose the number of trajectories $M = 10$ and the time step $h = 0.5 / 2^i$ with $i = 0, 1, \ldots, 11$. We then repeat $300$ times the computation of the estimator $\hat Z$ for all the values of the time step, thus estimating its variance and bias. Numerical results (Figure \ref{fig:MonteCarloVarianceH}) confirm the theoretical bound presented in Theorem \ref{thm:MSE}, as the order of convergence of the variance of $\hat{Z}$ to zero is of order $2$ and $8$ with respect to $h$ for Explicit Euler and RK4 respectively independently of $M$. We perform a second experiment fixing the value of $h$ to $0.5$ and varying the number of trajectories in the values $M = 2^i$ with $i = 0, 1, \ldots, 9$. As in the first experiment, we compute 300 times $\hat Z$ in order to estimate its variance. Results (Figure \ref{fig:MonteCarloVarianceM}) show that the variance has an order equal to $1$ for both the methods with respect to $M^{-1}$, thus confirming the theoretical result.

\section{Conclusion} We considered the probabilistic numerical scheme \eqref{eq:probabilisticMethod} that has been recently introduced \cite{CGS16} to solve numerically ODEs and we analyzed the behavior of Monte Carlo estimators drawn from its realizations. Moreover, we considered Bayesian inverse problems involving ODEs and showed how the posterior distribution is approximated when the likelihood function is computed through the trajectories of \eqref{eq:probabilisticMethod}. In particular, we deduced that if the reference ODE \eqref{eq:ODE} has deterministic initial conditions, Monte Carlo estimators converge to the true value of the solution independently of the simulated number of trajectories.

\bibliographystyle{siamplain}
\bibliography{anmc}
\end{document}
