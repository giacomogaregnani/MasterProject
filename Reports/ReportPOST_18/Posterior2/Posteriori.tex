\documentclass[10pt]{article}

\input{ex_shared}

\begin{document}
\maketitle	

\begin{abstract}
\end{abstract}

\textbf{Keywords.}

\textbf{AMS classification subjects.}


\section{Introduction}

\section{Prob. methods}

\begin{assumption}\label{ass:RVs} Assumption on the random variables.
\end{assumption}

\section{Technical part}

\begin{assumption} Assumptions on $f$ and on $\Psi_h$.
\end{assumption}

\begin{theorem} Error growth of deterministic RK \cite{CaH95b}. 
\end{theorem}

\begin{lemma}\label{lem:CondExp} For the additive noise method
	\begin{equation}
		\E(Y_n \mid Y_{n-j}) = \Psi_h^j(Y_{n-j}) + \OO\big((j-1)h^{2p+1}\big)
	\end{equation}
\end{lemma}

\begin{proof} We prove the result by induction on the index $j$. First, consider $\E(Y_n \mid Y_{n-1})$, which is trivially
	\begin{equation}
		\E(Y_n \mid Y_{n-1}) = \E\big(\Psi_h(Y_{n-1}) + \xi_{n-1} \mid Y_{n-1}\big) = \Psi_h(Y_{n-1}).
	\end{equation}
	Iterating once and exploiting the properties of conditional expectations, we get
	\begin{equation}
	\begin{aligned}
		\E(Y_n \mid Y_{n-2}) &= \E\big(\E(Y_n \mid Y_{n-1}) \mid Y_{n-2}\big) \\
		&= \E\big(\Psi_h(Y_{n-1}) \mid Y_{n-2}\big)\\
		&= \E\big(\Psi_h(\Psi_h(Y_{n-2}) + \xi_{n-2}) \mid Y_{n-2}\big).
	\end{aligned}
	\end{equation}
	A Taylor expansion, the properties of conditional expectations and \cref{ass:RVs} give
	\begin{equation}
	\begin{aligned}
		\E(Y_n \mid Y_{n-2}) &= \E\big(\Psi_h^2(Y_{n-2}) + \DD\Psi_h(Y_{n-2})\xi_{n-2} \\
		&\quad\quad\quad + \DD^2\Psi_h(Y_{n-2})(\xi_{n-2}, \xi_{n-2}) + \ldots \mid Y_{n-2}\big)\\
		&= \Psi_h^2(Y_{n-2}) + \OO(h^{2p+1}).
	\end{aligned}
	\end{equation}
	The induction step can then be written as
	\begin{equation}
	\begin{aligned}
		\E(Y_n \mid Y_{n-j}) &= \E\big(\E(Y_n \mid Y_{n-j+1}) \mid Y_{n-j}\big)\\
		&= \E\big( \Psi_h^{j-1}(Y_{n-j+1}) \mid Y_{n-j}\big) + \OO\big((j-2)h^{2p+1}\big)\\
		&= \E\big( \Psi_h^{j-1}(\Psi_h(Y_{n-j}) + \xi_{n-j}) \mid Y_{n-j}\big) + \OO\big((j-2)h^{2p+1}\big).
	\end{aligned}
	\end{equation}
	Expanding $\Psi_h^{n-j}$ in a Taylor series we get
	\begin{equation}
	\begin{aligned}
		\E(Y_n \mid Y_{n-j}) &= \E\big(\Psi_h^j(Y_{n-j}) + \DD\Psi_h^{j-1}(Y_{n-j})\xi_{n-j} \mid Y_{n-j}\big) + \OO\big((j-1)h^{2p+1}\big)\\
		&= \Psi_h^j(Y_{n-j}) + \OO\big((j-1)h^{2p+1}\big),
	\end{aligned}
	\end{equation}
	which proves the desired result.
\end{proof}

\begin{lemma} For the RTS-RK method
	\begin{equation}
	\E(Y_n \mid Y_{n-j}) = \ldots 
	\end{equation}
\end{lemma}

\begin{proof} Let us first consider the difference between a single step of a fixed time step method with respect to the RTS-RK method. Any Runge-Kutta method of order $q \geq 1$ can be written for any $y \in \R^d$ and $z > 0$ as
	\begin{equation}
		\Psi_z(y) = y + z f(y) + z^2 R(y),
	\end{equation}
	where $R(y)$ is a function depending on the coefficients of the method, on the function $f$ as well as on $z$. Hence, for a generic random time step $H$, we have
	\begin{equation}\label{eq:diffFixedRTS}
		\Psi_H(y) - \Psi_h(y) = (H - h) f(y) + (H^2 - h^2) R(y).
	\end{equation} 
	Let us now proceed by induction. For the first step we have thanks to the properties of conditional expectations and \eqref{eq:diffFixedRTS}
	\begin{equation}
	\begin{aligned}
		\E(Y_n \mid Y_{n-1}) &= \E(\Psi_{H_{n-1}}(Y_{n-1}) \mid Y_{n-1}) \\
		&= \E(\Psi_h(Y_{n-1}) + \Psi_{H_{n-1}}(Y_{n-1}) - \Psi_h(Y_{n-1})\mid Y_{n-1}) \\
		&= \Psi_h(Y_{n-1}) + \OO\big(\E(H_{n-1}^2 - h^2)\big).
 	\end{aligned}
 	\end{equation}
 	Conditioning with respect to the previous step therefore gives
 	\begin{equation}
 	\begin{aligned}
		\E(Y_n \mid Y_{n-2}) &= \E\big(\E(Y_n \mid Y_{n-1}) \mid Y_{n-2}\big) \\
		&= \E\big(\Psi_h(Y_{n-1})\mid Y_{n-2}\big) + \OO\big(\E(H_{n-1}^2 - h^2)\big).
 	\end{aligned}
 	\end{equation}
 	Goes on like before...
\end{proof}

\begin{theorem} For the additive noise method
	\begin{equation}
		\norm{\Var Y_n}_F \leq C_1 t_n h^{2p} + C_2 t_n^4 h^{4p-2},
	\end{equation}
	where $C_1, C_2$ are positive constants independent of $h$ and $n$.
\end{theorem}
\begin{proof} Let us first remark that since $\Var Y_n$ is symmetric positive definite we have
	\begin{equation}
		\norm{\Var Y_n}_F \leq \trace(\Var Y_n) = \E \norm{Y_n - \E Y_n}^2. 
	\end{equation}
	Let us therefore work with the right hand side of the inequality above. By a telescopic sum, we obtain
	\begin{equation}
	\begin{aligned} 
		\E &\norm{Y_n - \E Y_n}^2 = \E\norm{\E (Y_n \mid Y_n) - \E (Y_n \mid y_0)}^2\\
		&= \E \norm{\sum_{j=0}^{n-1} \E(Y_n \mid Y_{n-j}) - \E(Y_n \mid Y_{n-j-1}) }^2 \\
		&= \sum_{j=0}^{n-1} \E\norm{\E(Y_n \mid Y_{n-j}) - \E(Y_n \mid Y_{n-j-1})}^2 \\
		&\quad + 2 \sum_{j=0}^{n-1} \sum_{k<j} \E \Big(\E(Y_n \mid Y_{n-j}) - \E(Y_n \mid Y_{n-j-1}), \E(Y_n \mid Y_{n-k}) - \E(Y_n \mid Y_{n-k-1})\Big)
	\end{aligned}
	\end{equation}
	Let us consider the first term. Thanks to \cref{lem:CondExp}, we have
	\begin{equation}\label{eq:Increments}
	\begin{aligned}
		\E(Y_n \mid Y_{n-j}) - \E(Y_n \mid Y_{n-j-1}) &= \Psi_h^j(Y_{n-j}) - \Psi_h^{j+1}(Y_{n-j-1}) + \OO\big(jh^{2p+1}\big)\\
		&= \Psi_h^j\big(\Psi_h(Y_{n-j-1}) + \xi_{n-j-1}\big) - \Psi_h^{j+1}(Y_{n-j-1}) \\
		&\quad + \OO\big(jh^{2p+1}\big)\\
		&= \DD \Psi_h^j(\Psi_h(Y_{n-j-1}))\xi_{n-j-1} \\
		&\quad + \DD^2 \Psi_h^j(\Psi_h(Y_{n-j-1}))(\xi_{n-j-1}, \xi_{n-j-1}) + \OO\big(jh^{2p+1}\big).
	\end{aligned}
	\end{equation}
	Hence, we get for a constant $C > 0$
	\begin{equation}
	\begin{aligned}
		\sum_{j=0}^{n-1} \E \norm{\E(Y_n \mid Y_{n-j}) - \E(Y_n \mid Y_{n-j-1})}^2 &\leq  C \sum_{j=0}^n (h^{2p+1} + j^2 h^{4p+2}) \\
		&\leq C(t_n h^{2p} + t_n^3 h^{4p-1}).
	\end{aligned}
	\end{equation}
	Introducing the notation
	\begin{equation}
		P_{j,k} = \E\Big(\E(Y_n \mid Y_{n-j}) - \E(Y_n \mid Y_{n-j-1}), \E(Y_n \mid Y_{n-k}) - \E(Y_n \mid Y_{n-k-1})\Big),
	\end{equation}
	we have for $k < j$ thanks to \eqref{eq:Increments} and \cref{ass:RVs}
	\begin{equation}
	\begin{aligned}
		P_{j,k} &= \E\Big(\DD\Psi_h^j(\Psi_h(Y_{n-j-1}))\xi_{n-j-1}, \DD\Psi_h^k(\Psi_h(Y_{n-k-1}))\xi_{n-k-1} \Big) + \OO\big(jkh^{4p+2}\big) \\
		&= \OO\big(jkh^{4p+2}\big).
	\end{aligned}
	\end{equation}
	Therefore, we obtain for a constant $C > 0$
	\begin{equation}
		\sum_{j=0}^{n-1} \sum_{k<j} P_{j, k} \leq Ct_n^4 h^{4p-2},
	\end{equation}
	which concludes the proof.
\end{proof}



\begin{theorem} Assumption on local variance mimicking local error $\implies$ global error captured by variance.
\end{theorem}

\section{Calibration of the probabilistic integrator}
\begin{itemize}[label=-]
	\item Constant in the error term, need for calibration
	\item Explanation of the procedure in \cite{CGS16}
	\item Proposal of a new technique? Proving it works?
\end{itemize}

\section{Adaptive time stepping probabilistic} Is it doable?

\section{Inverse problems} Is it possible to have an estimation of variance under posterior measure $\implies$ well-calibrated UQ on the parameter.

\section{Numerical experiments}



\bibliographystyle{siam}
\bibliography{anmc}

\end{document}