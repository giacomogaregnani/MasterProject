\documentclass[10pt]{article}

\input{ex_shared}

\begin{document}
	\maketitle	

We thank the reviewers for their numerous comments and we are confident that clarifying certain aspects will help us to consistently ameliorate our work. We divided our answer in three sections. In Section 1, we discuss the modifications of Section 7 (Hamiltonian systems), since both referees had multiple comments about this part of our work. In Section 2 and Section 3 we answer separately to the comments of Reviewer \#1 and \#2, respectively, about the other topics treated by our work.

Some comments are not directly addressed in this report (e.g., language/syntax issues, typos, slight notation changes) but have nonetheless been considered in the rewriting of our paper.

All modifications have been typeset in \corr{red} in the new version of the paper for easing the review process.

\section{Modifications of Section 7}

Both Reviewer \#1 and Reviewer \#2 have multiple comments regarding symplecticity and the long-time conservation of Hamiltonians. Since we believe that this section may be the most relevant of our work, and that it is for sure the one presenting the most tedious calculations, we decided to let it undergo a thorough revision process and to dedicate it a separate section in this response. In the following, we highlight the major modifications.

\begin{enumerate}
	\item \textbf{Answer to the following comment by Reviewer \#1}
	\begin{itquote}
		The authors should demonstrate that the decomposition that occurs over (84), (85), and (86) is correct by showing the intermediate decompositions. For example, the decomposition could be formulated as a lemma using arbitrary quantities, e.g. $a_j$ instead of $\eta_j$ and $b_{j,k}$ instead of $(H^k_j-h^k)\Delta_{j,k}$, so that the reader can see and verify the correctness of the decomposition, without having to do so in terms of the visually more complicated $(H^k_j-h^k)\Delta_{j,k}$. The estimates in (87) and (88) should also be given in separate lemmas, each with their own proofs.
	\end{itquote}
	The proof of Theorem 6 has been shortened to make it easier for the reader to follow our reasoning and the calculations. This is achieved by the introduction of two lemmas (Lemma 7 and Lemma 8), where the most technical steps are performed. In particular, Lemma 7 deals with the decomposition of the sum, and Lemma 8 with the bound of the remainder terms. Since the proofs of these two results involve tedious calculations, we moved them to the Appendix, so that an interested reader can still verify their rightfulness. We thank Reviewer \#1 for having pointed out this lack of clarity, as we believe that now our manuscript is dealing with these technical steps in a much clearer manner.
	\item \textbf{Answer to the following comments by Reviewer \#2}
	\begin{itquote}
		After Eq. (63) {\normalfont [(62) in the new version]}: is $Q$ required to be a polynomial or $\nabla Q$?\\
		Try to given an outline of the argument where $\kappa$ comes from.\\
		Can you give a specific reference that explains the existence of $\tilde Q$ in (70)?
	\end{itquote}
	Concerning the first question, neither $Q$ nor $\nabla Q$ are required to be polynomials, but assumptions on the smoothness of $Q$ are needed for the analysis of long-time conservation to hold (Assumption 5). Methods for conserving \textit{exactly} polynomial first integrals can be designed, and we explain the properties of their RTS-RK counterpart in Section 6. In Section 7 our goal is to observe the \textit{good approximation} of Hamiltonian functions provided by symplectic integrators.\\
	Concerning the second and third questions, we believe that these points are clearly exposed in \cite[Chapter IX]{HLW06}. We added precise citations in the text in order to clarify this. 
	\item \textbf{Answer to the following comment by Reviewer \#1}
	\begin{itquote}
		$H_k$ is selected via a random mapping $\tau(y,h)=\tau(h)=h\Theta_k$, where $\Theta_k$ are opportunely scaled ...' -- Do the authors mean that $H_k=\tau(y,h)=h\Theta_k$? If so, they should explicitly state this. I did not find 'is selected via' to be sufficiently clear.
	\end{itquote}
	\textbf{and to the following comments by Reviewer \#2}
	\begin{itquote}
		Maybe hint at the argument why $\tau(y, h)$ exists and why $\Theta_k$ exists without violating Assumption 1 .\\
		The authors reference the paper by Skeel \& Gear. Given the main reference through the rest of the text is \cite{HLW06}, maybe the authors could use the notation thereof and point to specific sections of \cite{HLW06}.\\
		How can I see from (66) that (65) {\normalfont [(65) and (64) in the new version]} is satisfied?
	\end{itquote}
	The mapping $\tau(y, h)$ and $\Theta_k$ trivially exist by defining $\Theta_k \defeq H_k / h$, and $\tau(y, h) = h \Theta_k$. The fact we would like to highlight with Lemma 5 is that the numerical flow $\Psi_{\tau(y, h)}$ conserves the property of symplecticity when the choice of $h$ is independent of $y$. This is not the case when $h$ is chosen via standard adaptive techniques, which pick time steps as a function of error estimators, which are based on the solution. In our case, we have $\partial_y \tau(y, h) = 0$ and therefore (65) (with Definition 4) is trivially satisfied. Finally, we added a reference to \cite[Section VIII.1]{HLW06}, where these issues are treated thoroughly. We believe that with the modifications we made, the proof of Lemma 5 should be clear for a reader of Statistics and Computing.
	\item \textbf{Answer to the following comment by Reviewer \#2}
	\begin{itquote}
		Lemma 6: I found the arguments of the proof helpful for following the text. Consider putting it in the main body.
	\end{itquote}
	While we believe that building a geometry-aware probabilistic integrator may be the central result of our work, we would not want the reader to be distracted from the main messages by tedious computations. That is the reasoning behind our choice of placing the proof of Lemma 6 (and of the new Lemma 7 and Lemma 8) in the Appendix. Therefore, we decided to keep them in the Appendix in this revised version, and hope that this is satisfactory for both the referees. 
	\item \textbf{Answer to the following comment by Reviewer \#1}
	\begin{itquote}
		Do the authors intend for (74) {\normalfont [(75) in the new version]} to be the definition of the $\eta_j$'s? If so, then they should explicitly state this, reformulate (74) {\normalfont [(75) in the new version]}, and use the '$\defeq$' notation if possible to indicate that a definition is being made, instead of an assertion of equality between two objects. -- The authors should state the statistical properties of the $\eta_j$'s, e.g. whether or not they are independent with respect to certain random variables, whether or not they are identically distributed, etc. They should provide justifications for these assertions. Doing so provides a useful service to readers who wish to verify that the proof of Theorem 6 is correct. -- It is not clear why the $\eta_j$'s satisfy $\vert\eta_j\vert\leq CH_j e^{-\kappa/H_j}$ almost surely. Provide an explanation.
	\end{itquote}
	\textbf{and to the following comment by Reviewer \#2}
	\begin{itquote}
		How can I see that (74) {\normalfont [(75) in the new version]} can be written thusly?
	\end{itquote}
	Equation (75) appears now reversed in order to make it a proper definition of the variables $\eta_j$. Regarding the almost sure bound $\abs{\eta_j} \leq CH_je^{-\kappa/H_j}$, we found a mistake in equation (70), which in its previous version stated
	\begin{equation*}
		\abs{Q(\Psi_h(y)) - Q(y)} \leq Che^{-\kappa/h}.
	\end{equation*}
	The inequality above is indeed not correct, but it holds for the modified Hamiltonian as
	\begin{equation*}
	\abs{\tilde Q(\Psi_h(y)) - \tilde Q(y)} \leq Che^{-\kappa/h}.
	\end{equation*}
	For a reference of the latter, see \cite[Theorem IX.7.6]{HLW06} and the proof of \cite[Theorem IX.8.1]{HLW06}. In particular, this implies by definition of the modified Hamiltonian that for any $y \in \R^{2d}$ we have
	\begin{equation*}
		\Big\lvert Q(y) - Q(\Psi_h(y)) + \sum_{k=q}^{N-1} h^k \big(Q_{k+1}(y) - Q_{k+1}(\Psi_h(y))\big)\Big\rvert \leq Che^{-\kappa/h}.
	\end{equation*}
	Replacing $h$ with $H_j$, one gets the almost sure bound $\abs{\eta_j} \leq CH_je^{-\kappa/H_j}$. Indeed, the difference between the modified Hamiltonian $\tilde Q$ and the local modified Hamiltonians $\hat Q_j$ is their global vs. local nature. Hence, on a local level the two quantities behave in the same manner. In order to clarify this point, we added in the text a reference to (70) when stating the bound on the (random) local truncation error $\eta_j$.
	
	Regarding the statistical properties of the $\eta_j$'s, not much can be stated. In fact, we have that $\eta_j$ and $\eta_i$ for $i\neq j$, are not independent random variables. In fact, when following a trajectory, the variable $\eta_j$ depends on $Y_j$ (respectively $\eta_i$ and $Y_i$), and since $Y_i$ and $Y_j$ are not independent, the variables $\eta_j$ and $\eta_i$ are not independent. Nonetheless, the almost sure bounds $\abs{\eta_j} \leq CH_je^{-\kappa/H_j}$ form a sequence of independent random variables, as the constant $C$ is independent of $y$ (see \cite[Theorem IX.7.6]{HLW06}) and the random time steps are independent under Assumption 1. This means that we can compute, for example,
	\begin{equation}
		\E \eta_i \eta_j \leq C \E H_j e^{-\kappa/H_j} H_i e^{-\kappa/H_i} = C \E H_j e^{-\kappa/H_j} \E H_i e^{-\kappa/H_i},
	\end{equation}
	and then use the result provided by Lemma 6 to proceed with calculations. We employ this property in the proof of Lemma 8. 
	\item \textbf{Answer to the following comments by Reviewer \#1}
	\begin{itquote}
		``Moreover, for any $r,s>1$ such that $r+s<R$'' $\to$ ``Then for any $r,s>1$ such that $r+s<R$.'' This hypothesis should be stated explicitly. In addition, since the authors have not explicitly proof that value of $C$ may change from line to line, inequality is strictly speaking false. problem can resolved by adding a factor 2 or stating at beginning proof.
	\end{itquote}
	We updated our text to clarify this point, with a particular care on the value of the constants, which, in fact, were not correct in the previous version. 
	\item \textbf{Answer to the following comments by Reviewer \#1}
	\begin{itquote}
		``\ldots if the numerical solution $y_n$ \ldots is close enough to the initial condition'' [\ldots] The constraint that the numerical solution ``is close enough to the initial condition'' is vague. Theorem IX.8.1. of reference \cite{HLW06} is not very helpful: it just specifies some compact subset K of the domain of analyticity without even saying what K is. The authors should provide a more informative and specific constraint on the numerical solution.
	\end{itquote}
	Rigorous arguments on the domain of validity of results such as Theorem 6 or \cite[Theorem IX.8.1]{HLW06} ought to be found, for example, in \cite{BeG94}. We added this reference, but we believe that entering discussions in this direction would be out of the scope of our work.
	\item \textbf{Answer to the following comments by Reviewer \#1}
	\begin{itquote}
	Second (90) {\normalfont ((92) in the new version)}: for true, we need $t_n\geq 1$
	\end{itquote}
	We added Remark 14 to take care of this issue.
	\item \textbf{Answer to the following comments by Reviewer \#1}
	\begin{itquote}
		First (93): two terms inside parentheses been justified before their appearance here. are requested state justify bounds used. \\ Last help reader verify correct, write they use $t_n=nh$. an explicit bound $\sqrt{r}$ helps understand why $\mathcal O(e^{-\kappa (4mh)})$ \\ Why are the first two terms on the right-hand side of the first inequality of (94) bounded by $C_4 h^q$?
	\end{itquote}
	\textbf{and to the following comment by Reviewer \#2}
	\begin{itquote}
		I have not fully understood how this imposing of this terms should work in Eqs. (91) and (95).
	\end{itquote}
	We believe that the new version of the proof of Theorem 6, together with the proofs of Lemma 7 and Lemma 8, answer all the above questions.
	\item \textbf{Answer to the following comments by Reviewer \#1}
	\begin{itquote}
		Remark 9 {\normalfont [11 in the new version]}: -- From the point of view of probability theory, I do not think it is appropriate to refer to $p\to\infty$ as a 'deterministic limit'. It may be better to just write 'in the limit as $p\to\infty$'. -- Explain in more detail why the coefficient $M$ in Assumption 6 tends to 1 as $p\to\infty$.
	\end{itquote}
	We rephrased the Remark in light of this comment. In particular, the value $M$ does not \textit{tend} to 1, but it can be chosen arbitrarily close to 1 if $p \to \infty$.
	\item \textbf{Answer to the following comments by Reviewer \#1}
	\begin{itquote}
		Remark 10 {\normalfont [12 in the new version]}: To a reader unfamiliar with symplectic integrators, it is not immediately clear what is the significance of this remark with respect to the preceding results in this paper. If a connection exists, then the authors should state and explain it clearly; otherwise the remark does not serve any meaningful purpose and ought to be removed.
	\end{itquote}
	\textbf{and to the following comment by Reviewer \#2}
	\begin{itquote}
		I have not been able to build up an intuition about Remark 10 {\normalfont (12 in the new version)} in light of Theorem 6. How do these two statements relate to each other? What exactly am I to make of this as an user of your method?
	\end{itquote}
	We believe that this issue is clarified with the addition to Remark 12 of the sentence 
	\begin{quote}
		Conversely, Theorem 6 proves that random step sizes do not spoil, under the assumptions specified above, the good long time properties of symplectic integrators with fixed step size.
	\end{quote}
	\item \textbf{Answer to the following comment by Reviewer \#1}
	\begin{itquote}
		Remark 11 {\normalfont [13 in the new version]}: 'We introduce ... the remainder $\widehat{S}_1$.' -- In the proof, the assumption is also used to simplify the terms in $\widehat{S}_2$.
	\end{itquote}
	For clarity, since $p \geq 3/2$ is now one assumption behind Lemma 8 and since we now avoided the splitting of $S$ into $\hat S_1$ and $\hat S_2$, we rephrased this sentence as
	\begin{quote}
		 As it can be noticed in the proof of Lemma 8, we introduce the assumption $p \geq 3/2$ in order to simplify the terms composing the remainder $S(\Delta, \eta)$.
	\end{quote}
\end{enumerate}

\section{Reviewer \#1} 

As a service to Reviewer \#1 we specified in brackets [...] the places to which his/her comments refer to when it is not clear from the comment itself. Let us remind Reviewer \#1 that his comments regarding Section 7 have been addressed in Section 1 of this report. The points that are not directly answered here have been nonetheless taken into account for the review of our paper. 

\begin{enumerate}
	\item 
	\begin{itquote} 
		{\normalfont [Page 3]} ``An additive random term could force the solution on the negative plane with a non-zero probability, which can become significantly big in case the magnitude of one component is small.'' -- This sentence is unclear, because it is not clear what can become significantly big, and it is not precisely clear what the component is of.
	\end{itquote}
	We modified the sentence above as 
	\begin{quote} In particular, an additive random term could force the solution on the negative plane with a non-zero probability, and this probability could become non-negligibly big in case the magnitude of one component of the solution is small \end{quote}
	\item 
	\begin{itquote} 
		Definition 1: ``... for any function $\Phi$ ... with all derivatives bounded uniformly on $\mathbb{R}^d$.'' -- From the point of view of probability theory, the class of ``test functions'' that one considers is the class of bounded, continuous functions. To avoid any confusion, the authors should state very clearly whether or not the test functions that they consider are bounded. I consider ``all derivatives'' to be imprecise, as certain readers may take this to mean that the test functions themselves are bounded, when in fact the authors consider unbounded test functions later, e.g. $\Phi(x):=x^\top x$. -- For brevity and simplicity, perhaps the authors could define the appropriate function class in an equation and use a symbol to refer to the function class afterwards, instead of repeatedly writing ``... with all derivatives bounded uniformly on $\mathbb{R}^d$''.
	\end{itquote}
	We introduced the symbol $\mathcal C^\infty_b(\R^d, \R)$ for the functions in $\mathcal C^\infty(\R^d, \R)$ with all derivatives bounded uniformly in $\R^d$. The function $\Phi$ that we consider in the numerical experiment is indeed unbounded on $\R^2$ ($d = 2$ being the dimension of the FitzHugh--Nagumo system). Nonetheless, the FitzHugh--Nagumo system is known to present periodic solutions lying in a bounded attractor $\mathcal A$, and if the numerical solver is stable the numerical solution will stay in a neighbourhood of this attractor, too. Therefore, the restriction of the function $\Phi\colon \mathcal A \to \R$, $x \mapsto x^\top x$ on the attractor is a valid test function.
	\item 
	\begin{itquote} 
		{\normalfont [Regarding the beginning of Section 3]}``and denote in the following ... the Markov generator on the step size $h$.'' I am aware that Conrad et al. use $\mathcal{L}^h$ for the infinitesimal generator of the Markov chain. However, I think this notation can be improved for the present article, for the following reasons: 1) $\mathcal{L}$ has already been used for the Lie derivative of the flow, and $\mathcal{L}$ and $\mathcal{L}^h$ are not related by taking ``powers''; 2) I think the infinitesimal generator should depend on the distribution $H_0$ from which the random time steps are drawn, and this dependence is just as important as the dependence on the deterministic step size $h$. If the authors wish to emphasise the dependence of the generator on $h$, then I think they should also emphasise its $H_0$-dependence as well, at least for the first instance of the infinitesimal generator. If the authors wish to do so, they can can omit the $H_0$ dependence in subsequent instances to simplify notation, but I think it is advantageous to the authors to remind the reader that the present setting is different from that of Conrad et al.
	\end{itquote}
	In order to answer to the first part of this comment, we remark that the semi-group notation for the infinitesimal generator $e^{h\mathcal L}$ involves the Lie derivative of the flow $\mathcal L$, which in fact resolves this notation clash. We added references to \cite[Section 4.3]{PaS08}, \cite[Section III.5.1]{HLW06} and \cite[Section 2.3]{Pav14}, where this topic is treated extensively. For the problem of ``taking powers'', we agree that the notation $\mathcal L^h$ could be misleading, and we therefore changed it to $\mathcal L_h$, as for the infinitesimal generator of the Markov chain $\mathcal P_h$. Conversely to the operator $\diffL = f \cdot \nabla$, the expression of $\mathcal L_h$ cannot be written explicitly, and the exponential notation $\mathcal P_h = e^{h\mathcal L_h}$ is employed for analogy with the exact solution. \\
	We agree that the infinitesimal generator of the Markov chain depends on the distribution on the step sizes, as the infinitesimal generator of the probabilistic method defined in \cite{CGS17} depends on the distribution of the additive random variables $\xi_k$. Nonetheless, the index $h$ of $\mathcal P_h$ and $\mathcal L_h$ is chosen because these operators map an approximation of the solution at time $t$ to an approximation of the solution at time $t+h$. In fact, even though our method proceeds by random steps, the value $Y_k$ is still conceived as an approximation of $y(kh)$ (see in Section 2, just after (6), ``where $Y_k$ is still a random variable approximating $y(t_k)$ \ldots'' and/or Remark 6). Therefore we believe that writing explicitly the dependence on $H_0$ is not only unnecessary, but even not entirely correct.
	\item 
	\begin{itquote}	
		{\normalfont [Beginning of Section 3]} ``hence given $h>0$ there exists an operator $\mathcal{P}_h$ such that '' -- As a service to the reader, provide a reference to a theorem or proposition that justifies the use of ``hence''. I do not expect all readers of this article to be familiar with homogeneous Markov chains.
	\end{itquote}
	and
	\begin{itquote}
		{\normalfont [Beginning of Section 3]} Also, as a service to the reader, provide a reference to a theorem or proposition that justifies (15). I do not expect all readers of this article to be familiar with homogeneous Markov chains.
	\end{itquote}
	We added a reference to \cite[Section 2.3]{Pav14}, where the theory of infinitesimal generator for homogeneous Markov chains is treated extensively.
	\item 
	\begin{itquote}
		 In Remark 3, regarding the phrase 'Given the assumptions on $f$ and $\Phi$ above, …' the authors should state explicitly all the assumptions on $f$ and $\Phi$ (and $H_0$) that are necessary for (24) to hold. Note that in Assumption 3 of the paper of Conrad et al. (reference [6] in the present paper), both the analogues of (23) and (24) are stated in the assumption. In contrast, the formulation of Remark 3 of the present paper suggests that (24) follows from the assumptions for (23), and in particular with the same constant $L$. If this is what the authors intended, then a proof should be given. Alternatively, since neither Remark 3 nor equation (24) are used in the rest of the paper, the authors may choose to remove Remark 3.
	\end{itquote}
	We took the advice of Referee \#1 and deleted this remark, as, indeed, there was no further reference nor application to this property in the rest of our work. Nonetheless, we think that this assertion could have been proved by means of Lemma 4. 
	\item 
	\begin{itquote}{\normalfont [Proof of Theorem 1]} ``We then apply Lemma 1 to the first term and Assumption 4 to the second'' -- For an application of Lemma 1 to the first term to yield (28), note that the conclusion of Lemma 1 must hold with some C that is independent of the initial condition (this is not stated in the formulation of Lemma 1). Therefore, the formulation of Lemma 1 should be updated accordingly. -- Assumption 4 cannot be applied to the second term under the present level of generality. This is because the authors have not assumed that the vector field $f$ is infinitely differentiable. By the chain rule, $w_k$ is not infinitely differentiable. Therefore Assumption 4 cannot be applied to $w_k$. The authors should show that for all $k$, both $w_k$ and $W_k$ belong to the right class of functions (infinitely smooth, with all derivatives uniformly bounded in $\mathbb{R}^d$, etc.)
	\end{itquote}
	We corrected this statement. In particular, we noticed that in order to have a method of order $q$, the vector field must be continuously differentiable at least $q$ times. This gives both numerical and exact flows to be of the same class of functions, for the numerical flow it can be seen via Taylor expansions (see the theory of tree differentials developed in \cite[Chapter II]{HNW93}). Therefore, by the chain rule, if we take $\Phi$ to be a smooth function both $w_k$ and $W_k$ are continuously differentiable up to order $q$, too. Hence, we widened Assumption 4 to include functions in the right class, and we added a remark to explain our reasoning, with a precise reference to \cite[Theorem II.3.1]{HNW93} to justify it. Moreover, in light of these considerations, we slightly changed the statement of Definition 1, Lemma 1 and Theorem 1. In particular, for a matter of readability, we wrote ``sufficiently smooth function $\Phi\colon\R^d\to\R$'' in the definition. Then, in the following we just need $\Phi \in \mathcal C^l_b(\R^d, \R)$ where $l = \min\{q, 3\}$, so that both the condition of Assumption 4 and the Taylor development of the Proof of Lemma 1 hold.
	\item 
	\begin{itquote} 
		{\normalfont  [Proof of Theorem 1]} The authors should provide detailed, step-by-step, rigorously justified steps that proceed from (28) to the first inequality in (29). Do not use vague phrases such as 'Proceeding iteratively'. Note that 'proceeding iteratively' does not yield the first inequality in (29), even after using $w_0=W_0$. Instead, it yields $$\sup_{u\in\mathbb{R}^d}\vert W_k(u)-w_k(u)\vert \leq Ch^{min\{2p+1,q+1\}}\sum_{j=0}^{k-1}(1+Lh)^j.$$ In fact, using Gronwall's inequality for nonnegative sequences yields $$\sup_{u\in\mathbb{R}^d}\vert W_k(u)-w_k(u)\vert \leq Ch^{min\{2p+1,q+1\}}\exp\left(k(1+Lh)\right).$$ Since the upper bound of $k$ is $N=T/h$, the exponential will increase to infinity faster than any power of $h$.
	\end{itquote}
	We thank the referee for pointing this imprecision out. In particular, we noticed that from equation (29) we can get the result through an application of Lemma 2, which appeared in the previous version as Lemma 4 and was placed in Section 4. Therefore, we moved this Lemma in Section 3 and employed it to conclude the proof of Theorem 1.
	\item 
	\begin{itquote} 
		In Assumption 4, if $\mathcal{L}^h$ depends on the distribution $H_0$ (which I think it does), then this assumption should also state something about the distribution $H_0$, not just the vector field $f$. At the very least, I would expect ``The function $f$ is such that ...'' to be changed to ``The function f and the distribution $H_0$ are such that ...''. Also, although it may be obvious to the authors, I think it should be stated explicitly as a service to the reader that the positive constant $L$ may depend on $f$ and $H_0$, but not on $\Phi$ or $h$.
	\end{itquote}
	We modified Assumption 4 in order to include this comment.
	\item 
	\begin{itquote}{\normalfont [Before Remark 10]}
	``In the sub-optimal case ... are balanced.'' -- The authors could provide an explicit formula for $M$ in terms of $h$, $p$ and $q$ in the case that $p$ ``if for any''
	\end{itquote}
	A new formula has been added for treating this sub-optimal case.
	\item 
	\begin{itquote}
		Equation (99) {\normalfont [Beginning of Section 8]}: Random variables should be written using upper-case letters, so equation (99) should be $Z=\mathcal{G}(\theta)+\varepsilon$.
	\end{itquote}
	We have already employed the upper-case $Z$ in the section concerning Monte Carlo estimators. Moreover, we believe that employing lower-case letters for observations is widespread in literature. See, for example, equation (2.5) of Stuart's Acta Numerica \cite{Stu10}, which reads $y = \mathcal G(u) + \eta$, and later appearances of the same equation in the same work.
	\item 
	\begin{itquote}
		{\normalfont [Equation (103)]} The expression of the Hellinger distance is incorrect, because the integral on the right-hand side is not taken with respect to the prior. See Definition 6.35 in Stuart's Acta Numerica paper.
	\end{itquote}
	We adopted this expression for the Hellinger distance since we assumed that prior and posterior distributions admit a probability density function with respect to the Lebesgue measure. In this case, the Hellinger distance can be simplified and its expression is the one we provide. Since the section regarding Bayesian inverse problems delivers qualitative results, later confirmed by simple experiments, we decided to consider the case of finite-dimensional parameters. In this case, we feel that assuming that prior and posterior admit a probability density function is not limiting our analysis. In any case, we specified this assumption in the text.
	\item 
	\begin{itquote}
		{\normalfont [Just before the beginning of Section 8.1]} ``the standard random walk Metropolis--Hastings is usually employed'' -- Provide evidence for this by citing some literature.
	\end{itquote}
	We corrected the sentence above with ``the standard random walk Metropolis--Hastings can be employed''. We believe that the standard implementation of Metropolis--Hastings is nowadays a common sampling tool, and that a list of references of papers employing this algorithm would add no value to our work.  
	\item 
	\begin{itquote}
		{\normalfont [Section 8.1]} To make it explicitly clear how this section is connected to the preceding material (in particular, the introduction to Bayesian inverse problems), I recommend that the authors do the following: 1) Write '$Z=\varphi_h(y_0^\ast)+\epsilon$' instead of '$d= \varphi_h(y_0^\ast)+\epsilon$'. Upper case letters should be used for random variables and lower case letters should be used for deterministic values. 2) Specify the parameter space $\Theta$, observable, and linear forward operator. 3) As far as possible, use the same symbols as used in (99), (100), (101).
	\end{itquote}
	We modified the notation of this example by replacing $y_0 \leftarrow \theta$, $d \leftarrow Z$, $\bar y_0 \leftarrow \bar \theta$, $y_0^* \leftarrow \theta^*$ in order to get more uniformity with respect to the introduction to Bayesian inverse problems. Moreover, we added the sentence ``In this case, the parameter space is $\Theta = \R$ and the forward operator $\mathcal G$ is defined by $\mathcal G\colon \R \to \R$, $\mathcal G \colon \theta \mapsto \theta e^{-h}$''. 
	\item
	\begin{itquote}
		Section 8.1: The symbol '$\to$' and the phrase 'tends to' are used to suggest convergence. However, the authors do not specify the type of convergence. Thus, the statements should be considered only at an imprecise, heuristic level, and not at the level of rigorous mathematics. The authors should notify the reader of this.
	\end{itquote}
	We added the sentence ``In the following, we verify heuristically the convergence of the posterior distributions obtained with deterministic and probabilistic integrators with respect to a vanishing noise scale.''
	\item 
	\begin{itquote}
		{\normalfont [Figure 3]} It is difficult to distinguish the curves corresponding to different values of $\sigma$. I recommend that the authors reduce the range of values of $y_0$ in each subplot to make it easier for the reader to distinguish the curves for different values of $\sigma$
	\end{itquote}
	We increased the sizes of the plots and ``stretched'' the horizontal axis to make the lines more distinguishable. Nonetheless, we think that keeping the same horizontal range for the four figures renders the idea of how more concentrated deterministic posteriors are. Since the additive noise posterior densities have non-negligible values over the range $\theta \in [-1, 3]$, we believe that decreasing the range more would reduce the visual and conceptual impact of this figure.
	\item \label{it:Ref1Plots}
	\begin{itquote}
		Table 2: the sub-table for the explicit trapezoidal method should contain as many columns as the number of values of $h$ that were used.
	\end{itquote}
	We changed the way numerical results are presented from tables to plots (see Figures 4 and 5)
	\item 
	\begin{itquote}
		Section 9: The authors should specify all the parameters that they use in their experiments. For example, in some subsections below the value of the parameter $p$ is given, while in other subsections it is not. 
	\end{itquote}
	We reviewed the description of the numerical experiments and added the missing parameters.
	\item
	\begin{itquote}
		{\normalfont [Section 9.7]} Some readers may assess that the variance of the Gaussian random variable is excessively small (on the order of $10^{-8}$) relative to the initial condition. The authors should explain why they chose such a small variance.
	\end{itquote}
	and
	\begin{itquote}
		``Hence, initial conditions with a different energy level with respect to the observation are endowed with a high value of likelihood'' -- The authors should explicitly state the likelihood that they use and clearly explain how the energy level and the likelihood are connected.
	\end{itquote}
	We modified the introduction of the numerical experiment in order to clarify our choice of likelihood and the small noise as
	\begin{quote}
		Noise is then set to be a Gaussian random variable $\epl \sim \mathcal{N}(0, \sigma_\epl^2 I)$, where $\sigma_\epl = 5 \cdot 10^{-4}$, and we fix a standard Gaussian prior on the initial condition, i.e., $\pi_0 = \mathcal N(0, I)$, so that the likelihood is given by (101). We choose the observational noise to have a small variance (i.e., of order $\mathcal O(10^{-8})$) as in this case classical solvers present the misleading overconfident behaviour explained in Section 8.
	\end{quote}
	The observation about energy levels and likelihood is modified as
	\begin{quote}
		Hence, initial conditions with a different energy level with respect to the observation are mapped by the approximate forward model to points which are close to the observations, and as a result the posterior distribution is concentrated far from the true value.
	\end{quote} 
	The meaning of the above sentence is that if the approximate forward model is not conserving the Hamiltonian, the energy state of the approximate solution at time $t$ will be different than the one of the initial condition. Let us assume without loss of generality that the numerical integrator causes a positive drift in energy. In this case, an initial condition with a lower energy than the observations will be ``seen'' under the posterior as the most likely. Conversely, if the forward map conserves (i.e., approximately conserves) the Hamiltonian, \textit{at least} the correctness on the energy for the solution of the inverse problem with respect to the observations can be trusted.
	\item 
	\begin{itquote}
		{\normalfont [Proof of Lemma 6 -- in the appendix]} The authors need to bound the $Mh$ term by a constant that does not depend on $h$.
	\end{itquote}
	We added to the statement of the Theorem ``and if $h \leq \bar h < \infty$'' and bounded $Mh \leq M\bar h$. In the following results, we most often assume $\bar h = 1$.
	\item 
	\begin{itquote}
		 {\normalfont [Proof of Lemma 6 -- in the appendix]} The authors only consider the case where $H_j\geq h$. They should discuss the case where $H_j$ ``in the mean-square sense''
	\end{itquote}
	The proof is actually valid for $H_j < h$ as well. We nonetheless modified it for clarity by introducing the notation
	\begin{quote}
		In the following, we denote by $\llbracket a, b \rrbracket$ the interval $\llbracket a, b \rrbracket = [a, b]$ if $a < b$ and $\llbracket a, b \rrbracket = [b, a]$ if $a \geq b$.
	\end{quote}
	The rest of the proof is unchanged, and we believe correct, by modifying the intervals $[h, H_j]$ to $\llbracket h, H_j \rrbracket$.
\end{enumerate}

\section{Reviewer \#2}

Let us remind Reviewer \#2 that his comments regarding Section 7 have been addressed in Section 1 of this report. The points that are not directly answered here have been nonetheless taken into account for the review of our paper.
 
\begin{enumerate}
	\item 
	\begin{itquote}
		\textbf{Convergence of Monte Carlo for $M = 1$.} This result is remarkable and warrants some further discussion. On one hand, the whole point of a probabilistic method is to obtain a posterior distribution to quantify the numerical error. One Monte Carlo sample, however, only defines a posterior Dirac distribution, which could be understood	to be nothing else than a deterministic point estimation. So, the question than turns out to be how many samples are required to obtain a characterization of	the posterior in the non-limiting case for $h > 0$ fixed. On the other hand: maybe an user is really not interested in a posterior distribution per se, but is required to ``hedge against fixed step size risk''. Maybe the authors could come up with a scenario, where a certain fixed step size would yield in catastrophic error, but a randomized step size with same expected step size does not? Maybe the authors could try to run a sensitivity analysis on a hyper-grid (for a short time-span example) and report errors for best, worst and average choices for $H_k$. Or maybe the authors could run an experiment and report average errors over a grid of decreasing $h$ and increasing $M$. I would also be open for the authors own preferred choice for dissecting this result in a bit more detail, but I think some discussion is required.
	\end{itquote}
	In our view, the result presented in Theorem 3 guarantees 
	\begin{enumerate}[label=\alph*.]
		\item consistency of Monte Carlo estimators, which converge to the correct quantity for $h \to 0$ regardless of the number of samples,
		\item a good quality of Monte Carlo estimators for a relatively small number $M$ and $h > 0$, which is particularly relevant when doing Bayesian inference with the pseudo-marginal Metropolis--Hastings \cite{AnR09}, where the quality of the produced Markov chain is driven by the variance of the estimator employed in the acceptance ratio.
	\end{enumerate}
	In fact, we do not claim that the empirical distribution of the random numerical solution is well described regardless of $h$ and $M$, but just that the quality of Monte Carlo estimators depends on $h$ only. In order to clarify this points, we added a remark to Section 5 (Remark 10 in the new version).
	\item
	\begin{itquote} 
		\textbf{Experimental section.} Sections 9.1--9.3 seem to vary only in details. I suggest to rework this as a running example for the end of each according Section 3--5. Could the authors also please report standard deviations and explain the rationale for choosing different number of Monte Carlo samples and step sizes. In particular for the mean square convergence, the chosen step sizes seem very small. Could the authors give some analysis whether on this scale the variation of the step sizes or the variation due to randomness has the bigger effect. I suggest that for each	step size, the authors also run two deterministic solves with the lower and the	upper bound of the uniform distribution (but without adjusting the required steps).
	\end{itquote}
	Concerning the subdivision of the experiments in Section 9.1 -- 9.3, we believe that it is relevant to devote one subsection per experiment, as three important properties and theoretical results are verified numerically separately. Moreover, we would prefer to stick to the style of placing all numerical experiments at the end of our paper, as it is customary in the numerical analysis literature. Regarding the choice of the time step $h$ for these simulations, we agree that varying them between these sections can create confusion. Therefore, we repeated the experiments with uniformed values among all three experiments. In particular, we agree that $h = 0.01$ as a maximum step size in the mean-square convergence is very small, so we kept as a maximum value $h = 0.125$ and subsequently divided this value by 2. Concerning the number of samples $M$, we willingly choose it to be a large value so that any Monte Carlo error, which could make the convergence slopes ``noisier'', is killed. Finally, we decided to modify the way we present our results and made convergence plots instead of figures, in view of a comment by Referee \#1 (see point \ref{it:Ref1Plots} in Section 2).
	\item
	\begin{itquote}
		\textbf{Experimental section.} The experiment in Section 9.4 is interesting, but needs some more details. Were the same solvers used for RTS-RK and AN of Conrad et al. \cite{CGS17}? There is a remaining degree-of-freedom in the AN method, the scaling of the posterior variance. The question is how much posterior variance is left once the variance $\xi_k$ is reduced to the point where the solution does not diverge. This scaling could be found efficiently with interval bisection to give a distribution where 95\% of samples do not diverge before t = 100. How do distributions of RTS and AN compare at different points in time?
	\end{itquote}
	We used the same solver for both methods, specifically the Runge--Kutta--Chebyshev method, an explicit stabilized integrator. Regarding the variance scale for the additive noise method, let us remark that posterior variance could be modified in the RTS-RK method too by tuning, for example, the value $C$ in $H_k \iid \mathcal U(h - Ch^{p+1/2}, h + Ch^{p+1/2})$. In particular, the RTS-RK method is more robust than the additive noise method with respect to stiffness regardless of this scaling. In particular, in this numerical example we took $C = 1$ for both the methods (for the AN method, $Q = I$ using the notation of \cite{CGS17}). Let us remark that if we were given another stiff system, it would be necessary for the AN method to retune the method finding a ``good'' scaling of the variance, while for the RTS-RK method this is not required. Therefore, we believe that our numerical comparison is correct and we would not further modify it.
	\item 
	\begin{itquote}
		\textbf{Experimental section.} Finally, the experiment in Section 9.7 should also compare against \cite{CGS17}. Also, is the prior $\pi_0 = \mathcal N(0, I)$ a realistic choice in this setting? From $Q(\phi_{t_\mathrm{obs}}(y_0) + \epl)$, it should be possible to concentrate the prior much more strongly on values with a similar Hamiltonian, no?
	\end{itquote}
	We tested the method presented in \cite{CGS17} in this contest, and verified that the results are indeed qualitatively worse than the ones given by our probabilistic integrator. Nonetheless, after the comparisons of Section 9.4 and 9.5, we find it slightly redundant to compare the RTS-RK method and the AN method in this framework, too. In particular, the inadequacy of the AN method for problems endowed with geometric properties has already been highlighted in Section 9.5, and we believe this suffices to conclude the comparison. Let us finally remark that if we considered a non-geometric equation for our test, the AN-RK and the RTS-RK would have given very similar posteriors. An example is the FitzHugh--Nagumo model, which is indeed the inference test that is chosen in \cite{CGS17}.
	\item 
	\begin{itquote}
		\textbf{Motivating example: a posteriori error estimator.} Figure 2 is currently a source of some confusion. Embedded methods (as described in \cite{HNW93, HaW96}) typically estimate and control	the local error $|\Psi_{h}(y)−\phi_{h}(y)|$. This quantity seems to be compared to the
		global error $|\Psi_{hn}(y)−\phi_{hn}(y)|$. Is this indeed what is depicted in Figure 2? Secondly, the global errors level off on a quite high value. Could it be that this is simply the average distance between points on the strange attractor? Also, it seems a bit loaded to speak of a true error for a chaotic system. I am of the opinion that the motivation given without this section would suffice. If the authors want to keep this section, the above points need to be	addressed.
	\end{itquote}
	We agree with the referee that the motivation involving error estimators may be a source of confusion for the reader. We first included this example because we feel that probabilistic integrators as ours or the one presented in \cite{CGS17} could be potentially employed to build reliable variance-based error estimators of the \textit{global} error. It is indeed unclear how to build such estimators, even in the deterministic setting, and the procedure of embedded methods is undoubtedly the most used technique for error estimation in ODEs (e.g., \texttt{Matlab} routine \texttt{ode45}). Hence, we took embedded estimators as a comparison. Since at this time we are unable to provide a rigorous argument supporting the fact that probabilistic integrators indeed provide correct error estimators, we chose to follow the referee's advice, and to erase this section from our work.
	\item 
	\begin{itquote} 
		\textbf{Minor points and questions:} Page 4 line 20: I am not a chemistry expert. Wouldn't one speak of concentration/masses?
	\end{itquote}
	The expression ``population sizes'' is used in literature for chemical reactions, see e.g. \cite{HAL12}.
	\item 
	\begin{itquote} 
		\textbf{Minor points and questions:} Remark 1: it should probably be highlighted at this point that many symplectic integrators are implicit which needs to be considered when considering the runtime cost of numerical integration.
	\end{itquote}
	As stated in Remark 1, the computational requirements of the additive noise and the RTS-RK methods are indeed the same for explicit methods. For implicit methods, the nonlinear system arising from the iterations is indeed different, and therefore Newton's method will, a priori, require more (or less) iterations. We modified Remark 1 to take into account this interesting comment, as indeed symplectic methods are, apart of special cases of separable energies, implicit.
	\item 
	\begin{itquote}
		\textbf{Minor points and questions:} Is assumption 2.(ii) ever violated?
	\end{itquote}
	To our knowledge, it is not always guaranteed that 2.(ii) is true. Considering the simple case of the explicit Euler method, we have $\Psi_t(y) = y + tf(y)$, which means that $\partial_t \Psi_t(y) = f(y)$ and $\partial_{tt} \Psi_t(y) = 0$. Hence, for the explicit Euler method, we have that the map $t \mapsto \Psi_t(y)$ is $\mathcal C^{\infty}$ for all $y \in \R^d$. For a generic Runge--Kutta method on $s$ stages, we have that the flow $\Psi_t$ is defined by
	\begin{equation*}
	\begin{aligned}
		K_i(t) &= f\big(y + t\, \sksum_{j=0}^s a_{ij}K_j(t)\big), \quad i = 1, \ldots, s, \\
		\Psi_t(y) &= y + t \,\sksum_{i=0}^s b_i K_i(t).
	\end{aligned}
	\end{equation*}
	Therefore, the dependence of $\Psi_t(y)$ on $t$ is non-trivial and the regularity of the map $t \mapsto \Psi_t(y)$ is not given directly as in the explicit Euler case.
	\item 
	\begin{itquote}
		\textbf{Minor points and questions:} Why is $T = Nh$ specially mentioned in Definition 1? Also see Theorem 1 and similar.
	\end{itquote}
	It is quite standard to fix a final time $T$ and a time step $h$, thus getting $T = Nh$ for a positive integer $N$. 
	\item 
	\begin{itquote}
		\textbf{Minor points and questions:} The transformation from (17) to (18) is a bit much for one equation. I am not sure whether I understood correctly. Please expand.
	\end{itquote}
	Taking the conditional expectation with respect to $Y_0 = y$ as stated in the text, one gets that each term appearing in (17) corresponds to a term in (18) where $\E(H_0 - h \mid Y_0 = y) = 0$ and $\E((H_0 - h)^2 \mid Y_0 = y)$ can be estimated by means of Assumption 1.
	\item
	\begin{itquote}
		\textbf{Minor points and questions:} Where did $M^{-1}$ go from (54) to (55)? Cf. (51)?
	\end{itquote}
	The Monte Carlo estimator is unbiased, therefore $\E(\hat Z_{N,M} - Z) = \E(Z_N - Z)$ and then Theorem 1 can be applied consequently. We nonetheless explicitly highlighted this fact this in the revised version.
	\item 
	\begin{itquote}
		\textbf{Minor points and questions:} Remark 4 {\normalfont [Remark 5]}: please add an appendix sketching the arguments for such a construction.
	\end{itquote}
	We added an appendix where we show the existence of a modified SDE which resembles in its construction the argument of \cite[Section 2.4]{CGS17}. Since this construction is based on the theory of backward error analysis, which we present already in Section 7.2 of our work in a schematic manner (but rather exhaustive for our needs), we believe the addition of this appendix adds indeed value to our work, and we therefore thank the Referee for his comment.
	\item 
	\begin{itquote}
		\textbf{Minor points and questions:} System (122) might be easier to parse written as a second-order system.
	\end{itquote}
	We agree with the reviewer on the fact that the Kepler system is oftentimes found in literature as a second-order system. Nonetheless, since our work treats first-order equations, we believe that if we first presented the Kepler system as a second-order system, we would have to write its first-order transformation too, which would make the numerical experiment heavier to digest for the reader.  
	\item 
	\begin{itquote}
		\textbf{Minor points and questions:} Adding a concluding section might make it easier for the reader to remember the main take-away points of the paper.
	\end{itquote}
	We added a short concluding section summarising the main points of our work, with a particular emphasis on the geometry-awareness which was previously missing in the probabilistic numerics literature.
\end{enumerate}

%\bibliographystyle{siam}
%\bibliography{anmc}
\def\cprime{$'$}
\begin{thebibliography}{10}
	
	\bibitem{AnR09}
	{\sc C.~Andrieu and G.~O. Roberts}, {\em The pseudo-marginal approach for
		efficient {M}onte {C}arlo computations}, Ann. Statist., 37 (2009),
	pp.~697--725.
	
	\bibitem{BeG94}
	{\sc G.~Benettin and A.~Giorgilli}, {\em On the {H}amiltonian interpolation of
		near to the identity symplectic mappings with application to symplectic
		integration algorithms}, J.\ Statist.\ Phys., 74 (1994), pp.~1117--1143.
	
	\bibitem{CGS17}
	{\sc P.~R. Conrad, M.~Girolami, S.~S\"{a}rkk\"{a}, A.~Stuart, and
		K.~Zygalakis}, {\em Statistical analysis of differential equations:
		introducing probability measures on numerical solutions}, Stat. Comput., 27
	(2017), pp.~1065--1082.
	
	\bibitem{HLW06}
	{\sc E.~Hairer, C.~Lubich, and G.~Wanner}, {\em Geometric Numerical
		Integration. Structure-Preserving Algorithms for Ordinary Differential
		Equations}, Springer Series in Computational Mathematics 31, Springer-Verlag,
	Berlin, second~ed., 2006.
	
	\bibitem{HNW93}
	{\sc E.~Hairer, S.~P. N{\o{}}rsett, and G.~Wanner}, {\em Solving Ordinary
		Differential Equations I. Nonstiff Problems}, vol.~8, Springer Verlag Series
	in Comput. Math., Berlin, 1993.
	
	\bibitem{HaW96}
	{\sc E.~Hairer and G.~Wanner}, {\em Solving ordinary differential equations II.
		Stiff and differential-algebraic problems}, Springer-Verlag, Berlin and
	Heidelberg, 1996.
	
	\bibitem{HAL12}
	{\sc Y.~Hu, A.~Abdulle, and T.~Li}, {\em Boosted hybrid method for solving
		chemical reaction systems with multiple scales in time and population size},
	Commun. Comput. Phys., 12 (2012), pp.~981--1005.
	
	\bibitem{Pav14}
	{\sc G.~A. Pavliotis}, {\em Stochastic processes and applications: Diffusion
		processes, the Fokker-Planck and Langevin equations}, vol.~60 of Texts in
	Applied Mathematics, Springer, New York, 2014.
	
	\bibitem{PaS08}
	{\sc G.~A. Pavliotis and A.~M. Stuart}, {\em Multiscale methods: Averaging and
		Homogenization}, vol.~53 of Texts in Applied Mathematics, Springer, New York,
	2008.
	
	\bibitem{Stu10}
	{\sc A.~M. Stuart}, {\em Inverse problems: a {B}ayesian perspective}, Acta
	Numer., 19 (2010), pp.~451--559.
	
\end{thebibliography}
\end{document}
