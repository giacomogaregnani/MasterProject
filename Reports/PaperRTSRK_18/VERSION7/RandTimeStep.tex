\documentclass{siamart1116}

% basics
\usepackage[left=3cm,right=3cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[utf8x]{inputenc}
\usepackage[title,titletoc]{appendix}
\usepackage{afterpage}
\usepackage{enumitem}   
\setlist[enumerate]{topsep=3pt,itemsep=3pt,label=(\roman*)}

% maths
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\newsiamremark{assumption}{Assumption}
\newsiamremark{remark}{Remark}
\newsiamremark{example}{Example}
\numberwithin{theorem}{section}

% tables
\usepackage{booktabs}

% plots
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,positioning,fit,matrix}
\usepackage[labelfont=bf]{caption}
\setlength{\belowcaptionskip}{-5pt}
\usepackage{here}
\usepackage[font=normal]{subcaption}

% title and authors
\newcommand{\TheTitle}{A probabilistic Runge-Kutta method based on random selection of time steps for chaotic and geometric integration} 
\newcommand{\TheAuthors}{A. Abdulle, G. Garegnani}
\headers{Probabilistic Runge-Kutta method based on random time steps}{\TheAuthors}
\title{{\TheTitle}}
\author{Assyr Abdulle\thanks{Mathematics Section, \'Ecole Polytechnique F\'ed\'erale de Lausanne (\email{assyr.abdulle@epfl.ch})}
		\and
		Giacomo Garegnani\thanks{Mathematics Section, \'Ecole Polytechnique F\'ed\'erale de Lausanne (\email{giacomo.garegnani@epfl.ch})}}

% my commands 
\DeclarePairedDelimiter{\ceil}{\left\lceil}{\right\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\|}{\|}
\renewcommand{\phi}{\varphi}
\renewcommand{\theta}{\vartheta}
\renewcommand{\Pr}{\mathbb{P}}
\newcommand{\eqtext}[1]{\ensuremath{\stackrel{#1}{=}}}
\newcommand{\leqtext}[1]{\ensuremath{\stackrel{#1}{\leq}}}
\newcommand{\iid}{\ensuremath{\stackrel{\text{i.i.d.}}{\sim}}}
\newcommand{\totext}[1]{\ensuremath{\stackrel{#1}{\to}}}
\newcommand{\rightarrowtext}[1]{\ensuremath{\stackrel{#1}{\longrightarrow}}}
\newcommand{\leftrightarrowtext}[1]{\ensuremath{\stackrel{#1}{\longleftrightarrow}}}
\newcommand{\pdv}[2]{\ensuremath\partial_{#2}#1}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\epl}{\varepsilon}
\newcommand{\diffL}{\mathcal{L}}
\newcommand{\prior}{\mathcal{Q}}
\newcommand{\defeq}{\coloneqq}
\newcommand{\eqdef}{\eqqcolon}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\E}{\operatorname{\mathbb{E}}}
\newcommand{\MSE}{\operatorname{MSE}}
\newcommand{\trace}{\operatorname{tr}}
\newcommand{\MH}{\mathrm{MH}}
\newcommand{\ttt}{\texttt}
\newcommand{\Hell}{d_{\mathrm{Hell}}}
\newcommand{\sksum}{{\textstyle\sum}}
\newcommand{\dd}{\mathrm{d}}
\definecolor{shade}{RGB}{100, 100, 100}
\definecolor{bordeaux}{RGB}{128, 0, 50}
\newcommand{\corr}[1]{{\color{bordeaux}#1}}

\ifpdf
\hypersetup{
	pdftitle={\TheTitle},
	pdfauthor={\TheAuthors}
}
\fi

\begin{document}
\maketitle	

\begin{abstract} 
	We present a novel probabilistic numerical method for the integration of ODEs based on random time step selection. Theoretical analysis investigating the properties of mean square and weak convergence is fully developed. We show that the measure obtained with repeated sampling converges in mean-square sense independently of the number of samples. We present the geometric properties of conservation of polynomial first integrals and symplecticity, which constitute the main advantage with respect to an additive noise method \cite{CGS16}. A complete set of numerical experiments confirms our theoretical findings.
\end{abstract}

\begin{AMS} 65L20, 65C20, 65P10 \end{AMS}

\section{Introduction} 
A variety of methods for integrating ordinary differential equations (ODEs) has been studied in the last decades. The main focus of research has been building accurate and stable deterministic approximations of the exact solution. However, these methods often provide unreliable solutions when applied to chaotic nonlinear systems of ODEs, i.e., equations for which a small perturbation in the initial condition results in a possibly large variation of the exact solution. In this case, even if the initial condition used for numerical integration is exact, which is not obtainable in floating-point precision, the deviation given by numerical errors can lead to completely wrong solutions of the underlying equation. A classic example of these problems is the Lorenz system \cite{Lor63}, a simple model of atmospheric convection, which for certain parameter values presents a typical chaotic behavior, which is nonetheless fully deterministic. Systems resulting in deterministic chaos arise in many practical applications, with the most notable example represented by chaotic chemical reactions. There are furthermore cases of dynamical systems possessing geometric features but nonetheless exhibiting a chaotic behavior. An example of these problems is given by the Hénon-Heiles equation, an Hamiltonian system for which the solution tends to a strange attractor while retaining the property of symplecticity of the flow.

In order to overcome this issue, probabilistic numerical integrators of ODEs have recently been proposed \cite{CGS16, KeH16}. These methods have the purpose of accounting for the numerical error in a statistical manner, rather than with deterministic estimates. In particular, the common underlying idea is to furnish the solution in terms of a probability distribution instead of a punctual value, as in classic solvers for ODEs. One of the most remarkable efforts is presented in Conrad et al. \cite{CGS16}, who construct probabilistic solutions with a slight modification of one-step deterministic methods, as, for example, integrators belonging to the Runge-Kutta class. In particular, they propose to perturb the deterministic numerical solution with an additive noise contribution at each time step. Scaling opportunely the noise term with respect to the time step employed for integration, they manage to explore the phase space of chaotic dynamical systems without spoiling the features of convergence of the underlying deterministic scheme. 

The main danger of adding a noise term to deterministic numerical solutions is that the random contribution can produce disruptive effects on favourable geometric features of the deterministic scheme. A direct example of this non-robust behavior is given by ODEs for which the solution is supposed to stay positive and small. In this case, the addition of a random contribution could force the solution in the negative plane, hence the numerical solution could be physically meaningless. Chemical reactions with small population size for one species at some time of the evolution is a typical physical example of such a situation. Moreover, many of these equations present strong instabilities in case any of the components turns negative, which makes it even numerically inadvisable to have negative values. An additive noise contribution yields a non-zero probability to obtain negative solutions, which can become relevant in case the magnitude of one component is small with respect to the time step.

Motivated by this undesirable property, we present in this work a new probabilistic method for ODEs based on a random selection of the time steps, hence turning the stochastic component of the numerical solution from additive, thus somehow artificial, to intrinsic in the scheme itself. In this way, favorable geometric properties of deterministic integrators are inherited by our probabilistic integrator. 

For this new robust probabilistic integrator, we are able (as in Conrad et al. \cite{CGS16}) to prove strong and weak convergence towards the exact solution of the underlying ODE. Precisely, setting the variance of the random time steps to be proportional to some power of a deterministic time step allows to retrieve the rates of the underlying Runge-Kutta integrator.

It has been pointed out by Kersting and Hennig \cite{KeH16} that the performances of a probabilistic method strongly depend on the overall quality of a set of values drawn from the numerical solution. Indeed, the numerical solution given by a probabilistic integrator is a random variable and its numerical evaluation relies on a sampling strategy, e.g., a Monte Carlo technique. Hence, the relation between sample size and step size has to be studied in the context of probabilistic integrators. Following the notation introduced by Kersting and Hennig, the numerical solution is identified by a probability measure $Q_h$ which converges to the Dirac delta $\delta_y$ concentrated on the exact solution as $h\to 0$. The quantity we have access to is another measure, denoted by $Q_h(M)$, which is obtained after $M$ repeated samplings of $Q_h$. Hence, it is expected that
\begin{equation}
Q_h(M) \rightarrowtext{M\to\infty} Q_h \rightarrowtext{h\to 0} \delta_y.
\end{equation}
In this work we show that in a mean-square sense the quality of $Q_h(M)$ is independent of $M$, and therefore the computational cost due to repeated sampling can be considered as negligible. A similar property can be shown for the additive noise method \cite{CGS16}.

A large variety of dynamical systems is characterised by geometrical properties of their flow map \cite{HLW06}. Most notably, Hamiltonian systems, which are employed for modelling a variety of physical phenomena, are endowed with the property of symplecticity, i.e., the flow of the ODE conserves areas in the state space. While geometric properties of Runge-Kutta schemes have been analysed extensively in the deterministic case, they have not been considered yet within the probabilistic setting. It is nonetheless possible to find examples of Hamiltonian systems which present a chaotic behaviour \cite{HeH64}, and for which it is therefore relevant to provide a probabilistic quantification of the numerical error. Precisely, in case the considered ODE is Hamiltonian, approximating the trajectory with a geometric integrator yields accurate numerical approximations on the long-time. A probabilistic scheme inheriting this favourable property would provide with reliable uncertainty quantification over the same time spans. The method we present in this work, being only an intrinsic modification of a Runge-Kutta integrator, fully preserves all geometric properties. In particular, we show theoretically and with numerical experiments that
\begin{itemize}
	\item[-] any Runge-Kutta method with random time steps conserves linear first integrals,
	\item[-] if the Runge-Kutta method with fixed time steps conserves quadratic first integrals, then the same method with random time steps conserves quadratic first integrals,
	\item[-] if the Runge-Kutta method with fixed time steps is symplectic, then so is the same method with random time steps.
\end{itemize}
Moreover, the aforementioned robustness property of positivity of the probabilistic solution can be considered as the simplest geometric property conserved by our numerical method. Interestingly, all the properties above are true for each trajectory of the random time-stepping integrator. We thus say that they are verified in the strong sense. For an additive noise scheme, only simplecticity is automatically inherited from the underlying deterministic scheme in the strong sense. Conversely, linear first integrals are only conserved in expectation (or weakly), while quadratic first integrals are never conserved neither in the strong nor in the weak sense. 

Probabilistic numerical methods revealed themselves to be particularly effective when applied in the frame of Bayesian inverse problems and parameterised differential equations. In general, any Bayesian inverse problems can be formulated as 
\begin{equation*}
	\text{Find } \pi(u\mid \mathcal{Y}), \text{ with } u \in X, \mathcal{Y} \in Y, \text{ given observations } \mathcal{Y} = \mathcal{G}(u) + \epl,
\end{equation*}
where $X$ and $Y$ are Banach spaces, $\mathcal{G}\colon X \to Y$ is called the forward operator, $\epl$ is an additive noise contribution and the object of interest $\pi(u\mid\mathcal{Y})$ is called the posterior distribution. In our setting, $u$ is a parameter driving an ODE, $\mathcal{Y}$ is an observed quantity of interest computable from the solution, and $\mathcal{G}$ is an operator mapping the parameter to the quantity of interest. It is only possible to approximate the action of the operator $\mathcal{G}$ when the solution of the ODE does not admit a closed-form expression. It has been shown that employing a deterministic numerical scheme, especially with a coarse discretisation, can lead to overly confident posterior distributions \cite{CGS16,COS17}. This issue can be solved by computing the posterior distribution employing a probabilistic approximation of the forward operator. In this work, we show empirically via a numerical experiment that our novel method is effective in this sense.

The paper is organised as follows. In section \ref{sec:MethodIntro} we introduce the notation we will employ throughout this work and briefly present our novel numerical scheme. Then, we show in sections \ref{sec:WeakOrder} and \ref{sec:StrongOrder} the properties of weak and mean square convergence of the numerical solution towards the exact solution of the ODE. In section \ref{sec:MonteCarlo} we then analyse the accuracy of Monte Carlo estimators drawn from the numerical solution. The geometric properties of the numerical scheme are presented in section \ref{sec:GeomProperties}, which we claim being the main results of this work. In section \ref{sec:BayesianInference} we introduce Bayesian inverse problems in the ODE setting, and show how our method can be integrated in existing sampling strategies. Finally, we show a variety of numerical experiments confirming our theoretical results in section \ref{sec:NumericalExperiments}.

\begin{figure}
	\begin{center}
	\hspace{-0.15cm}\includegraphics[]{Lorenz}
	\begin{tabular}{c@{\hspace{0.3cm}}c@{\hspace{0.3cm}}c}
		\includegraphics[]{Lorenz10} & \includegraphics[]{Lorenz20} & \includegraphics[]{Lorenz30}\\
	\end{tabular}
	\end{center}
	\caption{Time evolution of the first component of the Lorenz system in the chaotic regime and its density estimated with $2000$ trajectories of RTS-RK at three different times.}
	\label{fig:Lorenz}
\end{figure}

\section{Method presentation}\label{sec:MethodIntro}

\begin{figure}
\centering
\begin{tikzpicture}
	\draw[->] (-0.3,0) -- (8,0) node[right] {$t$};
	\draw[->] (0,-0.3) -- (0,5);
	\draw[scale=1,domain=0.5:5,smooth,variable=\t,blue] plot ({\t},{1 + 0.15*\t*\t}) node[right] {$y(t)$};
	\draw[scale=1,domain=1:6,smooth,variable=\t,black] plot ({\t},{0.85 + 0.3*\t});
	\draw (1,1.15) node[circle,draw=black, fill=white, inner sep=0pt,minimum size=3pt]{};
	\draw[dashed] (1,0) -- (1,1.10) node[above]{$y_0$};
	\draw[dashed] (4,0) -- (4,2.05) node[below right] {$y_1$};
	\draw (4,2.05) node[circle,draw=black, fill=white, inner sep=0pt,minimum size=3pt]{};
	\draw[-latex] (6,2.65) -- (4.05,2.65) node[left] {$Y_1$};
	\draw (4,2.65) node[circle,draw=black, fill=white, inner sep=0pt,minimum size=3pt]{};
	\draw[dashed] (6,0) -- (6,2.65);
	\draw (6,2.65) node[circle,draw=black, fill=white, inner sep=0pt,minimum size=3pt]{};
	\draw (1,0) node[below] {$t_0$};
	\draw (4,0) node[below] {$t_0 + h$};
	\draw (6,0) node[below] {$t_0 + H_0$};
	\draw[latex'-latex',red] (4,2.1) -- (4,2.6);
\end{tikzpicture}
\caption{Graphical representation of one step of the RTS-RK method with $\Psi_h(y) = y + hf(y)$. The red arrow is the stochastic contribution due to random time-stepping.}
\label{fig:GraphRandomStep}
\end{figure}

Let us consider a Lipschitz function $f\colon\R^d\to\R^d$ and the ODE
\begin{equation}\label{eq:ODE}
	y' = f(y), \quad y(0) = y_0 \in \R^d.
\end{equation}
In the following, we will write for simplicity the solution $y(t)$ of \eqref{eq:ODE} in terms of the flow of the ODE. In particular, we consider the family  $\{\phi_t\}_{t \geq 0}$ of functions $\phi_t\colon\R^d\to\R^d$ such that 
\begin{equation}
	y(t) = \phi_t(y_0).
\end{equation}
Given a time step $h$, let us consider a Runge-Kutta method which approximates deterministically the solution $\phi_t(y_0)$ of \eqref{eq:ODE}. In particular, we can write the numerical solution $y_k$ approximating $\phi_{t_k}(y_0)$, with $t_k = kh$ in terms of the numerical flow $\{\Psi_t\}_{t \geq 0}$, with $\Psi_t\colon\R^d\to\R^d$, which is uniquely determined by the coefficients of the method, as
\begin{equation}
	y_{k+1} = \Psi_h(y_k), \quad k = 0, 1, \ldots.
\end{equation}
In order to provide a probabilistic interpretation of the numerical solution rather than a series of punctual values, Conrad et al. propose the scheme defined by
\begin{equation}\label{eq:ProbMethAddNoise}
	Y_{k+1} = \Psi_h(Y_{k}) + \xi_k(h), \quad k = 0, 1, \ldots,
\end{equation}
where $Y_k$ is a random variable approximating $y(t_k)$ with $Y_0 = y_0$, and $\xi_k(h)$ are opportunely scaled independent and identically distributed (i.i.d.) random variables with values in $\R^d$. Maintaining the same notation as in \eqref{eq:ProbMethAddNoise}, in this work we propose a random time-stepping Runge-Kutta method (RTS-RK), i.e., the scheme defined defined by the recurrence relation
\begin{equation}\label{eq:ProbMethVarH}
	Y_{k+1} = \Psi_{H_k}(Y_k), \quad k = 0, 1, \ldots,
\end{equation}
where $Y_k$ is still a random variable approximating $y(t_k)$ and the time steps $H_k$ are locally given by a sequence  of i.i.d. random variables with values in $\R^+$. A graphical representation of one step of the RTS-RK method is given in Figure \ref{fig:GraphRandomStep}.
\begin{remark} In terms of computational cost simulating the two methods \eqref{eq:ProbMethVarH} and \eqref{eq:ProbMethAddNoise} are almost equivalent, as they imply the same number of function evaluations as the underlying deterministic solver $\Psi_h$. Nonetheless, the random time-stepping method has the slight advantage that the random variable that has to be drawn at each time step takes values in $\R_+$, while for the additive noise $\xi_k$ takes values in $\R^d$. This difference is however negligible, as modern implementations of pseudo-random generators provide with values in constant time.
\end{remark}

\subsection{Assumptions and notations} We now present the main assumptions and notations which are used throughout the rest of this work. Firstly, we have to consider the possible values taken by the random step sizes, which have to satisfy restrictions that are necessary not to spoil the properties of deterministic methods. 
\begin{assumption}\label{as:hStrong} The i.i.d. random variables $H_k$ satisfy for all $k = 0, 1, \ldots$
	\begin{enumerate}
		\item\label{as:hStrong_Pos} $H_k > 0$ a.s.,
		\item\label{as:hStrong_E} there exists $h > 0$ such that $\E H_k = h$,
		\item\label{as:hStrong_Var} there exists $p \geq 1$ such that the scaled random variables $Z_k \defeq H_k - h$ satisfy
		\begin{equation}
		\E Z_k^2 = Ch^{2p},
		\end{equation}
		which is equivalent to $\E H_k^2 = h^2 + Ch^{2p}$.
	\end{enumerate}
\end{assumption}
The class of random variable satisfying the hypotheses above is general. However, it is practical for an implementation point of view to have examples of these variables.
\begin{example}\label{ex:uniformH} Let us consider the random variables $\{H_k\}_{k\geq 0}$ such that
	\begin{equation}
	H_k \iid \mathcal{U}(h-h^p, h+h^p), \quad 0 < h < 1, \quad p \geq 1.
	\end{equation}
	We easily verify that all the assumptions \ref{as:hStrong_Pos}, \ref{as:hStrong_E} and \ref{as:hStrong_Var} are satisfied, as 
	\begin{enumerate}
		\item $H_k > 0$ a.s. trivially since $h < 1$,
		\item $\E H_k = h$ since 
		\begin{equation}
		\E H_k = \frac{1}{2}(h + h^p + h - h^p) = h.
		\end{equation}
		\item The random variables $Z_k = H_k - h$ are $Z_k \sim \mathcal{U}(-h^p, h^p)$. Therefore
		\begin{equation}
		\E Z_k^2 = \frac{4h^{2p}}{12} = \frac{1}{3}h^{2p}.
		\end{equation}
	\end{enumerate}
	Another choice of random variables could simply be 
	\begin{equation}
	H_k \iid \log\mathcal{N}\big(\log h - \log\sqrt{1 + h^{2p - 2}}, \log(1 + h^{2p-2})\big),
	\end{equation}
	for which the properties above are trivially verified (with $C = 1$), provided $p > 1$.
\end{example}
We secondly introduce an assumption on the deterministic  method underlying the RTS-RK scheme, identified by its numerical flow $\Psi_h$. 
\begin{assumption}\label{as:PsiStrong} The Runge-Kutta method defined by the numerical flow $\{\Psi_t\}_{t\geq 0}$ satisfies the following properties.
	\begin{enumerate}
		\item\label{as:PsiStrong_Order} For $h$ small enough, there exists a constant $C > 0$ such that
		\begin{equation}
		\norm{\Psi_h(y) - \phi_h(y)} \leq Ch^{q+1}, \quad \forall y \in \R^d,
		\end{equation}
		i.e., the deterministic solver has order $q$.
		\item\label{as:PsiStrong_Time} The map $t \mapsto \Psi_t(y)$ is of class $C^2(\R^+, \R^d)$ and Lipschitz continuous of constant $L_\Psi$, i.e.,
		\begin{equation}\label{eq:LipschitzPsiT}
		\norm{\Psi_t(y) - \Psi_s(y)} \leq L_\Psi \abs{t - s}, \quad \forall t, s > 0.
		\end{equation}
	\end{enumerate}
\end{assumption}


\section{Weak convergence analysis}\label{sec:WeakOrder}

The first property of the RTS-RK method we wish to analyze is its weak convergence, which gives an indication about the behavior of the numerical solution \eqref{eq:ProbMethVarH} in the mean sense. Let us define the weak order of convergence. 
\begin{definition} The numerical method \eqref{eq:ProbMethVarH} has weak order $r$ for \eqref{eq:ODE} if for any function $\Phi\in \mathcal C^\infty(\R^d, \R)$ with all derivatives bounded uniformly on $\R^d$ there exists a constant $C > 0$ independent of $h$ such that
	\begin{equation}
		\abs{\E\Phi(Y_k) - \Phi(y(kh))} \leq Ch^r,
	\end{equation}
	for all $k = 1, 2, \ldots, N$.
\end{definition} 
Let us introduce the Lie derivative of the flow $\diffL = f \cdot \nabla$, which allows us to adopt the semi-group notation for the exact solution of \eqref{eq:ODE} and write for any smooth function $\Phi$
\begin{equation}\label{eq:LieDerivative}
	\Phi(\phi_h(y)) = e^{h\diffL} \Phi(y).
\end{equation}
Moreover, let us remark that the probabilistic numerical solution $\{Y_k\}_{k\geq 0}$ forms an homogeneous Markov chain, and hence there exists an operator $\mathcal{P}_t$, the infinitesimal generator, such that
\begin{equation}
	\E \big(\Phi(Y_{k})\mid Y_0 = y\big) = (\mathcal{P}_{t_k}\Phi)(y).
\end{equation}
The family of operators $\{\mathcal{P}_t\}_{t\geq 0}$ forms a Markov semi-group together with the associativity operation
\begin{equation}
	(\mathcal{P}_{t+s}\Phi)(y) = \mathcal{P}_t(\mathcal{P}_s \Phi)(y).
\end{equation}
In order to uniform the notation with \eqref{eq:LieDerivative}, we adopt the exponential form of the infinitesimal generator and denote in the following $\mathcal{P}_t = \exp(t\diffL^h)$, where we explicitely write the dependence of the Markov generator on the step size $h$. We can now state a result of local weak convergence of the probabilistic numerical solution.
\begin{lemma}[Weak local order]\label{thm:WeakLocalOrder} Let Assumptions \ref{as:hStrong} and \ref{as:PsiStrong} hold. If $\E|H_0^4| < \infty$, there exists a constant $C > 0$ independent of $h$ such that for any function $\Phi\in \mathcal{C}^{\infty}(\R^d,\R)$ with all derivatives bounded uniformly on $\R^d$
	\begin{equation}
		\abs{\E(\Phi(Y_1)\mid Y_0 = y) - \Phi(\phi_h(y))} \leq C h^{\min\{2p, q+1\}}.
	\end{equation}
\end{lemma}
\begin{proof} Let us expand the functional $\Phi$ computed on the numerical solution as
	\begin{equation}
		\begin{aligned}
			\Phi(Y_1) &= \Phi(\Psi_{H_0}(Y_0)) \\
			&= \Phi\Big(\Psi_h(Y_0) + (H_0-h)\partial_t\Psi_h(Y_0) + \frac{1}{2}(H_0-h)^2\partial_{tt}\Psi_h(Y_0) + \OO(\abs{H_0 - h}^3)\Big)\\
			&= \Phi(\Psi_h(Y_0)) + \Big((H_0 - h)\partial_t\Psi_h(Y_0)+\frac{1}{2}(H_0-h)^2\partial_{tt}\Psi_h(Y_0)\Big) \cdot \nabla\Phi(\Psi_h(Y_0))\\
			&\quad + \frac{1}{2}(H_0 - h)^2 \partial_t \Psi_h(Y_0) \partial_t \Psi_h(Y_0)^T \colon \nabla^2\Phi(\Psi_h(Y_0)) + \OO(\abs{H_0 - h}^3),
		\end{aligned}
	\end{equation}
	where we denote by $\nabla^2\Phi$ the Hessian matrix of $\Phi$, and by $\colon$ the inner product on matrices induced by the Frobenius norm on $\R^d$, i.e., $A\colon B = \trace(A^TB)$. Taking the conditional expectation with respect to $Y_0 = y$ and applying Assumption \ref{as:hStrong} we get
	\begin{equation}
		\begin{aligned}
			e^{h\diffL^h}\Phi(y) - \Phi(\Psi_h(y)) &= \frac{1}{2} Ch^{2p}\partial_{tt}\Psi_h(y)\cdot \nabla\Phi(\Psi_h(y))\\
			&\quad + \frac{1}{2} Ch^{2p}\partial_t \Psi_h(y) \partial_t \Psi_h(y)^T \colon \nabla^2\Phi(\Psi_h(y)) + \OO(h^{3p}),
		\end{aligned}
	\end{equation}
	where we exploited Hölder inequality for the last term. Moreover, expanding $\Phi$ in $y$ we get
	\begin{equation}
		\begin{aligned}
			\Phi(\Psi_h(y)) &= \Phi\left(\Psi_0(y) + h\partial_t \Psi_0(y) + \OO(h^2)\right) \\
			&= \Phi(y) + \OO(h).
		\end{aligned}
	\end{equation}
	which implies
	\begin{equation}\label{eq:DistanceProbDet}
		\begin{aligned}
			e^{h\diffL^h}\Phi(y) - \Phi(\Psi_h(y)) &= \frac{1}{2} Ch^{2p}\partial_{tt}\Psi_h(y) \cdot \nabla\Phi(y)\\
			&\quad +\frac{1}{2}Ch^{2p}\partial_t \Psi_h(y) \partial_t \Psi_h(y)^T \colon \nabla^2\Phi(y) + \OO(h^{2p+1}).
		\end{aligned}
	\end{equation}
	Let us remark that thanks to Assumption \ref{as:PsiStrong}.\ref{as:PsiStrong_Order} we have
	\begin{equation}\label{eq:DistanceExactDet}
		e^{h\diffL}\Phi(y) - \Phi(\Psi_h(y)) = \OO(h^{q+1}).
	\end{equation}
	Combining \eqref{eq:DistanceExactDet} and \eqref{eq:DistanceProbDet} we have the one-step weak error of the probabilistic method on the original ODE, i.e., 
	\begin{equation}\label{eq:LocalWeakError}
		e^{h\diffL}\Phi(y) - e^{h\diffL^h}\Phi(y) = \OO(h^{\min\{2p, q+1\}}),
	\end{equation}
	which proves the desired result.
\end{proof}

In order to obtain a result on the global order of convergence we need a further stability assumption, which is the same as Assumption 3 in \cite{CGS16}.

\begin{assumption}\label{as:Stability} The function $f$ is such that the operator $e^{h\diffL^h}$ satisfies for all functions $\Phi\in\C^{\infty}(\R^d, \R)$ with all derivatives uniformly bounded in $\R^d$ and a positive constant $L$
	\begin{equation}
		\sup_{u\in\R^d} \abs{e^{h\diffL^h}\Phi(u)} \leq (1 + Lh)\sup_{u\in\R^d}\abs{\Phi(u)}.
	\end{equation}
\end{assumption}
\begin{remark} Given the assumptions on $f$ and $\Phi$ above, it is true for the exact solution that
	\begin{equation}
		\sup_{u\in\R^d} \abs{e^{h\diffL}\Phi(u)} \leq (1 + Lh)\sup_{u\in\R^d}\abs{\Phi(u)}.
	\end{equation}
\end{remark}

We can now state the main result on weak convergence. Let us remark that the theorem and its proof are similar to Theorem 2.4 in \cite{CGS16}.

\begin{theorem}[Weak order]\label{thm:weakOrder} Let the assumptions of Lemma \ref{thm:WeakLocalOrder} and Assumption \ref{as:Stability} hold. Then, there exists a constant $C > 0$ independent of $h$ such that for all functions $\Phi \in \mathcal{C}^\infty(\R^d,\R)$ with all derivatives bounded in $\R^d$
	\begin{equation}
		\abs{\Phi(y(kh))) - \E\Phi(Y_k)} \leq Ch^{\min\{2p - 1, q\}},
	\end{equation}
	for all $k = 1, 2, \ldots, N$.
\end{theorem}
	
\begin{proof} Let us introduce the following notation
	\begin{equation}
		\begin{aligned}
			w_k(u) &= \Phi(\phi_{t_k}(u)),\\
			W_k(u) &= \E(\Phi(Y_k) \mid Y_0 = u).
		\end{aligned}
	\end{equation}
	By the triangular inequality and the associativity property of $\exp(t\diffL^h)$, we have
	\begin{equation}
		\begin{aligned}
			\sup_{u\in\R^d}\abs{w_k - W_k} \leq \sup_{u\in\R^d}\abs{e^{h\diffL}w_{k-1}(u) - e^{h\diffL^h}w_{k-1}(u)} + \sup_{u\in\R^d}\abs{e^{h\diffL^h}w_{k-1}(u) - e^{h\diffL^h}W_{k-1}(u)}.
		\end{aligned}
	\end{equation}
	We then apply Lemma \eqref{thm:WeakLocalOrder} to the first term and Assumption \ref{as:Stability} to the second, thus obtaining
	\begin{equation}
		\sup_{u\in\R^d}\abs{w_k(u) - W_k(u)} \leq Ch^{\min\{2p, q + 1\}} + (1 + Lh)\sup_{u\in\R^d}\abs{w_{k-1}(u) - W_{k-1}(u)}.
	\end{equation} 
	Proceeding iteratively on the index $k$ and noticing that $w_0 = W_0$, we obtain
	\begin{equation}
		\begin{aligned}
			\sup_{u\in\R^d}\abs{w_k(u) - W_k(u)} &\leq C k h^{\min\{2p, q + 1\}}\\
			&\leq C T h^{\min\{2p - 1, q\}},	
		\end{aligned}
	\end{equation}
	which proves the result for any chosen initial condition $y_0$ in $\R^d$.
\end{proof}

\begin{remark} In \cite{CGS16}, Conrad et al. define ordinary and stochastic modified equations in order to prove a result of weak convergence applying techniques of backward error analysis. In particular, they show that their probabilistic solver approximates in the weak sense a stochastic differential equation (SDE) where the deterministic part is given by the original ODE. For our probabilistic solver, it is possible to prove that the numerical solutions approximates in the weak sense the solution of an SDE where the diffusion term depends on the derivative of the map $t \mapsto \Psi_t(y)$.
\end{remark}


\section{Mean square convergence analysis}\label{sec:StrongOrder}

The second property of \eqref{eq:ProbMethVarH} we analyze is its mean square order of convergence, which gives an indication on the path-wise distance between each realisation of the numerical solution and the exact solution of \eqref{eq:ODE}. Let us define the mean square order of convergence. 
\begin{definition} The numerical method \eqref{eq:ProbMethVarH} has mean square order of convergence $r$ for \eqref{eq:ODE} if there exists a constant $C > 0$ independent of $h$ such that
	\begin{equation}
	\big(\E\norm{Y_k - y(kh)}^2\big)^{1/2} \leq Ch^r
	\end{equation}
	for all $k = 1, 2, \ldots, N$.
\end{definition} 

We start by analysing how the method converges with respect to the mean step size $h$ in the local sense, i.e., after one step of the numerical integration.
\begin{lemma}[Local mean square convergence]\label{thm:StrongOrderLocal} Under Assumptions \ref{as:hStrong} and \ref{as:PsiStrong} the numerical solution $Y_1$ given by one step of the RTS-RK method \eqref{eq:ProbMethVarH} satisfies 
	\begin{equation}
		\big(\E\norm{Y_1 - y(h)}^2\big)^{1/2} \leq C h^{\min\{q + 1, p\}},
	\end{equation}
	where $C$ is a real positive constant independent of $h$ and the coefficients $p$, $q$ are given in the assumptions.
\end{lemma}
\begin{proof} By triangular and Young's inequalities we have for all $y \in \R^d$ 
	\begin{equation}
		\E\norm{\Psi_{H_0}(y) - \phi_h(y)}^2 \leq 2\E\norm{\Psi_{H_0}(y) - \Psi_h(y)}^2 + 2\norm{\Psi_h(y) - \phi_h(y)}^2.
	\end{equation}		
	We now consider Assumption \ref{as:PsiStrong}.\ref{as:PsiStrong_Time} and \ref{as:PsiStrong}.\ref{as:PsiStrong_Order}, thus getting
	\begin{equation}\label{eq:LocalErrorDecomposition}
		\E\norm{\Psi_{H_0}(y) - \phi_h(y)}^2 \leq 2L_{\Psi}^2 \E\abs{H_0 - h}^2 + 2C_1 h^{2(q+1)}.
	\end{equation}
	We now apply Assumption \ref{as:hStrong} to obtain
	\begin{equation}
	\begin{aligned}
		\E\norm{\Psi_{H_0}(y) - \phi_h(y)}^2 &= 2L_{\Psi}^2 C_2 h^{2p} + 2 C_1 h^{2(q+1)} \\
		&\leq C^2 h^{2\min\{q+1, p\}},
	\end{aligned}
	\end{equation}
	which is the desired result with $C = \max\{2L_{\Psi}^2 C_2, 2 C_1\}^{1/2}$.
\end{proof}

As a consequence of the one-step convergence, we can prove a result of global mean square convergence.
\begin{theorem}[Global mean square convergence]\label{thm:StrongOrder} Let $t_k = kh$ for $k = 1, 2, \ldots, N$, where $Nh = T$. Then, under the assumptions of Lemma \ref{thm:StrongOrderLocal} the numerical solution given by \eqref{eq:ProbMethVarH} satisfies 
	\begin{equation}\label{eq:StrongGlobalClaim}
		\sup_{k=1,2, \ldots, N} \big(\E\norm{Y_k - y(t_k)}^2\big)^{1/2} \leq C h^{\min\{q, p-1/2\}},
	\end{equation}
	where $C$ is a real positive constant independent of $h$. 
\end{theorem}
In order to prove this result, let us introduce the following standard lemmas.
\begin{lemma}\label{lem:ODERepresentation} Given the ODE \eqref{eq:ODE} with $f$ globally Lipschitz, then for any couple of vectors $y$ and $w$ in $\R^d$ and $h > 0$ we have the representation
	\begin{equation}
		\phi_h(y) - \phi_h(w) = y - w + Z,
	\end{equation}
	where $Z \in \R^d$ satisfies
	\begin{equation}
		\norm{Z} \leq C h \norm{y - w},
	\end{equation}
	for a positive constant $C$ independent of $h$.
\end{lemma}
\begin{proof} We have trivially
	\begin{equation}
		\phi_h(y) - \phi_h(w) = y - w + \int_{0}^{h}\big(f(\phi_t(y)) - f(\phi_t(w))\big) \dd t.
	\end{equation}
	Taking the norm at both sides, applying the triangular inequality and since $f$ is Lipschitz continuous with constant $L$, we have
	\begin{equation}
		\norm{\phi_h(y) - \phi_h(w)} \leq \norm{y - w} + L\int_{0}^{h}\norm{\phi_t(y) - \phi_t(w)} \dd t.
	\end{equation}
	Applying Gronwall's inequality and assuming $h < 1$ we have
	\begin{equation}
		\norm{\phi_h(y) - \phi_h(w)} \leq e^{Lh} \norm{y - w}  \leq (1 + \hat C h) \norm{y - w},
	\end{equation}
	where $\hat C = e^L - 1$. Hence, defining $Z \defeq \phi_h(y) - \phi_h(w) - (y - w)$, we have
	\begin{equation}
	\begin{aligned}
		\norm{Z} &\leq L\int_{0}^{h} \norm{\phi_t(y) - \phi_t(w)} \dd t\\
		&\leq h L(1 + \hat C h) \norm{y - w}.
	\end{aligned}
	\end{equation}
	which proves the result with $C = L(1 + \hat C)$.	
\end{proof}

\begin{lemma}\label{lem:RecurrenceBound} Suppose that for arbitrary $N$ and $k = 0, \ldots, N$ we have
	\begin{equation}
		u_{k+1} \leq (1 + Ah) u_k + Bh^r,
	\end{equation}
	where $h = T / N$, $A > 0$, $B \geq 0$, $r \geq 1$ and $u_k \geq 0, k = 0, \ldots, N$. Then
	\begin{equation}
		u_k \leq e^{AT}u_0 + \frac{B}{A}(e^{AT} - 1) h^{r-1}.
	\end{equation}
\end{lemma}
The proof of this lemma is based on a recurrence argument and is omitted in this work. We can now prove the main result on mean square convergence.
\begin{proof}[Proof of Theorem \ref{thm:StrongOrder}] In the following, we denote by $C$ a generic positive constant. Let us define $e_k^2 \defeq \E\norm{Y_k - y(t_k)}^2$. Adding and subtracting the exact flow applied to the numerical solution, we obtain
	\begin{equation}\label{eq:StrongErrorDecomp}
		\begin{aligned}
			e_{k+1}^2 = &\E\norm{\Psi_{H_k}(Y_k) - \phi_{h}(Y_k)}^2 + \E\norm{\phi_{h}(Y_k) - \phi_{h}(y(t_k))}^2 \\
					  &+ 2\E\Big(\phi_{h}(Y_k) - \phi_{h}(y(t_k)), \Psi_{H_k}(Y_k) - \phi_{h}(Y_k)\Big),
		\end{aligned}
	\end{equation}
	where we denote by $(\cdot, \cdot)$ the inner product in $\R^d$. Let us consider the three terms in \eqref{eq:StrongErrorDecomp} separately. For the first term, we have by Lemma \ref{thm:StrongOrderLocal}
	\begin{equation}\label{eq:StrongFirstTerm}
		\E\norm{\Psi_{H_k}(Y_k) - \phi_{h}(Y_k)}^2 \leq C(h^{2p} + h^{2(q + 1)}).
	\end{equation}
    For the second term, since $\phi_h$ is Lipschitz with constant $(1 + Ch)$, we have
	\begin{equation}\label{eq:StrongSecondTerm}
		\E\norm{\phi_{h}(Y_k) - \phi_{h}(y(t_k))}^2 \leq (1 + Ch)^2 e_k^2.
	\end{equation}
	Let us now consider the inner product. Thanks to Lemma \ref{lem:ODERepresentation}, we can write 
	\begin{equation}\label{eq:StrongScalarProd}
	\begin{aligned}
		\E\Big(\phi_{h}(Y_k) - \phi_{h}(y(t_k)), \Psi_{H_k}(Y_k) - \phi_{h}(Y_k)\Big) = &\E\Big(Y_k - y(t_k), \Psi_{H_k}(Y_k) - \phi_{h}(Y_k)\Big) \\
		&+ \E\Big(Z, \Psi_{H_k}(Y_k) - \phi_{h}(Y_k)\Big).
	\end{aligned}
	\end{equation}
	We bound the two terms in \eqref{eq:StrongScalarProd} separately. For the first term, by the law of total expectation, we have
	\begin{equation}
	\begin{aligned}
		\E\Big(\big(Y_k - y(t_k)\big)^\top \big(\Psi_{H_k}(Y_k) - \phi_{h}(Y_k)\big)\Big) &= \E\E\Big(\big(Y_k - y(t_k)\big)^\top \big(\Psi_{H_k}(Y_k) - \phi_{h}(Y_k)\big) \mid Y_k\Big)\\
		&= \E\Big(\big(Y_k - y(t_k)\big)^\top\E \big(\Psi_{H_k}(Y_k) - \phi_{h}(Y_k) \mid Y_k \big)\Big).
	\end{aligned}
	\end{equation}
	Applying Cauchy-Schwarz inequality to the outer expectation we get
	\begin{equation}
	\begin{aligned}
		\E\Big(\big(Y_k - y(t_k)\big)^\top \big(\Psi_{H_k}(Y_k) - \phi_{h}(Y_k)\big)\Big) &\leq \Big(\E\norm{\E\big(\Psi_{H_k}(Y_k) - \phi_{h}(Y_k) \mid Y_k\big)}^{2}\Big)^{1/2}e_k  \\
		&\leq  C(h^{2p} + h^{q+1})e_k,
	\end{aligned}
	\end{equation}
	where we applied Lemma \ref{thm:WeakLocalOrder}. We now consider the second term in \eqref{eq:StrongScalarProd}. By Cauchy-Schwarz inequality we have
	\begin{equation}
		\E\Big(Z, \Psi_{H_k}(Y_k) - \phi_{h}(Y_k)\Big) \leq \big(\E\norm{Z}^{2}\big)^{1/2} \big(\E\norm{\Psi_{H_k}(Y_k) - \phi_{h}(Y_k)}^2\big)^{1/2}.
	\end{equation}
	We now apply Lemma \ref{lem:ODERepresentation} and Lemma \ref{thm:StrongOrderLocal} to obtain
	\begin{equation}
		\E\Big(Z, \Psi_{H_k}(Y_k) - \phi_{h}(Y_k)\Big) \leq C (h^{p+1} + h^{q+2}) e_k.
	\end{equation}
	We can hence bound the scalar product in \eqref{eq:StrongScalarProd} with Young's inequality and assuming $h < 1$ as
	\begin{equation}\label{eq:StrongThirdTerm}
	\begin{aligned}
		\E\Big(\phi_{h}(Y_k) - \phi_{h}(y(t_k)), \Psi_{H_k}(Y_k) - \phi_{h}(Y_k)\Big) &\leq C(h^{p+1} + h^{q+1})e_k \\
		&\leq \frac{he_k^2}{2} + C\frac{h^{2p+1} + h^{2q+1}}{2}.
	\end{aligned}
	\end{equation}
	Combining \eqref{eq:StrongFirstTerm}, \eqref{eq:StrongSecondTerm} and \eqref{eq:StrongThirdTerm}, we have
	\begin{equation}
		e_{k+1}^2 \leq C(h^{2p} + h^{2q + 1}) + (1 + Ch)e_k^2,
	\end{equation}
	which implies the desired result by Lemma \ref{lem:RecurrenceBound} and since $e_0 = 0$.
\end{proof}

\begin{remark} Let us remark that the difference between global and local orders of convergence is not exactly one, as it usually is in the purely deterministic case. In fact, thanks to the independence of the random variables there is only a $1/2$ loss in the random part of the exponent, while the natural loss of one order is verified in the deterministic component.
\end{remark}
\begin{remark} The result of mean square convergence suggests that a reasonable choice for the noise scale $p$ is to fix it to  $q + 1/2$, where $q$ is the order of the Runge-Kutta method $\Psi_h$. In this way, the properties of convergence of the underlying deterministic method are not spoiled, nonetheless getting a probabilistic interpretation of the numerical solution.
\end{remark}
\begin{remark} The result presented in Theorem \ref{thm:StrongOrder} implies that the strong order of the method is given by $\min\{q+1, p\}$, as for Jensen's inequality we have
	\begin{equation}
		\E{\norm{Y_1 - y(h)}} \leq \big(\E\norm{Y_1 - y(h)}^2\big)^{1/2}.
	\end{equation}
\end{remark}

\section{Monte Carlo estimators}\label{sec:MonteCarlo}

The third property we analyze is the mean-square convergence of Monte Carlo estimators drawn from the random time-stepping Runge-Kutta method. Given a function $\Phi\in\mathcal{C}^\infty(\R^d, \R)$ with Lipschitz constant $L_\Phi$ and a final time $T > 0$, we consider the value $Z = \Phi(y(T))$ and its Monte Carlo estimator 
\begin{equation}\label{eq:MSE}
	\hat Z = M^{-1} \sksum_{i = 1}^M \Phi(Y_N^{(i)}),
\end{equation}
where $T = hN$ is the final time, $M$ is the number of trajectories and we denote by $\{Y_N^{(i)}\}_{i=1}^M$ a set of realizations of the numerical solution. Since we are interested in analyzing the mean square behavior of the Monte Carlo estimator, we consider its mean square error (MSE), which is defined as
\begin{equation}
	\MSE(\hat Z) = \E(Z - \hat Z)^2.
\end{equation}
In the following result, we prove that this quantity converges to zero independently of the number of trajectories $M$.

\begin{theorem}\label{thm:MSEMonteCarlo} Under Assumptions \ref{as:hStrong} and \ref{as:PsiStrong}, the Monte Carlo estimator $\hat Z$ satisfies
	\begin{equation}\label{eq:MSEBound}
		\MSE(\hat Z) \leq C\Big(h^{2\min\{q, 2p - 1\}} + \frac{h^{2\min\{q, p-1/2\}}}{M}\Big),
	\end{equation}
	where $C$ is a positive constant independent of $h$ and $M$.
\end{theorem}
\begin{proof} Thanks to the classic decomposition of the MSE, we have
	\begin{equation}\label{eq:MSEDecomposition}
		\MSE(\hat Z) = \Var \hat Z  + \big(\E(\hat Z - Z)\big)^2.
	\end{equation}
	Applying Theorem \ref{thm:weakOrder} to the second term, we have
	\begin{equation}\label{eq:MSEWeakOrder}
		\MSE(\hat Z) \leq \Var \hat Z  + Ch^{2\min\{q, 2p - 1\}}.
	\end{equation}
	The variance of the estimator can be trivially bounded exploiting the Lipschitz continuity of $\Phi$ and the independence of the samples by
	\begin{equation}\label{eq:MSELipschitz}
		\Var\hat Z \leq M^{-1} L_\Phi^2 \E \norm{Y_N - y(T)}^2.
	\end{equation}
	Applying Theorem \ref{thm:StrongOrder} we get
	\begin{equation}
		\Var\hat Z \leq M^{-1} L_\Phi^2 Ch^{2\min\{q, p-1/2\}},
	\end{equation}
	which proves the desired result.
\end{proof}
\begin{remark} This result is of critical importance as it implies that the Monte Carlo estimators drawn from \eqref{eq:ProbMethVarH} converge in the mean square sense independently of the number of samples $M$ in \eqref{eq:MSE}. Moreover, if we impose that the two terms in \eqref{eq:MSEBound} are balanced, we get that the number of trajectories $M$ has to satisfy
	\begin{equation}
		M = \begin{cases} 
			\OO(1) & \mbox{if } q \leq p - 1/2, \\
			\OO\big(h^{-2(q - (p-1/2))}\big) & \mbox{if } p-1/2 < q \leq 2p - 1, \\
			\OO\big(h^{-(2p-1)}\big) & \mbox{if } q > 2p - 1.
		\end{cases}
	\end{equation}
	Fixing $M$ as above, we have that the $\MSE$ satisfies 
	\begin{equation}
		\MSE(\hat Z) = \begin{cases} 
					   \OO\big(h^{2q}\big) & \mbox{if } q \leq 2p - 1, \\
					   \OO\big(h^{2(2p - 1)}\big) & \mbox{if } q > 2p - 1.
					   \end{cases}
	\end{equation}
	Let us introduce as a measure of error the square root of the MSE and impose a fixed tolerance $\epl$, i.e.,
	\begin{equation}
		\MSE(\hat Z)^{1/2} = \OO(\epl),
	\end{equation}
	we have that the step size $h$ has to satisfy
	\begin{equation}
		h = \begin{cases} 
			\OO\big(\epl^{1 / q}\big) & \mbox{if } q \leq 2p - 1, \\
			\OO\big(\epl^{1 / (2p - 1)}\big) & \mbox{if } q > 2p - 1.
			\end{cases}
	\end{equation}
	Therefore, the computational cost to attain such a tolerance $\epl$ is given by
	\begin{equation}
		\mathrm{cost} = \OO(Mh^{-1}) = \begin{cases} 
										   \OO(\epl^{-1/q}) & \mbox{if } q \leq p - 1/2, \\
										   \OO\big(\epl^{-2 + 2 \frac{p - 1}{q}}\big) & \mbox{if } p-1/2 < q \leq 2p - 1, \\
										   \OO\big(\epl^{-\frac{2p}{2p - 1}}\big) & \mbox{if } q > 2p - 1.
									   \end{cases}
	\end{equation}
	Let us finally remark that with the choice $p = q + 1/2$, which is the minimum $p$ for which the order of convergence of the underlying deterministic method is not spoilt by the probabilistic setting, it is sufficient to employ $M = 1$ trajectory. 
\end{remark} 

\section{Geometric properties}\label{sec:GeomProperties}
Numerical methods for ODEs are often studied in terms of their geometric properties \cite{HLW06}. In particular, we investigate here whether the random choice of time steps in \eqref{eq:ProbMethVarH} spoils the properties of the underlying deterministic Runge-Kutta method. Let us recall the definition of first integral for an ODE.
\begin{definition} Given a function $I\colon\R^d\to\R$, then $I(y)$ is a first integral of \eqref{eq:ODE} if $I'(y)f(y) = 0$ for all $y \in \R^d$. In particular, if there exists $v \in \R^d$ such that $I(y) = v^Ty$, we say that $I$ is a linear first integral of \eqref{eq:ODE}. Moreover, if there exists a symmetric matrix $S \in \R^{d\times d}$ such that $I(y) = y^TSy$, then we say that $I$ is a quadratic first integral of \eqref{eq:ODE}.
\end{definition}
If this property of the ODE is conserved by a numerical integrator, i.e., if for the any $y\in\R^d$ it is true that $I(\Psi_h(y)) = I(y)$, then we say that the numerical method conserves the first integral. In particular, this implies that the invariant is conserved along the trajectory of the numerical solution, i.e., $I(y_k) = I(y_0)$ for all $k\geq 0$.
	
The first issue we treat is the conservation of linear first integrals, which can be seen as a general case of the conservation of mass in physical systems.
\begin{theorem}\label{thm:LinearInvariants} The numerical method \eqref{eq:ProbMethVarH} conserves linear first integrals.
\end{theorem}
\begin{proof} For any linear first integral $I(y) = v^T y$ and by the definition of Runge-Kutta methods, we have
	\begin{equation}
		I(Y_1) = v^T y_0 + H_0 \sksum_{i=1}^s b_iv^T f(y_0 + H_0\sksum_{j=1}^{s} a_{ij}K_j),
	\end{equation}
	where $\{b_i\}_{i=1}^s$, $\{a_{ij}\}_{i,j=1}^s$ and $\{K_i\}_{i=1}^s$ are the coefficients and the internal stages of the Runge-Kutta method respectively. Since $I(y)$ is a first integral, $v^T f(y) = 0$ for any $y \in \R^d$. Hence $I(Y_1)  = I(y_0)$ and iteratively $I(Y_k) = I(y_0)$ for all $k \geq 0$ along the numerical trajectory.
\end{proof}
\begin{remark} Let us remark that the conservation of linear first invariants is exact for any trajectory of the numerical method, and is not an average property. In other words, we can say that \eqref{eq:ProbMethVarH} conserves linear first integrals in the strong sense. For the additive noise numerical method \eqref{eq:ProbMethAddNoise}, we have
	\begin{equation}
	\begin{aligned}
		I(Y_1) &= v^T y_0 + h \sksum_{i=1}^s b_iv^T f(y_0 + h\sksum_{j=1}^{s} a_{ij}K_j) + v^T \xi_0(h), \\
		&= v^T (y_0 + \xi_0(h)).
	\end{aligned}
	\end{equation}
	If the random variable $\xi_0$ is zero-mean, then $\E I(Y_1) = I(y_0)$ and iteratively along the solution $\E I(Y_{k+1}) = \E I(Y_k) = I(y_0)$. The linear first invariants are therefore conserved in average, but not in a path-wise fashion.
\end{remark}

It is known that no Runge-Kutta method can conserve any polynomial invariant of order $n \geq 3$ \cite{HLW06}. Nonetheless, for some particular problems there exist tailored Runge-Kutta methods which can conserve polynomial invariants of higher order. We therefore can state the following general result.
\begin{theorem}\label{thm:PolyInvariants} If the Runge-Kutta scheme defined by $\Psi_h$ conserves an invariant $I(y)$ for an ODE, then the numerical method \eqref{eq:ProbMethVarH} conserves $I(y)$ for the same ODE.
\end{theorem}
\begin{proof} Trivially, if $I(\Psi_h(y)) = I(y)$ for any $h$, then $I(\Psi_{H_0}(y)) = I(y)$ for any value that $H_0$ can assume.
\end{proof}

We now consider quadratic first invariants, which are conserved by Runge-Kutta methods that satisfy the hypotheses of Cooper's theorem \cite{HLW06}. The conservation of quadratic first invariants is of the utmost importance, e.g., for Hamiltonian systems, as it implies the symplecticity of the scheme. 
\begin{corollary}\label{thm:QuadraticInvariants} If the Runge-Kutta scheme defined by $\Psi_h$ conserves quadratic first integrals then the numerical method \eqref{eq:ProbMethVarH} conserves quadratic first integrals.
\end{corollary}
This result is a direct consequence of Theorem \ref{thm:PolyInvariants}.

\begin{remark}\label{rem:QuadraticInvariants} We see from Corollary \ref{thm:QuadraticInvariants} that quadratic first integrals are conserved by each trajectory of the numerical method. Instead, for the additive noise numerical method \eqref{eq:ProbMethAddNoise}, we find
	\begin{equation}
	\begin{aligned}
		I(Y_1) &= (\Psi_h(y_0) + \xi_0(h))^T S (\Psi_h(y_0) + \xi_0(h))^T \\
		&= I(y_0) + 2\xi_0(h)^T S  \Psi_h(y_0) + \xi_0(h)^T S \xi_0(h).
	\end{aligned}
	\end{equation}
	If the random variables are zero-mean and if there exists a matrix $Q$ such that $\E\xi_0(h)\xi_0(h)^T = Qh^{2p + 1}$ for some $p \geq 1$ (Assumption 1 in \cite{CGS16}) we then have
	\begin{equation}\label{eq:BiasQuadraticAddNoise}
		\E I(Y_1) = I(y_0) + Q : S h^{2p + 1}.
	\end{equation}
	Hence, for this method quadratic first integrals are neither conserved path-wise nor in the weak sense, as there is a bias in \eqref{eq:BiasQuadraticAddNoise}.
\end{remark}

\subsection{Hamiltonian systems} A class of dynamical systems of particular interest for their geometric properties are the Hamiltonian systems. Given a function $E(p, q)$, called the Hamiltonian, with $p$ and $q$ in $\R^d$, they can be written as
\begin{equation}\label{eq:ODEHam}
y' = J^{-1}\nabla E(y), \quad y(0) = y_0,
\end{equation}
where the matrix $J\in\R^{2d \times 2d}$ is defined as
\begin{equation}
J = \begin{pmatrix} \mathbf{0} & I \\ -I & \mathbf{0} \end{pmatrix},
\end{equation}
and where $I$ and $\mathbf{0}$ are respectively the identity and the zero matrices in $\R^{d\times d}$. It is well-known that the flow $\phi_t\colon\R^{2d}\to\R^{2d}$ of any system of the form \eqref{eq:ODEHam} is symplectic, i.e., 
\begin{equation}\label{eq:SymplecticSystem}
\pdv{\phi_t(y)}{y}^T J \pdv{\phi_t(y)}{y} = J, \quad  
\end{equation}
where we denote by $y\in \R^{2d}$ the vector $(p, q)^T$. Condition \eqref{eq:SymplecticSystem} is equivalent in a geometric sense to saying that the flow $\phi_t$ of the system conserves areas in the phase space. In a natural manner, the flow $\Psi_h$ of a one-step numerical method for \eqref{eq:ODEHam} is called symplectic it satisfies
\begin{equation}\label{eq:SymplecticMethod}
\pdv{\Psi_h(y)}{y}^T J \pdv{\Psi_h(y)}{y} = J.
\end{equation}
It has been pointed out \cite{SkG92, HLW06} that applying an adaptive step size technique to a symplectic method can spoil the properties of simplecticity in the long-time behavior. In particular, Skeel and Gear \cite{SkG92} write any adaptive technique in function of a map $\vartheta(y, h)$ such that the $k$-th time step $h_k$ is selected as $h_k = h\vartheta(y_k, h)$, where $h$ is a base value for the time step. Hence, in order to have again a symplectic method, the new condition to be satisfied is
\begin{equation}
V^T J V = J, \quad V = \pdv{\Psi_{h\vartheta(y, h)}(y)}{y} + h\pdv{\Psi_{h\vartheta(y, h)}(y)}{h}\pdv{h\vartheta(y, h)}{y}^T.
\end{equation}
In view of this condition, we can state a result of symplecticity for our numerical scheme.
\begin{theorem}\label{thm:SymplecticRandH} If the flow $\Psi_h$ of the deterministic integrator is symplectic, then the flow of the random time-stepping probabilistic method \eqref{eq:ProbMethVarH} is symplectic.
\end{theorem}
\begin{proof} We can consider the random mapping $\vartheta(y_k, h) = \Theta_k$ such that $H_k = h\vartheta(y_k, h) = h\Theta_k$, where $\{\Theta_k\}_{k \geq 0}$ is a sequence of i.i.d. random variables such that $\Theta_k > 0$ a.s., $\E\Theta_k = 1$ and $\Var\Theta_k = h^{2p - 2}$ for any $k$ so that $H_k$ satisfies Assumption \ref{as:hStrong}. Then the probabilistic method \eqref{eq:ProbMethVarH} can be written as $Y_{k+1} = \Psi_{\Theta_k h}(Y_{k})$. Since the mapping $\vartheta(y, h)$ is independent of $y$, we have
	\begin{equation}
	V \defeq \pdv{\Psi_{\Theta_k h}(y)}{y} + h\pdv{\Psi_{\Theta_k h}(y)}{h}\pdv{\vartheta(y, h)}{y}^T = \pdv{\Psi_{\Theta_k h}(y)}{y}.
	\end{equation}
	Hence, we apply the simplecticity of the deterministic method and get
	\begin{equation}
	V^T J V = J,
	\end{equation}
	which is the desired result.
\end{proof}

Let us remark that Theorem \ref{thm:SymplecticRandH} is trivially proved from Theorem \ref{thm:QuadraticInvariants} for any deterministic method which conserves quadratic first integrals, as this property implies symplecticity for a one-step numerical method. It is though interesting that a similar reasoning can be applied to the additive noise method \eqref{eq:ProbMethAddNoise}, which does not conserve quadratic first integrals. 
\begin{theorem}\label{thm:SymplecticAddNoise} If the flow $\Psi_h$ of the deterministic integrator is symplectic, then the flow of the additive noise probabilistic method \eqref{eq:ProbMethAddNoise} is symplectic.
\end{theorem}
\begin{proof} If the flow of the deterministic integrator is $\Psi_h(y)$, the flow of the probabilistic method is given by $\tilde\Psi_h(y) = \Psi_h(y) + \xi_k(h)$. Since $\xi_k(h)$ is independent of $y$, we have $\pdv{\tilde\Psi_h(y)}{y} = \pdv{\Psi_h(y)}{y}$, which proves the result.	
\end{proof}

\section{Bayesian inference inverse problems}\label{sec:BayesianInference} It has been recently shown \cite{CGS16} that probabilistic or stochastic methods for ordinary and partial differential equations guarantee good results in the context of Bayesian inverse problems. In this section, we briefly introduce a Bayesian inverse problem in the ODE setting and how the RTS-RK method can be employed in this framework. 

Let us consider a function $f_\theta \colon \R^d \to \R^d$ which depends on a real parameter $\theta \in \Theta \subset \R^n$ and the ODE
\begin{equation}\label{eq:ODEParam}
	y_\theta' = f_\theta(y), \quad y_\theta(0) = y_0 \in \R^d.
\end{equation}
In order to simplify the notation, we consider $y_0$ to be a fixed initial condition. In general, $y_0$ could depend itself on $\theta$. In the classical setting of real and numerical analysis, the main problem of interest could be summarised as  
\begin{equation}\label{eq:ForwardProblem}
	\mbox{Given the parameter } \theta \in \Theta \mbox{, determine (or approximate) the solution } y \in \R^d.
\end{equation}
In the following, we will refer to \eqref{eq:ForwardProblem} as the forward problem. As the name suggests, conversely inverse problems can be in general stated as
\begin{equation}
	\mbox{Given information on the solution } y \in \R^d \mbox{, determine (or approximate) the parameter } \theta \in \Theta.
\end{equation}
In the Bayesian setting, the aim is not determining exactly the parameter $\theta$ or giving a punctual approximation to its value, but establishing a probability measure, the posterior measure, given observations and all prior knowledge available on $\theta$. 

It is not always possible to observe the solution $y_\theta(t)$ continuously in time, but we nonetheless consider a quantity derived from $y_\theta(t)$ to be accessible. We denote the observable quantity with $\mathcal{Y}$ and we consider it to be a vector of $\R^m$. We summarise the action of the forward problem with a function $\mathcal{G} \colon \Theta \to \R^m$ directly mapping the parameter to the observations. Moreover, in real applications measurements can not be exact, hence we consider observations to be always corrupted by an additive source of noise. In particular, we model noise as a Gaussian random variable $\epl \sim \mathcal{N}(0, \Sigma_\epl)$ independent of $\theta$ and taking values in $\R^m$. Given these assumptions, we can write observations as 
\begin{equation}
	\mathcal{Y} = \mathcal{G}(\theta) + \epl.
\end{equation}
Given a new value of $\theta$, it is possible to compute the function $\mathcal{G}(\theta)$, i.e., solve the forward problem, and obtain the likelihood of the observations as 
\begin{equation}
	\pi(\mathcal{Y} \mid \theta) = e^{-V_\mathcal{Y}(\theta)},
\end{equation}
where the potential function $V_\mathcal{Y}\colon\Theta \to \R$ is defined in the Gaussian case as
\begin{equation}
	V_\mathcal{Y}(\theta) = \frac{1}{2}\big(\mathcal{G}(\theta) - \mathcal{Y}\big)^\top \Sigma_\epl^{-1} \big(\mathcal{G}(\theta) - \mathcal{Y}\big).
\end{equation}

The second building block of Bayesian inverse problems is the prior distribution, which we denote by $\pi_0(\theta)$. The prior encodes all the knowledge on the parameter that is known before observations are provided. In the following, we adopt a common abuse of notation, confounding measures and their probability density function. 

Once the likelihood model and the prior distribution are established, it is possible to compute the posterior distribution $\pi(\theta \mid \mathcal{Y})$ via Bayes' theorem, i.e.,
\begin{equation}
	\pi(\theta\mid\mathcal{Y}) = \frac{1}{Z(\mathcal{Y})} \pi_0(\theta) \pi(\mathcal{Y}\mid \theta),
\end{equation}
where $Z(\mathcal{Y})$ is a normalising constant independent of $\theta$ given by
\begin{equation}
	Z(\mathcal{Y}) = \int_{\Theta} \pi_0(\theta) \pi(\mathcal{Y}\mid \theta) \, \dd \theta.
\end{equation} 

In case the forward model $\mathcal{G}$ involves the solution of an ODE, the likelihood function will not be computed exactly. In particular, we denote by $\mathcal{G}^h(\theta)$ the result of the forward model computed with a Runge-Kutta method with time step $h$, and consequently with $V^h_\mathcal{Y}(\theta)$ and $\pi^h(\theta\mid\mathcal{Y})$ the potential and the likelihood function obtained replacing $\mathcal{G}(\theta)$ with $\mathcal{G}^h(\theta)$. We can then define the approximated posterior distribution $\pi^h(\theta \mid \mathcal{Y})$ as
\begin{equation}
	\pi^h(\theta\mid\mathcal{Y}) = \frac{1}{Z^h(\mathcal{Y})} \pi_0(\theta) \pi^h(\mathcal{Y}\mid \theta),
\end{equation}
where the normalising constant $Z^h(\mathcal{Y})$ is given by
\begin{equation}
	Z^h(\mathcal{Y}) = \int_{\Theta} \pi_0(\theta) \pi^h(\mathcal{Y}\mid \theta) \, \dd \theta.
\end{equation}
In \cite[Theorem 4.6]{Stu10}, Stuart proves that the posterior distribution $\pi^h(\theta\mid\mathcal{Y})$ converges to $\pi(\theta\mid\mathcal{Y})$ with respect to $h$ with the same rate as $V^h_\mathcal{Y}(\theta)$ converges to $V_\mathcal{Y}(\theta)$. Convergences is proved in the Hellinger distance, which is defined for probability density functions as
\begin{equation}
	\Hell\big(\pi^h(\theta\mid\mathcal{Y}), \pi(\theta\mid\mathcal{Y})\big)^2 = \frac{1}{2}\int_{\Theta} \Big(\sqrt{\pi^h(\theta\mid\mathcal{Y})} - \sqrt{\pi(\theta\mid\mathcal{Y})}\Big)^2 \, \dd \theta.
\end{equation}
Hence, when there is no restriction in computational resources and it is possible to choose $h$ small, the approximated posterior distribution can be pushed arbitrarily close to the true posterior.

In this work we consider the case when $h$ is fixed, and in particular when discretization provides with a coarse approximation of $\mathcal{G}(\theta)$. More precisely, we are interested in the case where the numerical error dominates the noise contribution. It has been shown empirically \cite{CGS16, COS17} that in this small noise limit the approximated posterior distributions can be overly confident on the value of the parameter. In particular, the expectation of $\theta$ computed under the posterior distribution presents a bias with respect to the true value, which is not highlighted by higher order moments. This undesirable phenomenon can be corrected employing a probabilistic method, as the one presented by Conrad et al. in \cite{CGS16} or the RTS-RK method, to approximate the potential $V_\mathcal{Y}(\theta)$. Let us denote by $\xi \in \mathcal{X}$ the auxiliary random variable introduced by the probabilistic method. In the case of RTS-RK, we have $\xi = (H_0, H_1, H_1, \ldots, H_{N-1})^\top$ and $\mathcal{X} \subset \R_+^N$. The likelihood function, denoted as  $\pi^h_{\mathrm{pr}}(\mathcal{Y}\mid\theta)$ is then approximated as 
\begin{equation}\label{eq:LikelihoodProb}
	\pi^h_{\mathrm{pr}}(\mathcal{Y}\mid \theta) = \int_{\mathcal{X}} e^{-V^{h,\xi}_{\mathcal{Y}}(\theta)} \, \mu(\dd \xi),
\end{equation}
where $V^{h, \xi}_{\mathcal{Y}}$ is approximation of the potential function given by the probabilistic method and $\mu$ is the probability measure induced by the auxiliary variables. The corresponding posterior distribution $\pi^h_{\mathrm{pr}}$ is then obtained applying Bayes' rule. Modifying the posterior in this manner allows to obtain qualitatively better results, which account for the uncertainty introduced by the numerical solver.

\subsection{Sampling strategies} In the discussion above we presented how to theoretically construct posterior distributions on the parameter $\theta$ from observations. Since the posterior could be complex and high-dimensional, it is  necessary in practice to rely on a Monte Carlo method in order to obtain a sample $\{\theta^{(i)}\}_{i=1}^M$ from this distribution. In order to fulfil this purpose, we choose the Markov chain Monte Carlo method (MCMC), and in particular the Metropolis Hastings method (MH) \cite{KaS05}. The main idea of MCMC methods is to build a Markov chain $\{\theta^{(i)}\}$ that serves at termination as a sample from the posterior distribution.

The MH algorithm forms the Markov chain through a function $q\colon \Theta \times \Theta \to \R$, which is usually referred to as the proposal distribution, which is for any fixed $x \in \Theta$ a probability density function in its second argument, i.e., $q(x, y) \geq 0$ for any $y \in \Theta$ and 
\begin{equation}
	\int_{\Theta} q(x, y) \dd y = 1.
\end{equation}
A common choice for $q$ is a Gaussian distribution in the second argument, i.e., for any $x \in \Theta$ the function $q(x, \cdot)$ is the probability density function of a random variable distributed as $\mathcal{N}(x, \Sigma_{q})$. In this case, the MH is referred to in literature as random walk Metropolis (RWM), and its performances strongly depend on the choice of the variance $\Sigma_{q}$. Assuming that it is possible to evaluate the posterior distribution and given an initial guess $\theta^{(i)}$, the MH algorithm proceeds as follows
\begin{enumerate}
	\item compute $\pi(\theta^{(1)}\mid \mathcal{Y})$ and set $i = 1$;
	\item propose a new value $\theta^*$ sampling from $q(\theta^{(i)}, \cdot)$;
	\item with probability 
		\begin{equation}\label{eq:AcceptanceProbability}
			\alpha(\theta^{(i)}, \hat \theta) = \min\left\{\frac{\pi(\theta^*\mid\mathcal{Y})q(\theta^*, \theta^{(i)})}{\pi(\theta^{(i)}\mid\mathcal{Y})q(\theta^{(i)}, \theta^*)}, 1\right\},
		\end{equation}
		set $\theta^{(i+1)} = \theta^*$, otherwise set $\theta^{(i+1)} = \theta^{(i)}$;
	\item if $i < M$, set $i \leftarrow i + 1$ and return to step (ii).
\end{enumerate}
It is possible to show that the stationary distribution of the Markov chain $\{\theta^{(i)}\}_{i\geq 0}$ generated by the MH algorithm is in fact the posterior distribution $\pi(\theta\mid\mathcal{Y})$.

In the probabilistic case, it is not possible to compute exactly the integral in \eqref{eq:LikelihoodProb}, and hence the algorithm above has to be slightly modified. In particular, we employ the pseudo-marginal MH (PMMH) method proposed by Andrieu and Roberts \cite{AnR09}, which allows to incorporate auxiliary variables when generating the sample. It is possible to show that if the posterior is replaced by an unbiased estimator of its value which is then used to compute the acceptance probability \eqref{eq:AcceptanceProbability}, the PMMH targets exactly the distribution $\pi_{\mathrm{pr}}(\theta \mid \mathcal{Y})$. In order to guarantee this property of exactness, it is nonetheless necessary that the estimator from the previous value is recycled in the next iteration. If the estimator of the posterior for the previous value is recomputed, the algorithm gains in efficiency but loses the property of exactness \cite{MLR16}. The PMMH algorithm can be schematically summarised as follows
\begin{enumerate}
	\item compute an unbiased estimator $\hat{\pi}^{(1)}$ of $\pi(\theta^{(1)}\mid \mathcal{Y})$ and set $i = 1$;
	\item propose a new value $\theta^*$ sampling from $q(\theta^{(i)}, \cdot)$;
	\item compute an unbiased estimator $\hat{\pi}^*$ of $\pi_{\mathrm{pr}}(\theta^*\mid\mathcal{Y})$;
	\item with probability 
		\begin{equation}\label{eq:AcceptanceProbabilityProb}
			\alpha(\theta^{(i)}, \hat \theta) = \min\left\{\frac{\hat{\pi}^* q(\hat\theta, \theta^{(i)})}{\hat{\pi}^{(i)}q(\theta^{(i)}, \hat\theta)}, 1\right\},
		\end{equation}
	set $\theta^{(i+1)} = \theta^*$ and $\hat{\pi}^{(i+1)} = \hat{\pi}^*$, otherwise set $\theta^{(i+1)} = \theta^{(i)}$ and $\hat{\pi}^{(i+1)} = \hat{\pi}^{(i)}$;
	\item if $i < M$, set $i \leftarrow i + 1$ and return to step (ii).
\end{enumerate}
Let us finally remark that the estimator posterior can be computed with a plain Monte Carlo method, which guarantees unbiasedness and hence the exactness of the PMMH method.

\section{Numerical experiments}\label{sec:NumericalExperiments}

In this section, we show numerical experiments on some classic test cases that confirm the theoretical results presented above. 

\subsection{Mean square order of convergence}

\begin{table}[!t]
	\centering
	\begin{tabular}{l|ccccc|ccccc}
		\toprule
		Method & \multicolumn{5}{c|}{ET} & \multicolumn{5}{c}{RK4} \\ 
		\cmidrule(l{2pt}r{2pt}){2-6} \cmidrule(l{2pt}r{2pt}){7-11} 
		$q$ & \multicolumn{5}{c|}{2} & \multicolumn{5}{c}{4} \\
		$p$ & 1 & 1.5 & 2 & 2.5 & 3 & 3 & 3.5 & 4 & 4.5 & 5\\
		$\min\{q, p - 1/2\}$ & 0.5 & 1 & 1.5 & 2 & 2 & 2.5 & 3 & 3.5 & 4 & 4 \\
		strong order & 0.51 & 1.02 & 1.54 & 2.01 & 2.01 & 2.50 & 3.01 & 3.56 & 4.02 & 4.01 \\
		\bottomrule
	\end{tabular}
	\caption{Mean square order of convergence for the random time-stepping explicit trapezoidal (ET) and fourth-order Runge-Kutta (RK4) as a function of the value of $p$ of Assumption \ref{as:hStrong}.}
	\label{tab:NumericalResultsStrongOrder}
\end{table}

In order to verify the result predicted in Theorem \ref{thm:StrongOrder}, we consider the FitzHug-Nagumo equation, which is defined as
\begin{equation}\label{eq:FitzNag}
\begin{aligned}
y_1' &= c\big(y_1 - \frac{y_1^3}{3} + y_2\big), && y_1(0) = -1, \\
y_2' &= -\frac{1}{c}(y_1 - a + by_2), && y_2(0) = 1,
\end{aligned}
\end{equation}
where $a, b, c$ are real parameters with values $a = 0.2$, $b = 0.2$, $c = 3$. We integrate the equation from time $t_0 = 0$ to final time $T = 1$. The reference solution is generated with an high order method on a fine time scale. We consider as deterministic solvers the explicit trapezoidal rule and the classic fourth order Runge-Kutta method, which verify Assumption \ref{as:PsiStrong} with $q = 2$ and $q = 4$ respectively. Moreover, we consider random time steps as in Example \ref{ex:uniformH}, where we vary $p$ in order to verify the order of convergence predicted in Theorem \ref{thm:StrongOrder}. We vary the mean time step $h$ taken by the random time steps $H_n$ in the range $h_i = 0.01\cdot 2^{-i}$, with $i = 0, 1, \ldots, 4$. Then, we simulate $10^3$ realizations of the numerical solution $Y_{N_i}$, with $N_i = T / h_i$ for $i = 0, 1, \ldots, 4$, and compute the approximate mean square order of convergence for each value of $h$ with a Monte Carlo mean. Results (Table \ref{tab:NumericalResultsStrongOrder}) show that the orders predicted theoretically by Proposition \ref{thm:StrongOrder} are confirmed numerically. 

\subsection{Weak order of convergence}

\begin{table}[t]
	\centering
	\begin{tabular}{l|ccc|ccccc}
		\toprule
		Method & \multicolumn{3}{c|}{ET} & \multicolumn{5}{c}{RK4} \\ 
		\cmidrule(l{2pt}r{2pt}){2-4} \cmidrule(l{2pt}r{2pt}){5-9} 
		$q$ & \multicolumn{3}{c|}{2} & \multicolumn{5}{c}{4} \\
		$p$ & 1 & 1.5 & 2 & 1 & 1.5 & 2 & 3 & 4\\
		$\min\{q, 2p - 1\}$ & 1 & 2 & 2 & 1 & 2 & 3 & 4 & 4 \\
		weak order & 0.98 & 2.06 & 2.12 & 0.90 & 1.96 & 3.01 & 3.97 & 4.08 \\
		\bottomrule
	\end{tabular}
	\caption{Weak order of convergence for the random time-stepping explicit trapezoidal (ET) and fourth-order Runge-Kutta (RK4) as a function of the value of $p$ of Assumption \ref{as:hStrong}.}
	\label{tab:NumericalResultsWeakOrder}
\end{table}

We now verify the weak order of convergence predicted in Theorem \ref{thm:weakOrder}. For this experiment we consider the ODE \eqref{eq:FitzNag} as well, with the same time scale and parameters as above. The reference solution at final time is generated in this case as well with an high-order method on a fine time scale. The deterministic integrators we choose in this experiment are the explicit trapezoidal rule and the classic fourth-order Runge-Kutta method. The mean time step varies in the range $h_i = 0.1\cdot 2^{-i}$ with $i = 0, 1, \ldots, 5$, and we vary the value of $p$ in Assumption \ref{as:hStrong} in order to verify the theoretical result of Theorem \ref{thm:weakOrder}. The function $\Phi\colon\R^d\to\R$ of the solution we consider is defined as $\phi(x) = x^\top x$. Finally, we consider $10^6$ trajectories of the numerical solution in order to approximate the expectation with a Monte Carlo sum. Results (Table \ref{tab:NumericalResultsWeakOrder}) show that the order of convergence predicted theoretically is confirmed by numerical experiments. 

\subsection{Monte Carlo estimator}

%\begin{table}[t!]
%	\centering
%	\begin{tabular}{l|cc|cccc}
%		\toprule
%		Method & \multicolumn{2}{c|}{ET} & \multicolumn{4}{c}{RK4} \\ 
%		\cmidrule(l{2pt}r{2pt}){2-3} \cmidrule(l{2pt}r{2pt}){4-7} 
%		$q$ & \multicolumn{2}{c|}{2} & \multicolumn{4}{c}{4} \\
%		$p$ & 2 & 3 & 2 & 3 & 4 & 5\\
%		$\min\{2q, 2p - 1\}$ & 3 & 4 & 3 & 5 & 7 & 8\\
%		MSE order & 3.01 & 4.05 & 3.04 & 5.02 & 7.08 & 8.06\\
%		\bottomrule
%	\end{tabular}
%	\caption{Convergence of the MSE of the Monte Carlo estimator for the random time-stepping explicit trapezoidal (ET) and fourth-order Runge-Kutta (RK4) with respect to $p$ of Assumption \ref{as:hStrong}.}
%	\label{tab:NumericalResultsMSE}
%\end{table}

\begin{figure}[t!]
	\centering
	\begin{tabular}{c@{\hspace{0.3cm}}c}
		\includegraphics[]{MonteCarloET} & \includegraphics[]{MonteCarlo} \\
	\end{tabular}
	\caption{Convergence of the MSE of the Monte Carlo estimator for the random time-stepping explicit trapezoidal (ET) and fourth-order Runge-Kutta (RK4). The dashed line corresponds to the order predicted in Theorem \ref{thm:MSEMonteCarlo} with $M = 10^3$ for ET and $M = 10^4$ for RK4.}
	\label{fig:MonteCarlo}
\end{figure}

%In the same spirit of the previous numerical experiments, we now verify numerically the validity of Theorem \ref{thm:MSEMonteCarlo}. We consider the ODE \eqref{eq:FitzNag}, with final time $T = 10$ and the same parameters as above. In this case as well, we consider the explicit trapezoidal rule and the fourth-order explicit Runge-Kutta method with random time steps having mean $h_i = 0.1\cdot 2^{-i}$ with $i = 0, 1, \ldots, 5$. We thus vary the value of $p$ of Assumption \ref{as:hStrong} and compute the mean order of convergence over 300 repetitions of the experiment. For each repetition, we consider only one trajectory to approximate the Monte Carlo estimator, i.e., $M = 1$. We then compute the error with respect to a reference solution given by a high-order method with a small time step. Results (Table \ref{tab:NumericalResultsMSE}) show that the convergence order given in Theorem \ref{thm:MSEMonteCarlo} is respected in practice.

In the same spirit of the previous numerical experiments, we now verify numerically the validity of Theorem \ref{thm:MSEMonteCarlo}. We consider the ODE \eqref{eq:FitzNag}, with final time $T = 1$ and the same parameters as above. In this case as well, we consider the explicit trapezoidal rule and the fourth-order explicit Runge-Kutta method with random time steps having mean $h_i = 0.25\cdot 2^{-i}$ with $i = 0, 1, \ldots, 8$. For the explicit trapezoidal rule, we fix $M = 10^3$ and $p = 1.5$, so that for bigger values of $h$ the first term in the bound presented in Theorem \ref{thm:MSEMonteCarlo} dominates, while in the regime of small $h$, the higher order of the first term makes the second larger in magnitude. This behaviour results in the change of slope in the convergence plot which can be observed in Figure \ref{fig:MonteCarlo}, both in the theoretical estimate and in the numerical results. We perform the same experiment using the fourth-order explicit Runge-Kutta method, fixing $M = 10^4$ and $p = 2$, thus obtaining a numerical confirmation of the theoretical result.

\subsection{Robustness} In this numerical experiment we verify the robustness of RTS-RK when applied to chemical reactions. Let us consider the Peroxide-Oxide chemical reaction, which is macroscopically defined the following balance equation
\begin{equation}
	\mathrm{O}_2 + 2\mathrm{NADH} + 2\mathrm{H}^+ \to 2\mathrm{H}_2\mathrm{O} + 2\mathrm{NAD}^+.
\end{equation}
This reaction has to be catalyzed by an enzyme to take place, which reacts with the reagents to create intermediate products of the reaction. A successful model \cite{Ols83} to describe the time-evolution of the chemical system is the following
\begin{equation}
\begin{aligned}
	\mathrm{B} + \mathrm{X} &\rightarrowtext{k_1} 2 \mathrm{X}, 
	&&2\mathrm{X} \rightarrowtext{k_2} 2\mathrm{Y}, 
	&&\mathrm{A} + \mathrm{B} + \mathrm{Y} \rightarrowtext{k_3} 3 \mathrm{X}, \\
	\mathrm{X} &\rightarrowtext{k_4} \mathrm{P}, 
	&&\mathrm{Y} \rightarrowtext{k_5} \mathrm{Q}, 
	&&\mathrm{X_0} \rightarrowtext{k_6} \mathrm{X}, \\
	\mathrm{A_0} &\leftrightarrowtext{k_7} \mathrm{A}, 
	&&\mathrm{B_0} \rightarrowtext{k_8} \mathrm{B}.
\end{aligned}
\end{equation}
Here, A and B are respectively $[\mathrm{O}_2]$ and $[\mathrm{NADH}]$, P, Q are the products and X, Y are intermediates results of the reaction process. It is therefore possible to model the time evolution of the reaction with the following system of nonlinear ODEs 
\begin{equation}\label{eq:PeroxOx}
\begin{aligned}
	\mathrm{A}' &= k_7  (\mathrm{A}_0 - \mathrm{A}) - k_3  \mathrm{A}\mathrm{B}\mathrm{Y}, &&\mathrm{A}(0) = 6, \\
	\mathrm{B}' &= k_8\mathrm{B}_0 - k_1  \mathrm{B}\mathrm{X} - k_3  \mathrm{A}\mathrm{B}\mathrm{Y}, &&\mathrm{B}(0) = 58, \\
	\mathrm{X}' &= k_1  \mathrm{B}\mathrm{X} - 2  k_2  \mathrm{X}^2 + 3  k_3 \mathrm{A}\mathrm{B}\mathrm{Y} - k_4  \mathrm{X} + k_6\mathrm{X}_0,&& \mathrm{X}(0) = 0, \\
	\mathrm{Y}' &= 2  k_2  \mathrm{X}^2 - k_5  \mathrm{Y} - k_3  \mathrm{A}\mathrm{B}\mathrm{Y}, && Y(0) = 0,
\end{aligned}
\end{equation}
where $\mathrm{A}_0 = 8$, $\mathrm{B}_0 = 1$, $\mathrm{X}_0 = 1$ and the real parameters $k_i$, $i = 1, \ldots, 8$ representing the reaction rates take values
\begin{equation}
\begin{aligned}
k_1 &= 0.35, &&k_2 = 250, &&k_3 = 0.035, &&k_4 = 20,\\
k_5 &= 5.35, &&k_6 = 10^{-5}, &&k_7 = 0.1, &&k_8 = 0.825.
\end{aligned}
\end{equation}            
It has been shown \cite{Ols83} that for these values of the parameters the system exhibits a chaotic behavior. In particular, at long time the trajectories are captured in a strange attractor, and the system shows a strong sensitivity to perturbations on the initial condition. 

Since the components of the solution represent the concentration of chemicals, we require the numerical solution to be positive. Apart from physical considerations, numerically we observe that if one of the components takes negative values, the solution shows strong instabilities. Let us consider the maximum time step $h_{\max}$ for which the deterministic numerical solution stays positive and thus is stable. The probability that the probabilistic method presents instabilities before a time $T = hN$ is therefore given by
\begin{equation}
	\Pr(Y_k \text{ is unstable}) = \Pr\big(\textstyle \bigcup_{k=1}^N A_k\big), \quad A_k = \{H_k > h_{\max}\}.
\end{equation}
Depending on the distribution of the variables $H_k$, this probability can be computable explicitly. If they are drawn from a uniform distribution as in Example \ref{ex:uniformH}, we can just set $h + h^p < h_{\max}$ thus having a null probability of being unstable. Conversely, for the additive noise method we can have disruptive effects even with small time steps if the solution has a small magnitude. Let us consider a one-dimensional problem and \eqref{eq:ProbMethAddNoise} with $\xi_k(h)$ i.i.d. random variables $\mathcal{N}(0, h^{2p + 1})$. Then, for example, if the deterministic method would predict from time $t_k = kh$ to time $t_{k+1}$ a value $\Psi_h(Y_k) = h^{p + 1/2}$, the probability that $Y_{k+1}$ is negative is
\begin{equation}
	\Pr(Y_{k+1} < 0 \mid \Psi_h(Y_k) = h^p) = \frac{1}{2}(1 + \mathrm{erf}(-1 / \sqrt{2})) \approx 0.16.
\end{equation}
Hence, integrating over long time an equation whose components should stay positive with a small magnitude employing the additive noise method likely produces instabilities regardless of the chosen time step.

Let us apply the additive noise method \eqref{eq:ProbMethAddNoise} and the random time-stepping scheme \eqref{eq:ProbMethVarH} to equation \eqref{eq:PeroxOx}. We choose $h = 0.05$ as the mean time steps for \eqref{eq:ProbMethVarH} and as the time step for \eqref{eq:ProbMethAddNoise}, while we employ the stabilized explicit Runge-Kutta-Chebyshev method (RKC) \cite{HoK71} as deterministic integrator. Results (Figure \ref{fig:OxPeroxTraj}) show that the method we propose in this work conserves the positivity of the numerical solution while capturing the chaotic nature of the chemical reaction, while the additive noise scheme produces negative values, thus showing strong instabilities in the long-time behavior.
\begin{figure}
	\begin{center} 
		\begin{subfigure}[b]{1\textwidth}
			\centering
			\hspace{-0.3cm}\includegraphics[]{OxPerox}
		\end{subfigure}	
		\begin{subfigure}[b]{1\textwidth}
			\centering
			\includegraphics[]{OxPeroxAdd}
		\end{subfigure} 
	\end{center}
	\caption{Thirty trajectories of the numerical value of the concentration of the X species for the random time-stepping and additive noise methods.}
	\label{fig:OxPeroxTraj}
\end{figure}

\subsection{Conservation of quadratic first integrals} 

\begin{figure}[t]
	\begin{center}
		\begin{tabular}{c@{\hspace{0.3cm}}c}
			\includegraphics[]{KeplerOne} & \includegraphics[]{KeplerTwo} \\
			\includegraphics[]{KeplerOneAdd} & \includegraphics[]{KeplerTwoAdd} \\
		\end{tabular}
	\end{center}
	\hspace{0.84cm}\includegraphics[]{KeplerMom}
	\caption{Trajectories of \eqref{eq:KeplerPert} given by \eqref{eq:ProbMethVarH} for $0 \leq t \leq 200$ and $3800 \leq t \leq 4000$ (first row, left and right), and by \eqref{eq:ProbMethAddNoise} for $0 \leq t \leq 200$ and $200 \leq t \leq 400$ (second row, left and right). Error on the angular momentum for $0 \leq t \leq 4000$ given by the two methods.}
	\label{fig:Kepler}
\end{figure}

We consider the Kepler system with perturbation, a simple model for the two-body problems in celestial mechanics, which reads
\begin{equation}\label{eq:KeplerPert}
\begin{aligned}
	q_1' &= p_1, && p_1' = -\frac{q_1}{\norm{q}^3} - \frac{\delta q_1}{\norm{q}^5}, \\
	q_2' &= p_2, && p_2' = -\frac{q_2}{\norm{q}^3} - \frac{\delta q_2}{\norm{q}^5},
\end{aligned}
\end{equation}
where $p_1$, $p_2$ are the two components of the velocity and $q_1$, $q_2$ are the two components of the position. We assume the perturbation parameter $\delta$ to be equal to 0.015 and the initial condition to be
\begin{equation}
	q_1(0) = 1 − e,\quad q_2(0) = 0, \quad p_1(0) = 0, \quad p_2(0) = \sqrt{(1 + e)/(1 − e)},
\end{equation}
where $e = 0.6$ is the eccentricity. It is well-known that this equation has the Hamiltonian and the angular momentum as quadratic first integrals. In particular, we focus here on the angular momentum, which reads
\begin{equation}
	I(p, q) = q_1p_2 - q_2p_1.
\end{equation}
We consider the simplest Gauss collocation method, the implicit midpoint rule, as the deterministic Runge-Kutta method. It is known that all the integrators based on Gauss nodes conserve quadratic first integrals. We expect therefore that the random time-stepping method \eqref{eq:ProbMethVarH} implemented with $\Psi_h$ given by the implicit midpoint rule conserves this property thanks to Theorem \ref{thm:QuadraticInvariants}. We therefore integrate \eqref{eq:KeplerPert} with mean time step $h = 0.01$ from time $t = 0$ to time $t = 4000$ which corresponds to approximately $636$ revolutions of the system, thus considering the long-time behavior of the numerical solution. Moreover, we consider the additive noise method \eqref{eq:ProbMethAddNoise} with $h = 0.01$, expecting that the first integral will not be conserved. Results (Figure \ref{fig:Kepler}) show that the method \eqref{eq:ProbMethVarH} conserves perfectly the angular momentum, while \eqref{eq:ProbMethAddNoise} satisfies the result shown in Remark \ref{rem:QuadraticInvariants} only in the short time, while in the long time the solution does not reflect the physical features of the underlying system.

\subsection{Chaotic Hamiltonian systems}

\begin{figure}[t!]
	\begin{center}
		\begin{tabular}{cc}
			\includegraphics[]{HHStep} & \includegraphics[]{HHAdd} \\
		\end{tabular}
		\begin{tabular}{c}
			\includegraphics[]{HHStepChaos} \\
			\includegraphics[]{HHStepHam}
		\end{tabular}
	\end{center}
	\caption{A trajectory of \eqref{eq:HenHei} given by \eqref{eq:ProbMethVarH} for $0 \leq t \leq 350$ (first row, left), and by \eqref{eq:ProbMethAddNoise} (first row, right). Value of $q_1$ for the whole set of realizations of \eqref{eq:ProbMethVarH} for $0 \leq t \leq 600$. Error on the Hamiltonian $H(p_0, q_0) = 0.13$ for $0 \leq t \leq 600$.}
	\label{fig:HH}
\end{figure}

Probabilistic methods for differential equations are of particular interest when applied to chaotic systems. Hamiltonian systems can present chaotic features in the long-time behavior, while conserving geometric properties. It is therefore relevant to study whether both the geometric features and the chaotic nature are captured by the probabilistic integrator. An example of this class of system is given by the Hénon-Heiles problem \cite{HeH64}, whose Hamiltonian is given by
\begin{equation}\label{eq:HenHeiHam}
	H(p, q) = \frac{1}{2}\norm{p}^2 + \frac{1}{2}\norm{q}^2 + q_1^2q_2 - \frac{1}{3}q_2^3,
\end{equation}
where $p$ and $q$ are vectors of $\R^2$ representing velocity and position respectively. The corresponding system of ODEs can then be written as
\begin{equation}\label{eq:HenHei}
\begin{aligned}
	p' &= -\nabla_q H(p, q), &&p(0) = p_0 \in \R^2,\\
	q' &= \nabla_p H(p, q), &&q(0) = q_0 \in \R^2.
\end{aligned}
\end{equation}
It is well-known that this systems presents typical deterministic chaos for values $H > 1/8$. We consider as the deterministic integrator the $s$-stage trapezoidal rule \cite{IaT09}, which is defined by its coefficients $\{b_i\}_{i=1}^s$, $\{c_i\}_{i=1}^s$, with $c_1 = 0$ and $c_s = 1$. The coefficients $\{a_{ij}\}_{i,j=1}^s$ are then determined as $a_{ij} = c_ib_j$. It has been proved that this numerical scheme conserves polynomial Hamiltonians of degree less than $s$ for $s$ even, or $s + 1$ for $s$ odd. Thanks to Theorem \ref{thm:PolyInvariants}, if we choose as the $s$-stage trapezoidal rule, with $s \geq 4$, as the deterministic component $\Psi_h$ of \eqref{eq:ProbMethVarH}, the Hamiltonian \eqref{eq:HenHeiHam} should be conserved exactly by all the numerical trajectories. On the other hand, we do not expect the additive noise method \eqref{eq:ProbMethAddNoise} to conserve the Hamiltonian in this case.

For this numerical experiment, we assume the initial condition to be
\begin{equation}
	q_1(0) = 0.5,\quad q_2(0) = 0, \quad p_1(0) = 0, \quad p_2(0) = 0.1,
\end{equation}
so that the Hamiltonian is $H = 0.13 > 1/8$ and \eqref{eq:HenHei} is in the chaotic regime. We then integrate the equation for time $t \in [0, 600]$ with the two probabilistic versions of the five-stage trapezoidal rule choosing as mean time step $h = 0.01$ for \eqref{eq:ProbMethVarH} and fixing $h = 0.01$ for \eqref{eq:ProbMethAddNoise}. We compute $M = 10$ trajectories for both the numerical methods. Results (Figure \ref{fig:HH}) show that each trajectory given by the random time-stepping technique conserves the Hamiltonian function, nonetheless capturing the chaotic behavior of the system at long time. In fact, after a short time where all trajectories approximately coincide, the attractor in the phase space is explored fully by the set of trajectories. On the other hand, the additive noise method does not conserve the Hamiltonian neither path-wise nor in the mean sense. Furthermore, due to this deviation from the correct energy value, all trajectories present an unstable behavior after a short time, while the random time-stepping technique is stable for the same value of $h$.

\subsection{Bayesian inferential problems}

\begin{figure}[t!]
	\begin{center}
		\begin{tabular}{c@{\hspace{0.3cm}}c}
			\includegraphics[]{PostDet} & \includegraphics[]{PostProb} \\ 
		\end{tabular}
	\end{center}
	\caption{Posterior distribution for $\exp(\theta_3)$ obtained with standard RWM and Euler forward and PMMH and RTS-RK Euler forward. In the plot, $h_i = 0.1\cdot 2^{-i}$. The dashed vertical line indicates the true value of the parameter $\exp(\theta^*_3)$.}
	\label{fig:MCMC}
\end{figure}

Let us consider the FitzHug-Nagumo equation, which we recall being defined by
\begin{equation}
\begin{aligned}
y_{\theta, 1}' &= c\big(y_{\theta, 1} - \frac{y_{\theta, 1}^3}{3} + y_{\theta, 2}\big), && y_{\theta, 1}(0) = -1, \\
y_{\theta, 2}' &= -\frac{1}{c}(y_{\theta, 1} - a + b y_{\theta, 2}), && y_{\theta, 2}(0) = 1.
\end{aligned}
\end{equation}
In the spirit of Bayesian inverse problems, let us consider $\theta = (\log(a), \log(b), \log(c))^\top$ to be an unknown parameter and investigate the problem of inferring its value from observations. The exponential transformation operated on each component of the parameter is taken to ensure positiveness of $(a, b, c)^\top$ for any value of $\theta$.  We consider the forward problem operator $\mathcal{G} \colon \R^n \to \R^m$ introduced in Section \ref{sec:BayesianInference} to be defined by
\begin{equation}
	\mathcal{G}(\theta) = (y_\theta(0.1)^\top,\, y_\theta(0.2)^\top, \ldots,\, y_\theta(1)^\top)^\top,
\end{equation} 
thus $n = 3$ and $m = 20$. Synthetic observations are then generated fixing $\theta$ to its true value $\theta^* = (\log(0.2), \log(0.2), \log(3))^\top$ and integrating the equation with a Runge-Kutta solver with a small time step and biased by Gaussian observational noise, i.e.,
\begin{equation}
	\mathcal{Y} = \mathcal{G}(\theta) + \eta, \quad \eta \sim \mathcal{N}\big(0, (0.05)^2 I_{20}\big),
\end{equation}
where $I_{r}$ is the identity matrix in $\R^{r\times r}$ for $r\in\N$. We fix the prior distribution on $\theta$ to be a standard Gaussian $\mathcal{N}(0, I_3)$. Let us remark that since the amount of noise in the observational model is small, this choice of prior is not informative on the true value of the parameter. We then sample from the posterior distributions $\pi^h(\theta \mid \mathcal Y)$ and $\pi^h_{\mathrm{pr}}(\theta \mid \mathcal Y)$ employing respectively
\begin{enumerate}
	 \item the standard RWM with the forward map approximated by the explicit Euler method,
	 \item the PMMH with the forward map approximated via RTS-RK implemented with the explicit Euler method.
\end{enumerate}
In both cases, we perform the MCMC algorithm four times varying the step size $h$ (or mean step size for RTS-RK), precisely choosing $h_i = 0.1\cdot2^{-i}$, $i = 0, 1, 2, 3, 4$. Let us finally recall that for the PMMH algorithm the stationary distribution of the generated Markov chain is independent of the number of samples chosen to approximate the posterior at each iteration with Monte Carlo. Hence, we tune this number empirically in order to obtain good performances from the algorithm. In Figure \ref{fig:MCMC} we show the marginal posterior for the parameter $\exp(\theta_3)$. It is possible to remark that while for the standard deterministic approximation of $\mathcal{G}$ the coarser values of $h$ provide with an overconfident posterior centred in a biased value, the same computation performed with RTS-RK and PMMH accounts better for the statistical uncertainty introduced by the discretisation error.


\bibliographystyle{siamplain}
\bibliography{anmc}

\end{document}
