\documentclass{siamart1116}

% basics
\usepackage[left=3cm,right=3cm,top=3cm,bottom=4cm]{geometry}
\usepackage[utf8x]{inputenc}
\usepackage[title,titletoc]{appendix}
\usepackage{afterpage}
\usepackage{enumitem}   
\setlist[enumerate]{topsep=3pt,itemsep=3pt,label=(\roman*)}

% maths
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\newsiamremark{assumption}{Assumption}
\newsiamremark{remark}{Remark}
\newsiamremark{example}{Example}
\numberwithin{theorem}{section}

% plots
\usepackage{pgfplots} 
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{here}
\usepackage[labelfont=bf]{caption}

% tables
\usepackage{booktabs}

% title and authors
\newcommand{\TheTitle}{A probabilistic integrator of ordinary differential equations based on Runge-Kutta methods with random selection of the time steps} 
\newcommand{\TheAuthors}{A. Abdulle, G. Garegnani}
\headers{Probabilistic integrator of ODEs with random time steps}{\TheAuthors}
\title{{\TheTitle}}
\author{Assyr Abdulle\thanks{Mathematics Section, \'Ecole Polytechnique F\'ed\'erale de Lausanne (\email{assyr.abdulle@epfl.ch})}
	\and
	Giacomo Garegnani\thanks{Mathematics Section, \'Ecole Polytechnique F\'ed\'erale de Lausanne (\email{giacomo.garegnani@epfl.ch})}}

% my commands 
\DeclarePairedDelimiter{\ceil}{\left\lceil}{\right\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\|}{\|}
\renewcommand{\phi}{\varphi}
\newcommand{\eqtext}[1]{\ensuremath{\stackrel{#1}{=}}}
\newcommand{\leqtext}[1]{\ensuremath{\stackrel{#1}{\leq}}}
\newcommand{\iid}{\ensuremath{\stackrel{\text{i.i.d.}}{\sim}}}
\newcommand{\totext}[1]{\ensuremath{\stackrel{#1}{\to}}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\epl}{\varepsilon}
\newcommand{\diffL}{\mathcal{L}}
\newcommand{\prior}{\mathcal{Q}}
\newcommand{\defeq}{\coloneqq}
\newcommand{\eqdef}{\eqqcolon}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\E}{\operatorname{\mathbb{E}}}
\newcommand{\MSE}{\operatorname{MSE}}
\newcommand{\trace}{\operatorname{tr}}
\newcommand{\MH}{\mathrm{MH}}
\newcommand{\ttt}{\texttt}
\newcommand{\Hell}{d_{\mathrm{Hell}}}
\newcommand{\sksum}{\textstyle\sum}
\newcommand{\dd}{\mathrm{d}}


\ifpdf
\hypersetup{
	pdftitle={\TheTitle},
	pdfauthor={\TheAuthors}
}
\fi

\begin{document}
	
\maketitle	

\begin{abstract}
\end{abstract}

\section{Introduction}

\section{Random time stepping}

Let us consider a Lipschitz function $f\colon\R^d\to\R^d$ and the ODE
\begin{equation}\label{eq:ODE}
	y' = f(y), \quad y(0) = y_0 \in \R^d.
\end{equation}
We can write the solution $y(t)$ in terms of the flow of the ODE, i.e., the family $\{\phi_t\}_{t \geq 0}$ of functions $\phi_t\colon\R^d\to\R^d$ such that 
\begin{equation}
	y(t) = \phi_t(y_0).
\end{equation}
Let us consider a Runge-Kutta method for \eqref{eq:ODE}. Given a time step $h$, we can write the numerical solution at $y_n$ approximation of $y(t_n)$, with $t_n = nh$ in terms of the numerical flow $\{\Psi_t\}_{t \geq 0}$, with $\Psi_t\colon\R^d\to\R^d$, which is uniquely determined by the coefficients of the method, as
\begin{equation}
	y_{k+1} = \Psi_h(y_k).
\end{equation}
If the ODE is chaotic, integrating the equation with different time steps leads to different numerical solutions. In order to describe the uncertainty of the numerical solution, we choose at each time $t_k$ the time step as the realization of a random variable $H_k$. Therefore, the numerical solution is given by a discrete stochastic process $\{Y_k\}_{k\geq 0}$ such that
\begin{equation}\label{eq:ProbMethVarH}
	Y_{k+1} = \Psi_{H_k}(Y_k).
\end{equation}
This probabilistic numerical method shares the basic idea with \cite{CGS16}. 

\section{Strong order analysis}

We wish to deduce a result of strong convergence for the numerical method \eqref{eq:ProbMethVarH}. 
\begin{definition} The numerical method \eqref{eq:ProbMethVarH} has strong local order $r$ for \eqref{eq:ODE} if there exists a constant $C > 0$ such that
	\begin{equation}
	\E\abs{Y_1 - y(h)} \leq Ch^{r+1}.
	\end{equation}
\end{definition} 
In the following we will present the assumptions needed to prove a result of strong convergence. First of all, the following assumption on the random variables $\{H_k\}_{k\geq 0}$ is needed for strong convergence.
\begin{assumption}\label{as:hStrong} The i.i.d. random variables $H_k$ satisfy for all $k = 1, 2, \ldots$
	\begin{enumerate}
		\item $H_k > 0$ a.s.,
		\item there exists $h > 0$ such that $\E H_k = h$,
		\item there exists $p \geq 1$ such that the scaled random variables $Z_k \defeq H_k - h$ satisfy
		\begin{equation}
			\E Z_k^2 = Ch^{2p},
		\end{equation}
		which is equivalent to $\E H_k^2 = h^2 + Ch^{2p}$.
	\end{enumerate}
\end{assumption}
The class of random variable satisfying the hypotheses above is general. However, it is practical for an implementation point of view to have examples of these variables.
\begin{example}\label{ex:uniformH} An example of random variables $H_k$ satisfying the properties above is given by
	\begin{equation}
		H_k \iid \mathcal{U}(h-h^p, h+h^p), \quad 0 < h \leq 1, \quad p \geq 1.
	\end{equation}
	Let us verify all the assumptions above for this choice of the time steps
	\begin{enumerate}
		\item $H_k > 0$ a.s. trivially since $h \leq 1$,
		\item $\E H_k = h$ since 
		\begin{equation}
			\E H_k = \frac{1}{2}(h + h^p + h - h^p) = h.
		\end{equation}
		\item The random variables $Z_k = H_k - h$ are $Z_k \sim \mathcal{U}(-h^p, h^p)$. Therefore
		\begin{equation}
			\E Z_k^2 = \frac{4h^{2p}}{12} = \frac{1}{3}h^{2p}.
		\end{equation}
	\end{enumerate}
\end{example}
The following assumption on the numerical flow is needed.
\begin{assumption}\label{as:PsiStrong} The Runge-Kutta method defined by the numerical flow $\{\Psi_t\}_{t\geq 0}$ satisfies the following properties.
	\begin{enumerate}
		\item\label{as:PsiStrong_Order} For $h$ small enough, there exists a constant $C > 0$ such that
			\begin{equation}
				\abs{\Psi_h(y) - \phi_h(y)} \leq Ch^{q+1}, \quad \forall y \in \R^d.
			\end{equation}
		\item\label{as:PsiStrong_Time} The map $t \mapsto \Psi_t(y)$ is of class $\mathcal{C}^2(\R^+, \R^d)$ and is Lipschitz continuous of constant $L_\Psi$, i.e., 
			\begin{equation}\label{eq:LipschitzPsiT}
				\abs{\Psi_t(y) - \Psi_s(y)} \leq L_\Psi \abs{t - s}, \quad \forall t, s > 0.
			\end{equation}
		\item\label{as:PsiStrong_Space} There exists a constant $C > 0$ independent of $h$ such that 
			\begin{equation}\label{eq:LipschitzPsiY}
				\abs{\Psi_h(y) - \Psi_h(z)} \leq (1 + Ch) \abs{y - z}, \quad \forall y, z \in \R^d.
			\end{equation}
	\end{enumerate}
\end{assumption}
The local strong convergence of \eqref{eq:ProbMethVarH} can now be proved. 
\begin{theorem}[Strong local order]\label{thm:StrongOrderLocal} Under Assumptions \ref{as:hStrong} and \ref{as:PsiStrong} the numerical solution $Y_1$ given by one step of \eqref{eq:ProbMethVarH} satisfies 
	\begin{equation}
	\E\abs{Y_1 - y(h)} \leq C h^{\min\{q + 1, p\}},
	\end{equation}
	where $C$ is a real positive constant independent of $h$ and the coefficients $p$, $q$ are given in the assumptions.
\end{theorem}
\begin{proof} By the triangular inequality we have for all $y \in \R^d$ 
	\begin{equation}
		\E\abs{\Psi_{H_0}(y) - \phi_h(y)} \leq \E\abs{\Psi_{H_0}(y) - \Psi_h(y)} + \E\abs{\Psi_h(y) - \phi_h(y)}.
	\end{equation}		
	We now consider Assumption \ref{as:PsiStrong}.\ref{as:PsiStrong_Time} and \ref{as:PsiStrong}.\ref{as:PsiStrong_Order}, thus getting
	\begin{equation}
		\E\abs{\Psi_{H_0}(y) - \phi_h(y)} \leq L_{\Psi} \E\abs{H_0 - h} + C_1 h^{q+1}.
	\end{equation}
	We now apply Jensen's inequality and Assumption \ref{as:hStrong} to obtain
	\begin{equation}
	\begin{aligned}
		\E\abs{\Psi_{H_0}(y) - \phi_h(y)} & \leq L (\E\abs{H_0 - h}^{2})^{1/2} + C_1 h^{q+1}\\
		&\leq L h^p + C_1 h^{q+1} \\
		&\leq C h^{\min\{q+1, p\}},
	\end{aligned}
	\end{equation}
	which is the desired result.
\end{proof}
As a consequence of the one-step convergence, we can prove a result of strong global convergence.
\begin{theorem}[Strong global order]\label{thm:StrongOrder} If $t_k = kh$ for $k = 1, 2, \ldots, N$, where $Nh = T$, and under Assumptions \ref{as:hStrong} and \ref{as:PsiStrong} the numerical solution given by \eqref{eq:ProbMethVarH} satisfies 
	\begin{equation}\label{eq:StrongGlobalClaim}
		\sup_{k=1,2, \ldots, N} \E\abs{Y_k - y(t_k)} \leq C h^{\min\{q, p-1/2\}},
	\end{equation}
	where $C$ is a real positive constant independent of $h$. 
\end{theorem}
\begin{proof} Let us define $e_k \defeq \E\abs{Y_k - y(t_k)}$. Thanks to the triangular inequality we have
	\begin{equation}
		e_k \leq \E\abs{\Psi_{H_{k-1}}(Y_{k-1}) - \Psi_{H_{k-1}}(y(t_{k-1}))} + \E\abs{\Psi_{H_{k-1}}(y(t_{k-1})) - \phi_{h}(y(t_{k-1}))}.
	\end{equation}
	 We then apply Assumption \ref{as:PsiStrong}.\ref{as:PsiStrong_Space} for the first term and Theorem \ref{thm:StrongOrderLocal} for the second term, thus obtaining
	\begin{equation}
		e_k \leq (1 + Ch) e_{k - 1} + C h^{\min\{q + 1, p\}}.
	\end{equation}
	Hence, iterating over $k$ and noticing that $e_0 = 0$, we get
	\begin{equation}
		e_k \leq C h^{\min\{q + 1, p\}} \sksum_{i=0}^{k-1} (1 + Ch)^i.
	\end{equation}
	We then remark that
	\begin{equation}
		\sksum_{i=0}^{k-1} (1 + Ch)^i \leq Te^{CT}h^{-1},
	\end{equation}
	which proves the desired result.
\end{proof}

\section{Weak order analysis}

We apply techniques of backwards error analysis. Let us introduce the operators $\diffL$ and $\diffL^h$ such that, if $Y_1$ is the first value produced by the numerical scheme described above, 
\begin{equation}
\begin{aligned}
	\Phi(\phi_h(y)) &= e^{h\diffL}\Phi(y),\\
	\E \Phi(Y_1\mid Y_0 = y) &= e^{h\diffL^h}\Phi(y),
\end{aligned}
\end{equation}
for all functions $\Phi$ in $\mathcal{C}^{\infty}(\R^d, \R)$. In particular, we can write explicitly $\diffL = f\cdot \nabla$, while no closed form expression for $\diffL^h$ is available. We now expand the functional of the numerical solution as follows
\begin{equation}
\begin{aligned}
	\Phi(Y_1) &= \Phi(\Psi_{H_0}(Y_0)) \\
	&= \Phi\Big(\Psi_h(Y_0) + (H_0-h)\partial_t\Psi_h(Y_0) + \frac{1}{2}(H_0-h)^2\partial_{tt}\Psi_h(Y_0) + \OO(\abs{H_0 - h}^3)\Big)\\
	&= \Phi(\Psi_h(Y_0)) + \Big((H_0 - h)\partial_t\Psi_h(Y_0)+\frac{1}{2}(H_0-h)^2\partial_{tt}\Psi_h(Y_0)\Big) \cdot \nabla\Phi(\Psi_h(Y_0))\\
	&\quad + \frac{1}{2}(H_0 - h)^2 \partial_t \Psi_h(Y_0) \partial_t \Psi_h(Y_0)^T \colon \nabla^2\Phi(\Psi_h(Y_0)) + \OO(\abs{H_0 - h}^3),
\end{aligned}
\end{equation}
where we denote by $\nabla^2\Phi$ the Hessian matrix of $\Phi$, and by $\colon$ the inner product on matrices induced by the Frobenius norm on $\R^d$, i.e., $A\colon B = \trace(A^TB)$. Taking the conditional expectation with respect to $Y_0 = y$ we get
\begin{equation}
\begin{aligned}
	e^{h\diffL^h}\Phi(y) - \Phi(\Psi_h(y)) &= \frac{1}{2} Ch^{2p}\partial_{tt}\Psi(h,y)\cdot \nabla\Phi(\Psi(h,y))\\
	&\quad + \frac{1}{2} Ch^{2p}\partial_t \Psi_h(y) \partial_t \Psi_h(y)^T \colon \nabla^2\Phi(\Psi_h(y)) + \OO(h^{3p}),
\end{aligned}
\end{equation}
where we exploited Hölder inequality for the last term. Moreover, expanding $\Phi$ in $y$ we get
\begin{equation}
\begin{aligned}
	\Phi(\Psi(h, y)) &= \Phi\left(\Psi_0(y) + h\partial_t \Psi_0(y) + \OO(h^2)\right) \\
	&= \Phi(y) + \OO(h).
\end{aligned}
\end{equation}
which implies
\begin{equation}\label{eq:DistanceProbDet}
\begin{aligned}
	e^{h\diffL^h}\Phi(y) - \Phi(\Psi_h(y)) &= \frac{1}{2} Ch^{2p}\partial_{tt}\Psi_h(y) \cdot \nabla\Phi(y)\\
	&\quad +\frac{1}{2}Ch^{2p}\partial_t \Psi_h(y) \partial_t \Psi_h(y)^T \colon \nabla^2\Phi(y) + \OO(h^{2p+1}).
\end{aligned}
\end{equation}
Consider now the following modified ODE and modified SDE
\begin{align}
	\hat y' &= f^h(\hat y), \label{eq:ModifiedODE} \\
	\begin{split}
	\dd\tilde y &= \Big(f^h(\tilde y) + \frac{1}{2}Ch^{2p-1}\partial_{tt}\Psi_h(\tilde y)\Big) \dd t \label{eq:ModifiedSDE}  + \sqrt{Ch^{2p-1}\partial_t \Psi_h(\tilde y)\partial_t\Psi_h(\tilde y)^T} \dd W_t,
	\end{split}
\end{align}
where $W_t$, with $t \geq 0$, is a standard $d$-dimensional Wiener process. These equations define the differential operators $\hat \diffL$ and $\tilde \diffL$ such that
\begin{equation}
	\Phi(\hat y(h) \mid \hat y(0) = y) = (e^{h\hat{\diffL}}\Phi)(y), \quad \E\Phi(\tilde y(h)\mid \tilde y(0) = y) = (e^{h\tilde{\diffL}}\Phi)(y).
\end{equation}
The operators $\hat \diffL$ and $\tilde \diffL$ can be written explicitly from \eqref{eq:ModifiedODE} and \eqref{eq:ModifiedSDE} as
\begin{equation}
	\hat \diffL = f^h \cdot \nabla, \quad \tilde \diffL = \Big(f^h + \frac{1}{2}Ch^{2p-1}\partial_{tt}\Psi_h\Big) \cdot \nabla + \frac{1}{2}Ch^{2p-1}\partial_t \Psi_h \partial_t \Psi_h^T \colon \nabla^2.
\end{equation}
Let us remark that the function $f^h$ is of the form
\begin{equation}
	f^h = f + \sum_{i=q}^{q+l} h^i f_i,
\end{equation}
where $q$ is given in Assumption \ref{as:PsiStrong}.\ref{as:PsiStrong_Order} and the terms $f_i$ are chosen such that
\begin{equation}\label{eq:DistanceModDet}
	e^{h\hat \diffL}\Phi(y) - \Phi(\Psi_h(y))	= \OO(h^{q+2+l}).
\end{equation}
It has been shown \cite{HNW93} that such a function always exists for Runge-Kutta methods. Let us compute the distance between the generator of the SDE and the modified ODE
\begin{equation}
	e^{h\tilde \diffL}\Phi(y) - e^{h\hat \diffL}\Phi(y) = e^{hf^h\cdot \nabla}(e^{\frac{1}{2}Ch^{2p}(\partial_{tt}\Psi_h \cdot \nabla + \partial_t \Psi_h \partial_t \Psi_h^T\colon\nabla^2)}-I)\Phi(y).
\end{equation}
Expanding with Taylor the two factors we get
\begin{equation}\label{eq:DistanceSDEMod}
\begin{aligned}
	e^{h\tilde \diffL}\Phi(y) - e^{h\hat \diffL}\Phi(y) &= (I + \OO(h))\Big(\frac{1}{2}Ch^{2p}\partial_{tt}\Psi_h \cdot \nabla + \frac{1}{2}Ch^{2p} \partial_t \Psi_h \partial_t \Psi_h^T\colon\nabla^2 + \OO(h^{4p})\Big)\Phi(y)\\
	&= \frac{1}{2}Ch^{2p}\partial_{tt}\Psi_h(y) \cdot \nabla \Phi(y) + \frac{1}{2}Ch^{2p}\partial_t \Psi_h(y) \partial_t \Psi_h(y)^T\colon\nabla^2\Phi(y) + \OO(h^{2p + 1})
\end{aligned}
\end{equation}
Now considering \eqref{eq:DistanceProbDet} and \eqref{eq:DistanceModDet}, we have
\begin{equation}\label{eq:DistanceProbMod}
\begin{aligned}
	e^{h\diffL^h}\Phi(y) - e^{h\hat \diffL}\Phi(y) &= \frac{1}{2}Ch^{2p}\partial_{tt}\Psi_h(y) \cdot \nabla \Phi(y) + \frac{1}{2}Ch^{2p}\partial_t \Psi_h(y) \partial_t \Psi_h(y)^T \colon \nabla^2\Phi(y) \\
	&\quad + \OO(h^{2p+1}) + \OO(h^{q+2+l}).
\end{aligned}
\end{equation}
Then, combining \eqref{eq:DistanceSDEMod} and \eqref{eq:DistanceProbMod} we then get the distance between the probabilistic method and the solution of the SDE as
\begin{equation}
	e^{h\tilde \diffL}\Phi(y) - e^{h\diffL^h}\Phi(y) = \OO(h^{2p+1}) + \OO(h^{q+2+l}).
\end{equation}
Choosing $l = 2p - 1 - q$ we have the balance between the two terms and
\begin{equation}\label{eq:DistanceSDEProb}
	e^{h\tilde \diffL}\Phi(y) - e^{h\diffL^h}\Phi(y) = \OO(h^{2p+1}),
\end{equation}
which is the one-step weak error between the probabilistic numerical method and the modified SDE. For the original equation, let us remark that thanks to Assumption \ref{as:PsiStrong}.\ref{as:PsiStrong_Order} we have
\begin{equation}\label{eq:DistanceExactDet}
	e^{h\diffL}\Phi(y) - \Phi(\Psi(h, y)) = \OO(h^{q+1}).
\end{equation}
Combining \eqref{eq:DistanceExactDet} and \eqref{eq:DistanceProbDet} we have the one-step weak error of the probabilistic method on the original ODE, i.e., 
\begin{equation}\label{eq:LocalWeakError}
	e^{h\diffL}\Phi(y) - e^{h\diffL^h}\Phi(y) = \OO(h^{\min\{2p, q+1\}}).
\end{equation}
In order to obtain a result on the global order of convergence we need a further stability assumption, which is the same as Assumption 3 in \cite{CGS16}.

\begin{assumption}\label{as:Stability} The function $f$ is in $\mathcal{C}^\infty(\R^d, \R^d)$ and all its derivatives are uniformly bounded on $\R^d$. Furthermore, $f$ is such that the operators $e^{h\diffL}$ and $e^{h\diffL^h}$ satisfy, for all functions $\Phi\in\C^{\infty}(\R^d, \R)$ and a positive constant $L$
	\begin{equation}
	\begin{aligned}
		\sup_{u\in\R^d} \abs{e^{h\diffL}\Phi(u)} &\leq (1 + Lh)\sup_{u\in\R^d}\abs{\Phi(u)},\\
		\sup_{u\in\R^d} \abs{e^{h\diffL^h}\Phi(u)} &\leq (1 + Lh)\sup_{u\in\R^d}\abs{\Phi(u)}.
	\end{aligned}
	\end{equation}
\end{assumption}

We can now state the main result on weak convergence. Let us remark that the theorem and its proof are similar to Theorem 2.4 in \cite{CGS16}.

\begin{theorem}\label{thm:weakOrder} Under Assumptions \ref{as:hStrong}, \ref{as:PsiStrong} and \ref{as:Stability} there exists a constant $C > 0$ such that for all functions $\phi\colon\R^d\to\R$ in $\mathcal{C}^\infty(\R^d,\R)$
	\begin{equation}
		\abs{\phi(u(T)) - \E\phi(U_N)} \leq Ch^{\min\{2p - 1, q\}},
	\end{equation}
	where $u(t)$ is the solution of \eqref{eq:ODE} and $T = Nh$. Moreover, 
	\begin{equation}
		\abs{\phi(\tilde u(T)) - \E\phi(U_N)} \leq Ch^{2p},		
	\end{equation}
	where $\tilde u(t)$ is the solution of \eqref{eq:ModifiedSDE}.
\end{theorem}

\begin{proof} Let us introduce the following notation
	\begin{equation}
	\begin{aligned}
		w_k &= \Phi(y(t_k) \mid y(0) = y_0)\\
		W_k &= \E\Phi(Y_k \mid Y_0 = y_0).
	\end{aligned}
	\end{equation}
	Then by the triangular inequality and the Markov property we have
	\begin{equation}
	\begin{aligned}
		\abs{w_k - W_k} \leq \abs{e^{h\diffL}w_{k-1} - e^{h\diffL^h}w_{k-1}} + \abs{e^{h\diffL^h}w_{k-1} - e^{h\diffL^h}W_{k-1}}.
	\end{aligned}
	\end{equation}
	Applying \eqref{eq:LocalWeakError} to the first term and Assumption \ref{as:Stability} to the second, we have
	\begin{equation}
		\abs{w_k - W_k} \leq Ch^{\min\{2p, q + 1\}} + (1 + Lh)\abs{w_{k-1} - W_{k-1}}.
	\end{equation} 
	Proceeding iteratively on the index $k$ and noticing that $w_0 = W_0$, we obtain
	\begin{equation}
	\begin{aligned}
		\abs{w_k - W_k} &\leq C k h^{\min\{2p, q + 1\}}\\
		&\leq C T h^{\min\{2p - 1, q\}},	
	\end{aligned}
	\end{equation}
	which proves the first inequality. The proof of the second inequality follows the same steps as above considering $w_k = \E\Phi(\tilde y\mid \tilde y(0) = y_0)$ and applying \eqref{eq:DistanceSDEProb}. 
\end{proof}

\section{Monte Carlo estimators}
We are interested in understanding the behavior of Monte Carlo estimator drawn from the numerical solution \eqref{eq:ProbMethVarH}. Given a function $\Phi\colon\R^d\to\R$ which is for simplicity $\mathcal{C}^\infty(\R^d, \R)$ with Lipschitz constant $L_\Phi$, we consider the mean square error (MSE) of the estimator $\hat Z$ defined as
\begin{equation}\label{eq:MSE}
	\hat Z = M^{-1} \sksum_{i = 1}^M \Phi(Y_N^{(i)}),
\end{equation}
where $T = hN$ is the final time, $M$ is the number of trajectories and we denote by $Y_N^{(i)}$ the realizations of the solution for $i = 1, 2, \ldots, M$. It is trivial to remark that 
\begin{equation}
\begin{aligned}
	\MSE(\hat Z) &= \E(\hat Z - Z)^2\\
	&= \Var(\hat Z) + \big(\E(\hat Z - Z)\big)^2.
\end{aligned}
\end{equation}
Hence, thanks to the result of Theorem \ref{thm:weakOrder}, we have
\begin{equation}\label{eq:MSEDecomposition}
	\MSE(\hat Z) = \Var(\hat Z) + \OO(h^{2\min\{q, 2p - 1\}}).
\end{equation}
The variance of the estimator can be trivially bounded exploiting the Lipschitz continuity of $\Phi$ and the independence of the samples by
\begin{equation}\label{eq:MSELipschitz}
	\Var\hat Z \leq M^{-1} L_\Phi^2 \Var Y_N.
\end{equation}
We can now prove a bound for the MSE of the Monte Carlo estimator.
\begin{theorem}\label{thm:MSEMonteCarlo} Under Assumptions \ref{as:hStrong} and \ref{as:PsiStrong}, the MSE of the Monte Carlo estimator satisfies
	\begin{equation}
		\MSE(\hat Z) \leq C h^{\min\{2q, 2p -1\}} + \mathrm{h.o.t.},
	\end{equation}
	where $C$ is a positive constant independent of $h$.
\end{theorem}
\begin{proof} Thanks to \eqref{eq:MSEDecomposition} and \eqref{eq:MSELipschitz}, we just have to show
	\begin{equation}
		\Var (Y_k \mid Y_0 = y) \leq \hat C h^{2p - 1},
	\end{equation}
	for a positive constant $\hat C$. From the definition of variance, of the operator $e^{h\diffL^h}$ and from the Markov property we have
	\begin{equation}
		\Var (Y_k \mid Y_0 = y) = e^{(k-1)h\diffL^h}\E(Y_1^2 \mid Y_0 = y) - \big(e^{(k-1)h\diffL^h}\E(Y_1\mid Y_0 = y)\big)^2.
	\end{equation}
	Using the definition of the numerical method \eqref{eq:ProbMethVarH} we have
	\begin{equation}
	\begin{aligned}
		\E(Y_1^2 \mid Y_0 = y) &= \E (\Psi_{H_0}(y))^2 \\
		&= \E(\Psi_{H_0}(y) - \Psi_h(y))^2 + \Psi_h(y)^2 \\
		&\leq L_\Psi^2 C h^{2p} + \Psi_h(y)^2,
	\end{aligned}
	\end{equation}
	where we exploited Assumption \ref{as:hStrong} and \ref{as:PsiStrong}.\ref{as:PsiStrong_Time}. Moreover, it is trivial that
	\begin{equation}
		\E(Y_1\mid Y_0 = y) = \Psi_h(y).
	\end{equation} 
	Hence, we can bound the variance at the $N$-th step as
	\begin{equation}
	\begin{aligned}
		\Var (Y_k \mid Y_0 = y) &\leq L_\Psi^2 Ch^{2p} + \OO(h^{2p + 1}) + e^{(k-1)h\diffL^h}\Psi_h(y)^2 - \big(e^{(k-1)h\diffL^h}\Psi_h(y)\big)^2 \\
		&= L_\Psi^2 Ch^{2p} + \OO(h^{2p + 1}) + \Var(\Psi_h(Y_k) \mid Y_0 = y).
	\end{aligned}
	\end{equation}
	Thanks to Assumption \ref{as:PsiStrong}.\ref{as:PsiStrong_Space} we now have
	\begin{equation}
		\Var (Y_k \mid Y_0 = y) \leq L_\Psi^2 Ch^{2p} + \OO(h^{2p + 1}) + (1 + \hat Ch)^2 \Var(Y_k \mid Y_0 = y),
	\end{equation}
	for some constant $\hat C > 0$ independent of $h$. Hence,
	\begin{equation}
		\Var (Y_k \mid Y_0 = y) \leq  \big(L_\Psi^2 Ch^{2p} + \OO(h^{2p + 1})\big) \sksum_{i=0}^{k-1} (1 + \hat Ch)^{2i}.
	\end{equation}
	We now remark that the sum is bounded as
	\begin{equation}
		\sksum_{i=0}^{k-1} (1 + \hat Ch)^{2i} \leq k e^{2Chk} \leq Te^{2CT}h^{-1},
	\end{equation}
	where $T$ is the final time of \eqref{eq:ODE}. The desired result is then proved setting $C = Te^{2CT}$.
\end{proof}
\begin{remark} This result is of critical importance as it implies that the Monte Carlo estimators drawn from \eqref{eq:ProbMethVarH} converge in the mean square sense independently of the number of samples $M$ in \eqref{eq:MSE}. Hence, we can set $M = \OO(1)$ when computing Monte Carlo averages without losing accuracy in the result if $h$ is small enough. In fact, introducing as a measure of error the square root of the MSE, and imposing a fixed tolerance $\epl$, i.e.,
\begin{equation}
	\MSE(\hat Z)^{1/2} = \OO(\epl),
\end{equation}
we have thanks to Theorem \ref{thm:MSEMonteCarlo} that the time step has to satisfy
\begin{equation}
	h = \OO\big(\epl^{1 / \min\{q, p - 1/2\}}\big),
\end{equation}
regardless of the number of trajectories $M$. Therefore, the computational cost to attain such a tolerance $\epl$ is given by
\begin{equation}
	\mathrm{cost} = \OO(Mh^{-1}) = \OO(\epl^{\min\{q, p - 1/2\}}).
\end{equation}
\end{remark} 

\section{Numerical experiments} 
We show numerical experiments verifying the theoretical results presented above.

\subsection{Strong order of convergence}

\begin{table}[!t]
	\centering
	\begin{tabular}{lcccccccccc}
		\toprule
		Method & \multicolumn{5}{c}{ET} & \multicolumn{5}{c}{RK4} \\ 
		\cmidrule(l{2pt}r{2pt}){2-6} \cmidrule(l{2pt}r{2pt}){7-11} 
		$q$ & \multicolumn{5}{c}{2} & \multicolumn{5}{c}{4} \\
		$p$ & 1 & 1.5 & 2 & 2.5 & 3 & 3 & 3.5 & 4 & 4.5 & 5\\
		$\min\{q, p - 1/2\}$ & 0.5 & 1 & 1.5 & 2 & 2 & 2.5 & 3 & 3.5 & 4 & 4 \\
		strong order & 0.52 & 1.01 & 1.52 & 2.02 & 2.01 & 2.50 & 2.99 & 3.55 & 3.99 & 3.98 \\
		\bottomrule
	\end{tabular}
	\caption{Strong order of convergence for the random time-stepping explicit trapezoidal (ET) and fourth-order Runge-Kutta (RK4) as a function of the value of $p$ of Assumption \ref{as:hStrong}.}
	\label{tab:NumericalResultsStrongOrder}
\end{table}

We now verify the weak order of convergence predicted in Theorem \ref{thm:weakOrder}. We consider the FitzHug-Nagumo equation, which is defined as
\begin{equation}\label{eq:FitzNag}
\begin{aligned}
y_1' &= c\big(y_1 - \frac{y_1^3}{3} + y_2\big), && y_1(0) = -1, \\
y_2' &= -\frac{1}{c}(y_1 - a + by_2), && y_2(0) = 1,
\end{aligned}
\end{equation}
where $a, b, c$ are real parameters with values $a = 0.2$, $b = 0.2$, $c = 3$. We integrate the equation from time $t_0 = 0$ to final time $T = 1$. The reference solution is generated with an high order method on a fine time scale. We consider as deterministic solvers the explicit trapezoidal rule and the classic fourth order Runge-Kutta method, which verify Assumption \ref{as:PsiStrong} with $q = 2$ and $q = 4$ respectively. Moreover, we consider random time steps as in Example \ref{ex:uniformH}, where we vary $p$ in order to verify the order of convergence predicted in Theorem \ref{thm:StrongOrder}. We vary the mean time step $h$ taken by the random time steps $H_n$ in the range $h_i = 0.01^{i}$, with $i = 0, 1, \ldots, 4$. Then, we simulate $10^4$ realizations of the numerical solution $Y_{N_i}$, with $N_i = T / h_i$ for $i = 0, 1, \ldots, 4$, and compute the approximate strong order of convergence for each value of $h$ with a Monte Carlo mean. Results (Table \ref{tab:NumericalResultsStrongOrder}) show that the orders predicted theoretically by Proposition \ref{thm:StrongOrder} are confirmed numerically. 

\subsection{Weak order of convergence}

\begin{table}[t]
	\centering
	\begin{tabular}{lcccccccc}
		\toprule
		Method & \multicolumn{3}{c}{ET} & \multicolumn{5}{c}{RK4} \\ 
		\cmidrule(l{2pt}r{2pt}){2-4} \cmidrule(l{2pt}r{2pt}){5-9} 
		$q$ & \multicolumn{3}{c}{2} & \multicolumn{5}{c}{4} \\
		$p$ & 1 & 1.5 & 2 & 1 & 1.5 & 2 & 3 & 4\\
		$\min\{q, 2p - 1\}$ & 1 & 2 & 2 & 1 & 2 & 3 & 4 & 4 \\
		weak order & 0.98 & 2.06 & 2.12 & 0.90 & 1.96 & 3.01 & 3.97 & 4.08 \\
		\bottomrule
	\end{tabular}
	\caption{Weak order of convergence for the random time-stepping explicit trapezoidal (ET) and fourth-order Runge-Kutta (RK4) as a function of the value of $p$ of Assumption \ref{as:hStrong}.}
	\label{tab:NumericalResultsWeakOrder}
\end{table}

We now verify the weak order of convergence predicted in Theorem \ref{thm:weakOrder}. For this experiment we consider the ODE \eqref{eq:FitzNag} as well, with the same time scale and parameters as above. The reference solution at final time is generated in this case as well with an high-order method on a fine time scale. The deterministic integrators we choose in this experiment are the explicit trapezoidal rule and the classic fourth-order Runge-Kutta method. The mean time step varies in the range $h_i = 0.1\cdot 2^{-i}$ with $i = 0, 1, \ldots, 5$, and we vary the value of $p$ in Assumption \ref{as:hStrong} in order to verify the theoretical result of Theorem \ref{thm:weakOrder}. The function $\Phi\colon\R^d\to\R$ of the solution we consider is defined as $\phi(x) = x^Tx$. Finally, we consider $10^6$ trajectories of the numerical solution in order to approximate the expectation with a Monte Carlo sum. Results (Table \ref{tab:NumericalResultsWeakOrder}) show that the order of convergence predicted theoretically is confirmed by numerical experiments. 

\subsection{Monte Carlo estimator}

\begin{table}[t!]
	\centering
	\begin{tabular}{lcccccc}
		\toprule
		Method & \multicolumn{2}{c}{ET} & \multicolumn{4}{c}{RK4} \\ 
		\cmidrule(l{2pt}r{2pt}){2-3} \cmidrule(l{2pt}r{2pt}){4-7} 
		$q$ & \multicolumn{2}{c}{2} & \multicolumn{4}{c}{4} \\
		$p$ & 2 & 3 & 2 & 3 & 4 & 5\\
		$\min\{2q, 2p - 1\}$ & 3 & 4 & 3 & 5 & 7 & 8\\
		MSE order & 3.01 & 4.05 & 3.04 & 5.02 & 7.08 & 8.06\\
		\bottomrule
	\end{tabular}
	\caption{Convergence of the MSE of the Monte Carlo estimator for the random time-stepping explicit trapezoidal (ET) and fourth-order Runge-Kutta (RK4) with respect to $p$ of Assumption \ref{as:hStrong}.}
	\label{tab:NumericalResultsMSE}
\end{table}

In the same spirit of the previous numerical experiments, we now verify numerically the validity of Theorem \ref{thm:MSEMonteCarlo}. We consider the ODE \eqref{eq:FitzNag}, with final time $T = 10$ and the same parameters as above. In this case as well, we consider the explicit trapezoidal rule and the fourth-order explicit Runge-Kutta method with random time steps having mean $h_i = 0.1\cdot 2^{-i}$ with $i = 0, 1, \ldots, 5$. We thus vary the value of $p$ of Assumption \ref{as:hStrong} and compute the mean order of convergence over 300 repetitions of the experiment. For each repetition, we consider only one trajectory to approximate the Monte Carlo estimator. We then compute the error with respect to a reference solution given by a high-order method with a small time step. Results (Table \ref{tab:NumericalResultsMSE}) show that the convergence order given in Theorem \ref{thm:MSEMonteCarlo} is respected in practice.


\bibliographystyle{siamplain}
\bibliography{anmc}

\end{document}