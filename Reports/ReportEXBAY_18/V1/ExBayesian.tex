\documentclass{siamart1116}

% basics
\usepackage[left=3cm,right=3cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[utf8x]{inputenc}
\usepackage[title,titletoc]{appendix}
\usepackage{afterpage}
\usepackage{enumitem}   
\setlist[enumerate]{topsep=3pt,itemsep=3pt,label=(\roman*)}

% maths
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\newsiamremark{assumption}{Assumption}
\newsiamremark{remark}{Remark}
\newsiamremark{example}{Example}
\numberwithin{theorem}{section}

% tables
\usepackage{booktabs}

% plots
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,positioning,fit,matrix}
\usepackage[labelfont=bf]{caption}
\setlength{\belowcaptionskip}{-5pt}
\usepackage{here}
\usepackage[font=normal]{subcaption}

% title and authors
%\newcommand{\TheTitle}{Probabilistic numerical methods with random time steps for chaotic and geometric integration} 
\newcommand{\TheTitle}{Report -- Analytical posteriors in a simple case} 
%\headers{Probabilistic Runge-Kutta method based on random time steps}{\TheAuthors}
\title{{\TheTitle}}
\author{\empty}

% my commands 
\DeclarePairedDelimiter{\ceil}{\left\lceil}{\right\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\|}{\|}
\renewcommand{\phi}{\varphi}
\renewcommand{\theta}{\vartheta}
\renewcommand{\Pr}{\mathbb{P}}
\newcommand{\eqtext}[1]{\ensuremath{\stackrel{#1}{=}}}
\newcommand{\leqtext}[1]{\ensuremath{\stackrel{#1}{\leq}}}
\newcommand{\iid}{\ensuremath{\stackrel{\text{i.i.d.}}{\sim}}}
\newcommand{\totext}[1]{\ensuremath{\stackrel{#1}{\to}}}
\newcommand{\rightarrowtext}[1]{\ensuremath{\stackrel{#1}{\longrightarrow}}}
\newcommand{\leftrightarrowtext}[1]{\ensuremath{\stackrel{#1}{\longleftrightarrow}}}
\newcommand{\pdv}[2]{\ensuremath\partial_{#2}#1}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\epl}{\varepsilon}
\newcommand{\diffL}{\mathcal{L}}
\newcommand{\prior}{\mathcal{Q}}
\newcommand{\defeq}{\coloneqq}
\newcommand{\eqdef}{\eqqcolon}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\E}{\operatorname{\mathbb{E}}}
\newcommand{\MSE}{\operatorname{MSE}}
\newcommand{\trace}{\operatorname{tr}}
\newcommand{\MH}{\mathrm{MH}}
\newcommand{\ttt}{\texttt}
\newcommand{\Hell}{d_{\mathrm{Hell}}}
\newcommand{\sksum}{{\textstyle\sum}}
\newcommand{\dd}{\mathrm{d}}
\definecolor{shade}{RGB}{100, 100, 100}
\definecolor{bordeaux}{RGB}{128, 0, 50}
\newcommand{\corr}[1]{{\color{red}#1}}

\begin{document}
\maketitle	

Let us consider as a toy problem the one dimensional ODE
\[
	y' = -y, \quad y(0) = y_0.
\]
We consider the inferential problem of determining the true initial condition $y_0^* = 1$ from the observations. We consider a single observation $d = y^{y_0^*}(1/2) + \eta$, where $y^{y_0^*}(1/2) = e^{-1/2}$ is the true solution at time $t =  1/2$ and $\eta \sim \mathcal N (0, \sigma^2)$ is a source of noise. If a Gaussian prior $\pi_0 = \mathcal N(0, 1)$ is given for $y_0$, the posterior distribution is computable analytically and is given by
\[
	\pi(y_0 \mid d) = \mathcal N\Big(y_0; \frac{de^{-1/2}}{\sigma^2 + e^{-1}}, \frac{\sigma^2}{\sigma^2 + e^{-1}}\Big),
\]
where $\mathcal N(x; \mu, \alpha^2)$ is the density of a Gaussian random variable of mean $\mu$ and variance $\alpha^2$ evaluated in $x$. Consistently, if $\sigma^2 \to 0$, we have that $d \to e^{-1/2}$ and therefore $\pi(y_0 \mid d) \to \delta_1 = \delta_{y_0^*}$. 

If we approximate $y^{y_0}(1/2)$ for a given initial condition $y_0$ with a single step of the explicit Euler method (i.e., $h = 1/2$), we get $y^{y_0}(1/2) \approx y_0 / 2$. Computing the posterior distribution obtained with this approximation leads to 
\[
	\pi_{\mathrm{EE}}(y_0 \mid d) = \mathcal N\Big(y_0; \frac{d}{2(\sigma^2 + 1/4)}, \frac{\sigma^2}{\sigma^2 + 1/4}\Big).
\]
In the limit of $\sigma^2 \to 0$, we get in this case that the posterior distribution tends to $\pi_{\mathrm{EE}}(y_0 \mid d) \to \delta_{2e^{-1/2}} \approx \delta_{1.213}$. The posterior distribution is hence tending to a biased Dirac delta with respect to the true value.

Let us consider the additive noise explicit Euler (AN-EE), i.e., the approximation $y^{y_0}(1/2) \approx Y_1$, where $Y_1 = y_0 / 2 + \xi$ and $\xi$ is a random variable $\mathcal N(0, h^3) = \mathcal N(0, 1/8)$, so that the method converges consistently with the deterministic method. In this case, the posterior distribution is given by
\[
	\pi_{\mathrm{AN-EE}}(y_0 \mid d) = \mathcal N\Big(y_0; \frac{d}{2(\tilde \sigma^2 + 1/4)}, \frac{\tilde \sigma^2}{\tilde \sigma^2 + 1/4}\Big),
\]
where $\tilde \sigma^2 = \sigma^2 + h^3 = \sigma^2 + 1/8$. In this case, taking the limit $\sigma^2 \to 0$ leads to 
\[
	\pi_{\mathrm{AN-EE}}(y_0 \mid d) \to \mathcal N\Big(y_0; \frac{e^{-1/2}}{2(h^3 + 1/4)}, \frac{h^3}{h^3 + 1/4}\Big),
\]
which shows that while the asymptotic mean is still biased with respect to the true value ($\approx 0.8087$), the uncertainty in the forward model is reflected by a positive variance ($= 1/3$).

Let us now consider the random time step explicit Euler (RTS-EE) with step size distribution $H \sim \mathcal U(h - h^p, h + h^p)$. In this case, the forward model acts as
\[
	Y_1 = y_0 - H y_0 = y_0 - h y_0 + (h - H) y_0 = \frac{y_0}{2}  + Uy_0, \quad U \sim \mathcal U (-h^p, h^p).
\]
The posterior distribution over $y_0$ can be computed as
\begin{align*}
	\pi_{\mathrm{RTS-EE}}(y_0 \mid d) &\propto \pi_0(y_0) \E^U\pi(d \mid y_0) \\
	&\propto \exp\Big(-\frac{y_0^2}{2}\Big) \E^U \exp\Big(-\frac{(d - y_0/2 - Uy_0)^2}{2\sigma^2}\Big).
\end{align*}
Let us compute the likelihood term. With a change of variable $z = Uy_0$ we obtain
\begin{equation*}
	\E^U\pi(d \mid y_0) = \frac{1}{2h^p y_0}\int_{y_0 h^p}^{y_0 h^p} \exp\Big(-\frac{(d - y_0/2 - z)^2}{2\sigma^2}\Big) \dd z.
\end{equation*}
Now a change of variable $w = (z - (d - y_0/2)) / \sigma$ gives
\begin{equation*}
	\E^U\pi(d \mid y_0) = \frac{\sigma}{2h^p y_0}\int_{(-y_0 h^p - (d - y_0/2)) / \sigma}^{(y_0 h^p - (d - y_0/2)) / \sigma} \exp\Big(-\frac{w^2}{2}\Big) \dd z,
\end{equation*}
Hence the likelihood can be expressed in terms of the cumulative distribution function $\Phi$ of a standard Gaussian random variable, i.e.,
\begin{equation*}
	\E^U\pi(d \mid y_0) = \frac{\sigma\sqrt{2\pi}}{2h^p y_0} \Big(\Phi\Big(\frac{y_0 h^p - (d - y_0/2)}{\sigma}\Big) - \Phi\Big(\frac{-y_0 h^p - (d - y_0/2)}{\sigma}\Big) \Big).
\end{equation*}
Disregarding all multiplicative constant that are independent of $y_0$, we get the posterior
\[
	\pi_{\mathrm{RTS-EE}}(y_0 \mid d) \propto \exp\Big(-\frac{y_0^2}{2}\Big) \frac{1}{y_0} \Big(\Phi\Big(\frac{y_0 h^p - (d - y_0/2)}{\sigma}\Big) - \Phi\Big(\frac{-y_0 h^p - (d - y_0/2)}{\sigma}\Big) \Big).
\]
The natural choice of $p$ is $p = q + 1/2 = 3/2$, and being $h = 1/2$ we get
\[
	\pi_{\mathrm{RTS-EE}}(y_0 \mid d) \propto \exp\Big(-\frac{y_0^2}{2}\Big) \frac{1}{y_0} \Big(\Phi\Big(\frac{\frac{y_0}{2} (1 + \frac{1}{\sqrt{2}}) - d}{\sigma}\Big) 
																							  - \Phi\Big(\frac{\frac{y_0}{2} (1 - \frac{1}{\sqrt{2}}) - d}{\sigma}\Big) \Big).
\]
In the limit for $\sigma \to 0$, the difference between the cumulative distribution functions of tends to $1$ or to $0$ depending on the sign of the argument. In practice, the limiting distribution is
\[
	\pi_{\mathrm{RTS-EE}}(y_0 \mid d) \propto \exp\Big(-\frac{y_0^2}{2}\Big)\frac{1}{y_0} \chi_{\{y_0 > y_{\mathrm{lim}}\}},
\]
where $y_{\mathrm{lim}}$ is given by
\[
	y_{\mathrm{lim}} = \frac{2\sqrt{2}d}{\sqrt{2} + 1}.
\]

The analytical posterior distributions for four values of $\sigma = \{0.1, 0.05, 0.025, 0.0125\}$ are plotted in Figure \ref{fig:ExactRK} and \ref{fig:Prob}.
\begin{figure}
	\centering
	\includegraphics[]{PostEx} \\
	\includegraphics[]{PostRK}
	\caption{Exact posterior distribution and explicit Euler posterior.}
	\label{fig:ExactRK}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[]{PostProb} \\
	\includegraphics[]{PostRTS}
	\caption{Posterior distributions for the AN-EE (top) and the RTS-EE (bottom) methods. The mean of the posterior distribution is represented by vertical dashed lines for the different values of $\sigma$.}
	\label{fig:Prob}
\end{figure}

\end{document}